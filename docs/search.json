[
  {
    "objectID": "thefix/putting-it-out-there/index.html",
    "href": "thefix/putting-it-out-there/index.html",
    "title": "Putting It Out There",
    "section": "",
    "text": "Seventh in a series of essays humbly titled “Fixing Journalism”. First posted on April 15, 2006.\nRecently I invited my washingtonpost.com colleague Adrian Holovaty to speak to the computer-assisted reporting class I teach at George Washington University. As I expected, he made a big impression with the students; here was a guy who did journalism that had obvious value and appeal, yet he didn’t write a single story. After he left, one of the students told me that he was the first guest speaker in three years of journalism classes who had left an impression that there was reason for optimism about the future of the industry.\nAdrian’s philosophy, if I can be so bold as to attempt to restate it, is to think about what kind of information people would want to look at and to provide it for them in the form of database-backed web sites. Browsable data isn’t the first thing you think of when journalism is the topic, and it probably won’t ever be the first thing. But it’s creeping up there, and the pressure isn’t coming from within newsrooms.\nAmong the changes that technology has given us, few may be as influential in the long-term as the irresistible movement forcing news organizations to open up. I’ve previously written that we need to open up to each other, and broaden lines of communication with our readers. We also need to turn what is the thin copper line represented by the daily newspaper into the broadband pipe that can carry more information than we could ever hope to package into a single paper.\nThere’s a bit of schizophrenia as newspapers in particular struggle with two seemingly conflicting goals: to produce the best daily paper while also creating and maintaining a web site that offers readers reasons to stick around after they’ve finished with the news. Breaking news or evergreen features? Daily congressional coverage or a congressional votes database? The truth is, this is not a choice we can afford to make. We have to do both, or as much of both as we can manage. This is the key part of the memo sent to Miami Herald staffers by that paper’s top editor, Tom Fiedler:\nEvery job in the newsroom — EVERY JOB — is going to be redefined to include a web responsibility and, if appropriate, radio. For news gatherers, this means posting everything we can as soon as we can. It means using the web site to its fullest potential for text, audio and video.\nVision: good. Now, what about execution? In order to use the web site “to its fullest potential,” newspapers have to approach information the way that Google does: automate as much as you can. To be clear, I’m not talking about automated news or robot editors making decisions. What I mean is delivering a consistent flow of information that needs as little human intervention as possible. For if we are to develop the kind of websites that demand and retain attention, we’ll need to make sure that they, like our readers, are always on, always fresh. If a reader can go someplace else to get the newest details of home sales in their area, why would they come back to the paper’s site? Such automation is, to me, perhaps the most important facet of Adrian’s work, and one that is only appreciated by a small percentage of newsroom leaders. As important as the features provided to readers by database-driven web applications are, the automated aspect is very important.\nThe technical side of this challenge is something that we can control and get right, but we don’t have the luxury of time. As an industry, we’ve spent some time making mistakes, and that’s a good thing for us as long as we learn from them. But we don’t have forever; nimble and automated upstarts like Newsvine may not cause journalists to quiver now, but they or their successors will eventually get it right and take away more of the audience that should be ours.\nThe change necessary to make this happen requires not only technical expertise, but also the recognition by newspaper folks that the work we do online is not supplemental, but elemental to our success. This cultural shift is probably a more difficult hurdle than any technical task, but it’s one that needs to happen. As Fiedler wrote: “We’ll come to appreciate that MiamiHerald.com is not an appendage of the newsroom; itss a fundamental product of the newsroom.” The question, for all of us, is how soon that realization takes place. The clock is ticking."
  },
  {
    "objectID": "thefix/journalism-lost-and-found/index.html",
    "href": "thefix/journalism-lost-and-found/index.html",
    "title": "Journalism Lost - and Found",
    "section": "",
    "text": "Eighth in a series of essays humbly titled “Fixing Journalism”\nLately the folks at my employer, The Washington Post, are counting the number of newsroom employees who have chosen to take the company’s early retirement offer, otherwise known as the buyout. Given journalism’s other challenges and the depth of talent at the Post, few other journalists should feel very sorry for us. But we are losing something when people with decades of experience and knowledge head towards retirement or another job.\nThe worst part? We’re losing much more than we have to lose, if only newspapers acted upon the principle that its information is as valuable as its people. Oral history programs are good and necessary, but they serve few current journalistic purposes. What newspapers need, not just at buyout time but all the time, is a plan to capture as much information and expertise as they can during the tenure of its employees.\nSuch a plan needn’t take on the aspects of a commune – we’re not all willing to share every bit of our treasured contact list or our important documents. But every time newspapers are able to salvage notes, research and transcripts in a manner that is accessible to other reporters, the paper benefits.\nPeople often ask me how this is working out at a place as competitive as the Post. Yes, we make good use of our wikis, but many of the internal wiki sites are password-protected, and thus restricted from general newsroom access. This is still a good thing. Let me explain why.\nFirst, at least this information is captured somewhere that eventually will make it easier to get into the hands of the people who need it. Maybe some notes, interviews and research are similar to classified documents – they need a cooling-off period before they become broadly available – but without the restricted wiki, they might never see aid another reporter. I’ll fight to open up restricted stores of information when I have to; at least we have them available.\nThe same could be done for departing reporters and editors. Have them write handover documents describing their beats, their success and their mistakes. Encourage them to be as generous as they are willing to be. If necessary, reach an agreement on how long such information would be restricted from the general newsroom. Just save the data. If this can’t be done, try the next best thing – use an archive of a reporter’s stories to create some valuable information. Who or what did he or she write about most often? Who were the most commonly-cited sources? What angles did he or she pursue, or neglect? Libraries may be silent, but newspaper archives should sing the songs of their creators.\nThe technical means to capture a reporter’s bookmarks, web-surfing habits and email are already here. But let’s not force this upon our newsrooms as if we’re replacing our desktops with HAL. Let’s make the pitch that the gifts and talents of journalism can live on for years if we allow them to, if we make room for them even as we see our colleagues head out the door.\nChris Peck of the Memphis Commercial Appeal has it right: “News will rely on the wisdom of the many, not the insight of the few, with journalists being knowledge leaders.” Who better to lead than researchers, who were the pioneers in using online resources, the ones who have brought the riches of the Internet to the newsroom? If we don’t preserve the information generated within our newsrooms, who will?"
  },
  {
    "objectID": "thefix/better-tools/index.html",
    "href": "thefix/better-tools/index.html",
    "title": "Better Tools",
    "section": "",
    "text": "Tenth in a series of essays humbly titled “Fixing Journalism”\nJournalists – Walt Mossberg and most of my fellow CAR geeks aside – aren’t known for their early adoption of technology. You see quite a few typewriters in newsrooms, not being used, of course, but as a reminder of simpler times. They’re like comfort food for folks who haven’t really become adjusted to the changes in our culture and industry. They could also be considered museum pieces, which is what they really are.\nGiven the tools reporters and editors have had to learn and then ditch a few years later – I’m thinking of ATEX terminals, “custom” editing and layout systems and the like – it’s not a surprise that journalists aren’t always eager to use the next best thing. Or even the pretty good thing. But the tools for reporting and analysis have improved so much and become so accessible that we are falling behind in the race for more insightful and original stories. Worse, our competition pool is growing as people equipped with cheaper and more powerful tools compile information that should be in our domain.\nI’ve written before about the state of a typical newsroom: some of the most valuable information will be stored in a reporter or editor’s account on the publishing system. This might be a Rolodex file of sources or beat responsibilities or interview notes. Some of it might be in Word files scattered around individual PCs. Some of it might even live in individual spreadsheets or Access databases on a network drive accessible by everyone. And some of it might reside in centralized database servers or in software controlled by certain departments.\nNone of these are a perfect solution, but one thing is clear: the days of using Word, let alone your newsroom’s editing system, as a repository of information are past. Word has its uses, but they mostly deal with the ability to use bold or italics. Actually, there is one very good use for Word – introducing reporters to the concept of pattern-matching, since Word’s search and replace function is probably its best feature. If you have text – words and numbers – store them in a text editor or a structured repository like a spreadsheet or a database. Using Word to store information can be considered harmful to journalism. If you use Word or WordPerfect or some other word processor to store your contacts, notes and other details, you need to stop doing this and move onto something better.\nSo what to use? How about a wiki, or maybe a content management system that stores information in a database. Make structured what needs to be retrieved often – important dates, figures and texts. Think of it this way: your newsroom probably stores its photographs in a database of some kind (if not, you’ve got an even bigger problem). Why should your raw information be harder to find, explore and share?\nLet’s start with something almost every newsroom covers: homicides. Unless your newsroom has an odd definition of news, murders usually warrant some coverage. The papers I have worked for have always done an annual “this year in homicide” story, usually around the 1st of the new year. “Last year [some number] people were murdered in our [city/county/area], an [increase/decrease] from the previous year.” And so it goes. While we should have something to say on whether murder is rising or falling, in my experience too many of these stories are thrown together in the final weeks of December so that all the reporter really can do is a big-picture overview. Because this annual story sometimes involves multiple reporters who originally covered some of the murders, it’s hard to write with authority.\nWhat if, in addition to asking the police department about homicides in December, we actually gathered and stored information throughout the year as murders occurred? By the end of the year the reporter would have a ready-to-go database and, with any luck, had spent some time with it – even if that only meant some data entry – in the prior months. It’s much more efficient than an archive search and much more likely to allow for some deeper analysis. But that’s only possible if we use better tools.\nI pick on Microsoft and its products a lot, but Excel is a great tool. It’s accessible to most people and serves as a sort of “gateway drug” to more robust tools. Plus, it’s pretty powerful itself. I know folks who could practically live inside Excel, data-wise. So if the idea of putting information into a database scares you, or your newsroom doesn’t really get why anybody would want a browser-based front end to a database system, use a spreadsheet to store information. And always think about what you can do with that information.\nAs long as I’ve taught a class on CAR, I’ve taught only Microsoft Access as a database manager. That ends now. I cannot recommend it any longer as a proper tool for reporters serious about working with data on a regular basis. The main reason is lock-in: in order to read Access files you need Access (or you can have your fun with ODBC, I suppose). The secondary reason, which will only become more important, is that when it comes to the Web, Access does nothing for you. It’s slow, only runs on Windows and has a relatively low file size limit compared to other databases. Access files seem to grow exponentially, a problem not unique to Access, but still a problem.\nAccess has two things going for it: it’s usually in the newsroom already, and it’s a known entity with lots of menus and wizards. The first is purely an accident of corporate software purchasing habits, and the second actually can make interacting with a database more confusing, not less. Chances are that if you have data in Access and want to do something with it on the Web, you’ll need to get the data out of Access and into something else. It doesn’t fit into the “information wants to be free” theory very well at all.\nIf not Access, then what? For work purposes, I’ve already switched to higher-end databases like SQL Server, MySQL and PostgreSQL. But in my CAR class this semester I briefly introduced SQLite and will use that next year. It runs on any system, is highly portable and a true SQL-based database. It also plays well with the Web and is free. Yes, I suppose there will be times when I think about the “crosstab query wizard” in Access, but already those occur less often than I anticipated. Don’t trade surface ease-of-use for power and flexibility.\nWhen it came time for my students to work on group projects, several of them chose to do their spreadsheet work using Google Docs, which made for more efficient use of time and also eliminated the need to email various versions of the data to each other. Now, I know full well that most news organizations are not going to approve of reporters putting sensitive material on Google’s servers, but the point is that you don’t have to work in the same way you did 5 or 10 years ago. Not using better tools isn’t showing some kind of stubborn respect for your tradition. It’s stupidity.\nInformation is literally trapped in newsroom applications just because we’ve been reluctant (or worse, ignorant) of the existence of better tools to do our jobs. At a time when journalism is being challenged by individuals who build useful tools in their spare time, most of us continue to work with the kind of constraints that no serious business would tolerate. If we cannot harness our expertise and vast stores of information and put them to better use than a 25-year-old with a good idea and some spare server capacity, then we deserve to be punished by the market.\nHow to get better tools in the hands of reporters and editors? Well, this one has always been something of a mystery to me, since journalists are inquisitive people. Unfortunately that usually doesn’t extend to the tools they use unless you can demonstrate clearly the benefits of adopting a new piece of software or equipment. That’s why newsrooms need people who are not just trainers but evangelists for better tools. And they need to get serious about requiring basic levels of proficiency from the people they hire. My students tell me that editors they interview with get really excited when they mention that they’ve taken a class in basic computer-assisted reporting methods. We need to get to the point where this is not a delightful surprise but an expectation.\nThe basic way that journalists gather information – reading, calling, interviewing, researching – will not go away, and this isn’t an attempt to argue that they should. Rather, it’s a suggestion that we need to start doing all of those things better, not just because we face greater competition for news, but because we can. That we have better tools and do not use them may not be apparent to our readers right now, but will be soon enough. Let’s not wait for that day, because it won’t be a happy one for any of us."
  },
  {
    "objectID": "thefix/index.html",
    "href": "thefix/index.html",
    "title": "Fixing Journalism",
    "section": "",
    "text": "This is a series of essays on the future of journalism and some of my ideas on how advances in technology have changed the way that we report and write the news. Your comments are welcome at dwillis AT gmail DOT com.\nThe Collaboration Issue, 4/29/2005\nJournalism suffers from an inability to recognize the shifts in ownership of information and from an astonishingly weak response to the changes that have taken place in the way that information is gathered and disseminated.\nThe Information Gap, 5/09/2005\nWith increased access to information comes a greater need for better management of it.\nThe Annotated Archive, 5/20/2005\nThe archive that newspapers provide to their employees should be more valuable than the one they sell to vendors.\nThe Engagement Process, 6/8/2005\nNow plenty of media outlets have blogs, chats and other platforms that allow readers and viewers to get a little closer to their news outlets. But we should be doing more.\nRivers of Data, 8/9/2005\nIt’s time we grappled with the permanent river of data. Because like news, data never stops.\nInnovate or Die, 11/14/2005\nIf [news libraries] drive innovation in the newsroom, we become invaluable to the growth and success of the paper.\nPutting It Out There, 4/15/2006\nAmong the changes that technology has given us, few may be as influential in the long-term as the irresistable movement forcing news organizations to open up.\nJournalism Lost – and Found, 6/5/2006\nWhat newspapers need, not just at buyout time but all the time, is a plan to capture as much information and expertise as they can during the tenure of their employees.\nThe Canvas for CAR, 1/13/2007\nThe Web is the canvas for CAR, better than any other platform we’ve come up with as an industry.\nBetter Tools, 5/19/2007\nThe tools for reporting and analysis have improved so much and become so accessible that we are falling behind in the race for more insightful and original stories."
  },
  {
    "objectID": "thefix/the-annotated-archive/index.html",
    "href": "thefix/the-annotated-archive/index.html",
    "title": "The Annotated Archive",
    "section": "",
    "text": "Third in a series of essays humbly titled “Fixing Journalism”\nI love archives. As a kid, my favorite books were reference books, the kind that had baseball stats for every team in every year since 1903. My mother, an English teacher, probably wondered what she had done wrong when I opted for lists over literature.\nSo I was a pretty happy fellow when I first walked into the news library at the Palm Beach Post in May 1995 as an intern. Here was a place loaded with archives, not just in the physical sense but also electronic, where I could search for the proverbial needle in a haystack – and sometimes find it.\nSo I love archives. But you know what I’d really love?\nAn annotated archive. An archive that doesn’t just display vertical depth going back years but can show relationships between archived items and the individuals and institutions named within them. An archive that can help find connections easier and can help new or unfamiliar users get up to speed quickly.\nNow this isn’t a knock on current archives; they are indispensable tools for research. But imagine if we could create and grow an archive that provided supplemental information and showed networks between topics, individuals and organizations.\nFor example, if you have a story on a controversial new development, the annotated archive could have links within the story to information researchers have about the company or its executives. A list of articles that mention a local businessman would be a single click away, along with other important information.\nThis isn’t impossible; in fact, the tools exist to build a rudimentary example of the idea right now. And unlike my previous essays, for this one I’ve tried to come up with a small example of what an annotated archive might look like.\nThe tool I chose is the MediaWiki software that powers Wikipedia. It’s free and open-source software, easy to install and administer. The reason I picked MediaWiki is that many folks have heard of Wikipedia and might be able to better envision how a newspaper archive with Wiki principles might work.\nNow, onto my example archive. It’s not much – only a few articles that I wrote or co-wrote while I was at Congressional Quarterly. The concept is pretty simple: each article in the archive can contain one or more internal references (in addition to external references) to other pages, which may contain other stories, information about a person, company or organization, or other notes.\nSo, for example, a story includes an internal link to Tom DeLay, the House Majority Leader. That page currently has links to external Web sites, but it could contain internal-only notes, contact information or pointers to documents or other references.\nOn each page, there’s a link on the lower left called ’’What links here,” which pulls up each of the other pages in the archive that reference that page. With the proper tending and weeding, the usefulness of such a feature only grows, illuminating connections that even beat reporters may not have realized.\nOf course that’s a best case scenario. Such an annotated archive requires regular maintenance – an archive editor, basically – who must be a news generalist and yet recognize key people and organizations within the community. Those people are inside our newsrooms – we must tap their knowledge in a better way.\nOne of the best features of an annotated archive is its flexibility. It can grow to incorporate new topics and can be updated when a familiar subject suddenly takes on a new importance. This doesn’t have to be done with Wiki software, either; that’s just an easy way to demonstrate the principles. We have access to such a range of technologies that can enable us to think about our information in new and better ways.\nUsers are already thinking about these issues. Consider The Annotated Times, which essentially tracks New York Times articles by topic and author, and even provides custom RSS feeds so that users can track articles by reporter. Or Chicago Crime, which took crime reports from the Chicago Police Department and made them available in ways that were interesting to readers. We need to treat our archives as our most valuable data, because it is.\nArchives have long been both an internal resource and an external product for newspapers. They still are, but we don’t have to treat them as the same thing. The archive that newspapers provide to their employees should be more valuable than the one they sell to vendors, and the newsroom is the place that can and must make it happen."
  },
  {
    "objectID": "thefix/innovate-or-die/index.html",
    "href": "thefix/innovate-or-die/index.html",
    "title": "Innovate or Die",
    "section": "",
    "text": "Sixth in a series of essays humbly titled “Fixing Journalism”\nThe news from Philadelphia isn’t good: the Philadelphia Inquirer and Daily News, which share a news research library, basically eliminated it by reducing the staff to “no more than two people” who mainly do archival work. Ten years ago there were 15 staffers.\nTo adapt to this change, the rest of the newsroom will be given “extensive training” in online services such as Lexis/Nexis and Internet resources in general, according to top newsroom editors.\nNow, I think this is a bad idea: the quality of both papers will suffer, and the reporters and editors who relied on the researchers could easily take their loss as a sign that Knight-Ridder, the corporate parent, does not understand how vital a good research department can be to a newspaper.\nThat said, what K-R has done in Philadelphia is challenged its news staff to develop and increase their research and reporting skills. If they provide adequate training, rather than sending reporters and editors off into the new information landscape alone, they could eventually wind up with a group of journalists with a skill-set appropriate, even necessary, to succeed.\nPerhaps “challenged” isn’t the right word, since this is a cost-saving measure that may turn out very badly for the newspapers involved. It’s more like an ultimatum. But it should be closely watched by many journalists, because however poorly the message was delivered, the need for journalists to more fully learn the skills of research is spot on.\nThe value of a news research department to the newsroom depends on many factors, including the paper’s budget and its leaders. Some do archival work first and research second, others try to balance the two. At the Post, we’re luckier than most: nearly all of our research staff are involved in what essentially are reporting tasks: backgrounding individuals and organizations, finding records or phone numbers or getting specific information from printed materials and the Internet.\nBut in light of the changes in Philly, news researchers can rightly ask, “Do our managers know what we do and understand its value?” In too many instances, the answer surely is “No.” Given the incredible advances in information storage and retrieval over the past 10-15 years, it’s not surprising that top newsroom leaders aren’t fully versed in the expanding range of research skills and tasks.\nIf any newsroom honcho has the idea that the news library is just the place where we clip old editions and file them away, then we who work in news libraries need to do a much better job of communicating about our roles and offerings. And not just to management, but throughout the newsroom.\nSo how do we do that? Greater openess, for one. Particularly at larger organizations, the known quantity that comes from working one-on-one with researchers doesn’t naturally translate up the ladder. So let’s literally show the newsroom what we do: let’s make our work as transparent as possible.\nMike Meiners of The St. Louis Post-Dispatch writes in the Fall issue of News Library News (pdf) that the P-D’s library has established a wiki for researchers to share and maintain information. The advantage is two-fold: not only are we providing more information to the newsroom, but it’s easy to see the contribution. Even if you have to keep certain information behind a password, it’s still there to show editors and others. For each project or story, a wiki serves as a notebook that can be searched, archived or linked to other research efforts.\nToo often, we in news libraries work in ways very similar to our colleagues in the newsroom: we bounce from story to story without managing our information. If we’re supposed to be the information experts, shouldn’t we do it differently, and better?\nM.J. Crowley, the Information Editor at the Newark Star-Ledger in New Jersey, asked several questions on the Newslib listserv in the wake of the Philadelphia announcement. They included:\n\nWhat is our worth to the news organization…and the bottom line?\nAre we an asset or a liability? How can we demonstrate our value?\nHave you served your boss today? And does he/she know it?\n\nThe third is a local question between employee and boss, but the first two are broader. To my mind, there’s no department better positioned to be the innovators in the newsrooms. Reporters and editors don’t have time, particularly at smaller papers with fewer training opportunities. They also are removed from the breadth of information in the entire newsroom, seeing only their individual pieces.\nWe need to show our newsrooms things they haven’t seen before – mapping applications, combinations of information resources that they haven’t thought of, automated feeds from crucial sources, custom-built collections of resources using our expertise. If we drive innovation in the newsroom, we become invaluable to the growth and success of the paper. If we anticipate the needs of our reporters and editors – and fulfill them – we become an integral part of the operation.\nThe Philadelphia papers probably think that they can save money by replacing their true knowledge workers with a bunch of online services. While they just might save some money, chances are they won’t make the papers any better in the long run. Not because reporters and editors can’t learn how to search Lexis or the Internet, but because they’re not the natural innovators of the newsroom. The innovators are the ones who know their organization’s information best."
  },
  {
    "objectID": "thefix/the-information-gap/index.html",
    "href": "thefix/the-information-gap/index.html",
    "title": "The Information Gap",
    "section": "",
    "text": "Second in a series of essays humbly titled “Fixing Journalism”\nAn email list for news researchers that I subscribe to recently featured a discussion on the kinds of skills necessary to get a job in the news library, which performs research for reporters and editors. In several of the posts, managers said that they were looking for people with CAR skills in addition to the traditional abilities of searching records and providing important background and missing information.\nThis is, to my mind, an acknowledgment of the growing challenge facing newsrooms: information management. Whether we’re talking about timelines, interview notes or databases, newsrooms have ever-more information in digital form and we’re doing too little to index it, manage it or make it broadly available.\nThis situation is exacerbated by what I see as an industry trend in which the people who write the stories are removed little by little from the information and subjects they write about. More frequent deadlines, increased competition and the ability of the Internet to spread information quickly demand that reporters be able to assemble a broad picture quickly, many times with the assistance of researchers.\nThe way that we gather information has changed dramatically over the past decade, and reporters and editors have not kept up with this revolution. The wealth of material available through online databases in particular and also on the Internet means that reporters have access to much more information than they did in years past.\nSo, for example, if a reporter is seeking to write a profile of a relatively well-known person, he or she could request a “clip-job” of what other papers have written to see what else has been published. They can request searches of public databases and other records. In most cases, this is a good thing; more information is hardly a negative. But with increased access to information comes a greater need for better management of it.\nThe real trouble comes when research tasks are repeated within newsrooms and the cumulative value of that work is either locked away in individual computers or simply thrown away. In the worst of cases, we end up doing the same task more than once and while the information gets to its consumer, nobody in the newsroom learns on a permanent basis.\nInstitutional knowledge in newsrooms now largely resides inside the heads of reporters and editors or within some files that few people can access. Aside from the incredible inefficiency, this model also suffers from the “hit-by-a-bus” scenario – what happens when key people leave suddenly? The case for sharing information broadly inside newsrooms has never been better than now, when we have access to so much data.\nNewsrooms collectively face a couple of situations from this massive availability of information. The first I’ll call the Google problem. This isn’t Google’s fault, but rather an unintended consequence of its influence and ease of use. The Google problem can appear in several ways, but one of the most common arises when reporters and editors attempt to use the newsroom as their own search engine, for example by asking for sources or seeking out information through emails to the entire staff.\nThis is also inefficient, particularly when the newsroom in question is a poor approximation of a Web search engine: results may vary wildly and can be easily skewed by the respondents to such queries. If reporters want to use the newsroom like they do Google, we need to have a better system of harnessing the knowledge within it and making it available to reporters in a more systematic way.\nReporters also come to love Google’s ease of use and think that all archives or databases should be as simple to use. But most newsroom resources have their own search syntax or begin with a complex interface or suffer from neglect. People turn to Google because it’s simple and it doesn’t change much. We need to make our information much easier to search.\nThe second problem is that many reporters don’t keep pace with what they need to know about searching databases. Newspapers and other media organizations need to do a better job training reporters and making such instruction an important and recurring part of the job. This is crucial because it is a cultural problem rather than a practical one. It isn’t that reporters can’t do this, because many are expert at finding hidden gems within paper documents like transcripts, budgets and court records. Some of them either opt to depend on other people when available or don’t keep up with search skills, and so the layer separating them from raw information only grows. Any such layer increases the risk of misinterpreting or just flat-out missing good information. In an age where a growing set of readers have both experience in finding information online and access to some of the same tools, newspapers need to get this right to maintain and enhance their credibility.\nFinally, there’s a very important reason to adopt better information management practices: we’ve got to keep pace with the people and organizations that we cover. We see some of the most dramatic examples of the information gap in political coverage, where the consultants and party officials that papers cover have used technology much better than we have. We see it in baseball, where executives rely on data analysis as newspapers continue to print boxscores and offer only occasional insights based on empirical evidence.\nSo how to close this gap? Let’s start in the news library, where a lot of information already is generated or compiled and where the culture of collaboration is typically strong. Create a network for researchers to post or upload both their information-gathering strategies and the best results, and allow others to view them, comment on them and share them within the newsroom. It’ll save duplicative efforts and teach the library staff best practices that can help spread the word throughout the newsroom.\nThen, when reporters ask for information, researchers can check to see if it already exists (or needs to be updated) and find out the best ways to do so. They also could begin to standardize their delivery methods so that the information they pass out is archived and searchable, if not categorized in some way. It need not be a universal archive, but perhaps within a single section, team or beat. If reporters want to print out the results, fine. But having a permanent archive will benefit the entire paper, not just a single reporter.\nAn additional benefit is that the record of information requests and results could be available to the reporters and editors, so they can see what was done and get more involved in evaluating search strategies and sifting through results. But we need to be clear on using a computer for these tasks: money that the newspaper spends on costly databases and people to search them should benefit the entire newsroom, and the best way of ensuring that is to keep an electronic record of that process.\nThe platform to accomplish this already exists: the Web. It’s greatest strengths are simplicity – reporters know how to use a Web browser – and flexibility. We can make data appear in different ways, allow various types of searches and, perhaps most important, reporters can be given a share of ownership in the information. That, more than anything, will motivate them to use a system of collaboration. The Web also is accessible from the office, home or the road. It doesn’t require purchasing and installing expensive software, and it cuts down on specialized training.\nBut the best reason for using the Web to distribute information internally is adaptability. Few software makers design products specifically for newsrooms, which can have unique needs. We should steal the best ideas from the growing suite of applications that address information management. Consider the manifesto of Backpack, a new Web-application from a group of designers and information architects in Chicago called 37Signals. It’s a dream come true for the often sloppy world of journalism:\nYour information is everywhere. Scattered across notebooks, emails, post-it notes, and god knows where else. With Backpack, there’s a better way. Now Information is easy to organize, centralize, and retrieve. Information works for you instead of vice versa. Backpack adjusts to organize things your way. It’s a blank slate that offers you less structure and more space. Easily fill your pages with anything you want, from notes to to-do lists to images to files. And then group them however you want, wherever you want. It’s organization the way you want it.\nCustomized database applications like Lotus Notes or FileMaker Pro can work in certain circumstances, but nothing works like the Web. Imagine if, instead of newsroom Intranets, we had internal Wikipedias in which reporters, editors and researchers worked together to preserve and improve the paper’s institutional knowledge? Some of the time now spent by researchers answering requests could be handled by a living Intranet updated by the people who know best would enable them to spend time on more complex (or urgent) tasks.\nThe basic tasks of journalism are not threatened by the Web or its principles. Indeed, they provide newspapers with an additional boost, if reporters and editors grasp the long-term benefits. But the goal of journalism – accurate, informed information – can be done easier when reporters, researchers and editors are working closer together, all sharing in the process of finding and evaluating information.\nWe will always have anonymous sources and reporters who hold their secrets tightly. Newspapers should not seek the perfect internal archive but instead try to build the best they can. Some areas may need to be password-protected within teams or sections, with a clear point-person who can grant acccess or provide details. But these are exceptions and should not be the rule of thumb throughout the newspaper. Instead, we should share information with each other and with the public. It’s time for newspapers to stop treating the information age as simply a topic of coverage and instead make it a central part of the newsgathering process.\nAt a recent computer conference in California, Adam Bosworth of Google told the audience that he wished to move beyond the current state of Internet searching. Search engines like Google return information, but he wanted a Web search engine that would provide content – things with specific meaning and value. That’s what people want from newspapers – meaningful content, not just information.\nSo how to get better content? Well, you could make sure that newspapers hire only the best and brightest and then give them the freedom to report widely and consult with the best experts in their fields. Again, we’ve got a scaling problem – in this scenario we’ve got a handful of journalistic stars working for an even smaller number of outlets, and underpaid and overworked folks down the ladder. This is not a long-term solution – journalism at the lower levels presents an inviting target for niche businesses that can capitalize on the strengths of the Web.\nWe must, then, be better than we currently are at every level. Good information management practices benefit the reporter through better recognition of trends and relationships. They benefit newsrooms in general through increased efficiency and wider access to common data. And they benefit readers by giving them better-informed journalism."
  },
  {
    "objectID": "thefix/rivers-of-data/index.html",
    "href": "thefix/rivers-of-data/index.html",
    "title": "Rivers of Data",
    "section": "",
    "text": "Fifth in a series of essays humbly titled “Fixing Journalism”\nNews never stops. We’ve accepted this as a condition of our modern age, for better or worse, and all types of news organizations are coming to grips with serving an audience that is more connected than ever. Yet one of the ironies of the 24-hour news cycle is that even as newsrooms get better tools, when they use data it tends to be in lengthier project reporting rather than daily reporting.\nThis results in impressive and exclusive stories with big impact that win awards and garner plenty of attention among the journalism industry. In combing newspaper websites for entries for The Scoop, I’ll admit that my eyes are drawn to those projects that involve analyzing hundreds of thousands or millions of records. Bigger seems better to a lot of folks, and sometimes it is.\nThis isn’t an argument against such stories. They are necessary, even vital to journalism’s public service role and, if anything, we should do more of them. But not every paper has the resources to commit to such extensive work on a continous basis, and even if they did, there are plenty of daily and short-term stories that could benefit from data.\nWhat’s more, the flow of information is increasing. Now newsrooms confront rivers of data both large and small. They flow continously, shifting in subtle and obvious ways. Some are external and some we create, even if we don’t recognize the process. We (grudgingly) accept the permanent campaign, the permanent news cycle. It’s time we grappled with the permanent river of data. Because like news, data never stops.\nThe pace of data has been somewhat slow in the past, thanks to the stubborn popularity of paper and the limitations of our own equipment – to say nothing of the relative lack of data-savvy folks within newsrooms. At many places the people with CAR skills are fenced off in the projects team, although slowly this seems to be changing.\nData isn’t just useful for finding one thing, or for one story. Like much of the other information that passes through a newsroom, data too often falls through the floor instead of being reused. Too often it’s not even collected. But even an older dataset that does not get updated can have research value – how many times do journalists need to know something about a situation that occurred years ago, or to trace the history of a person or organization?\nPart of the problem is the word “data,” to be honest. To many reporters and editors, that either refers to a character from “Star Trek: The Next Generation” or to a government-produced chart in a report. There’s nothing systematic about the way we treat it, and so it’s no surprise that it doesn’t become a priority. But as a source of information, data should be given equal footing with other sources. Reporters pore over current and past statements, positions and documents. We should do the same with data.\nIt’s easier to sell editors and reporters on large datasets, particularly when they are exclusive and relate to a hot topic. Plus, it’s just fun to say that you analyzed hundreds of thousands or millions of records, as if the more records you have the harder the computer has to work. But we should care as much, if not more, about the smaller sets of data that flow through the newsroom: the regular reporting checks, the timelines we build and maintain, even the monotonous parade of press releases and reports.\nWhy? Because we’re dealing with so much information that seeing both individual items of importance and recognizing shifts in long-term patterns is harder than ever. Sure, we have Google, we have Nexis. But a typical search in either is looking for a single point of information (when was a person born?) or a broad summary (how many times has the phrase “supersize me” appeared in major newspapers?). You don’t get both, but you could. You just have to have a process flexible enough to allow for both the big picture and the little details.\nHere’s an example: a few months ago I cooked up a Python script that would produce both a database of electronic federal campaign filings and an RSS feed of filings that updated several times a day. The latter is built from the former and thus I’m adding to a larger database and monitoring the flow of information into it. Doing this allowed me to spot a single filing – the final one by Enron’s PAC – and send it on to a colleague who used it for his column. Meanwhile, the larger dataset enabled me to figure out that one political committee had filed 18 amended reports in a two-day period and figure out how often something like that had happened in the past.\nThe secret to this is that by compiling the details, you automatically get the big picture. So even if individual events don’t yield many stories, chances are that the aggregation of them all will.\nThe best part is that we do some of this already – lots of newspapers run weekly property sales, crime reports and other small rivers of data, but they don’t always think about how those can be deployed to help everyday reporting. Think about the crime reports: a database editor could keep tabs on the larger data set fed by weekly updates, while the cops reporter could get running updates in a feed that could spark questions or alert him or her to emerging patterns.\nAnd the Web is making this process easier, as governments and organizations put more and more information online. Part of the barrier of building and maintaining databases is the constant effort required for upkeep. While this won’t disappear entirely, the Web cuts down on the time and manpower needed, and it provides the best way to deliver the data.\nI suspect a lot of newsroom folks might get hung up on the “but it’s a bunch of numbers” issue. Yes, data can be numbers, but it also can be a collection of events. Jo Craven McGinty of the New York Times – a longtime friend via IRE – recently demonstrated how simply collecting event information can produce interesting content. She took data on hate crimes in New York City, mapped it and summarized it for the paper (the national edition version trimmed the story but included the map and other graphics).\nThe advantage of maintaining this dataset is that, when future hate crimes occur, they can instantly be checked against the history for contextual information. Anybody can say there was a hate crime on Broad Street. Saying that it was the fifth in two years within a four-block span is what can separate one newspaper from the multitude of competing voices.\nTo do that, newsrooms need people who can “read” data like they can read the face of an interview subject or spot the meaningful paragraph buried in a lengthy report. That used to be asking a lot, but now we can present information in a variety of ways that are familiar to even those with basic computer skills, thanks to dynamic Web pages, RSS feeds that appear in your inbox and maps. We now have the means to make data accessible and interesting to even the least data-savvy journalist.\nDone well, the display of data actually overcomes the massive nature of some datasets and draws in even people inexperienced in using data. Chicagocrime.org is a perfect example of this – Adrian Holovaty put a simple yet compelling front-end on a publicly-available dataset, and now everybody can be a part-time crime analyst.\nThis all may sound like a justification for hiring people with CAR skills, and I suppose that’s true in a certain sense. But it’s more than that – it’s a call for newsrooms to put their reporters and editors in the best position to publish with authority, context and the kind of detail that readers will appreciate and come to expect.\nThe tools and skills to make this happen are here now and will only get better. But we as journalists must decide to invest in our ability to handle information in order to make sense of those rivers of data that flow by us every day."
  },
  {
    "objectID": "thefix/the-engagement-process/index.html",
    "href": "thefix/the-engagement-process/index.html",
    "title": "The Engagement Process",
    "section": "",
    "text": "Fourth in a series of essays humbly titled “Fixing Journalism”\nJournalists are, by nature and necessity, curious people – and not just about the people and institutions they cover. We like to know what others say about us, or if other media outlets use or credit our work. The popularity of Jim Romenesko’s column on Poynter’s website is all the proof anyone would ever need.\nAs Sy Hersh put it during his IRE panel last week, journalism can a “bitchy” profession that seems unlikely to adopt many of the ideas I wrote about in my essay on collaboration. But that’s ok.\nEven if some of the traits possessed by journalists keep us from collaborating (a point I don’t entirely agree with), others open a door for a broader engagement with our audience. Here’s an example: reporters have long been used to responding to readers and critics, usually through the telephone or the mail. We’re not always excited about it, but we do it. Such communication is largely one-way – responding to every inquiry is impossible, and even when that happens it exists between two people.\nThen came email, a double-edged sword for some in that it became much easier for our critics to reach us, but for many others email represents a new way to receive tips and connect with readers or viewers. Email can still be mostly one-way communication, since it’s hard to find the time to respond individually. But the door of conversation opened a bit wider.\nNow we have the ease and popularity of personal publishing on the Web, mostly in the form of weblogs but also other types of sites. The immediate impact on newspapers was hard to gauge, since papers didn’t adopt those types of publishing technologies or do much to respond to them. Now plenty of media outlets have blogs, chats and other platforms that allow readers and viewers to get a little closer to their news outlets. But we should be doing more.\nThe reason is simple: if you are, say, a daily newspaper, you have one product a day and probably a website that gets a few updates during that period. Your readers, on the other hand, have as many opportunities to respond to your articles as they are able to produce. And they’re doing it now, or they soon will be. More than ever, media outlets are put in the position of being reactive to what others are saying and doing. We’re not the sole owners of the news anymore.\nWant proof? Try visiting Technorati and searching for news stories. I did this last week for a piece I worked on involving lobbyist Jack Abramoff and Indian tribes, and found half a dozen blogs that linked to the story and provided some comments. Some asked further questions, others offered faint praise. Another story I worked on in May received similar commentary but also a series of questions.\nWhat if I had wanted to respond to a question? Sure, there’s email, but unless I follow up in the paper no one would know if I had done anything at all. Outside of chats on newspaper sites or in stories written by media critics, it’s hard to find examples of reporters responding to comments posted on a website.\nThis is not, I believe, because papers want to shut out bloggers, but rather because we haven’t really thought about it or discussed it within newsrooms. The result is that while many reporters are in regular contact with readers by phone or email, the conversations taking place on the Web are too often missing our participation. To recap: phone OK, email OK, Web – well, we’re not sure.\nYet of those three platforms, the Web offers the greatest reach and transparency, both enormously beneficial to media outlets. In his inaugural column as the New York Times’ second Public Editor, Byron Calame wrote that “I hope to raise the blinds at The Times in some new ways to allow readers to get a clearer view inside the newsroom process. Greater transparency, I believe, can help you as readers better understand the news judgments that shape each day’s paper – and hold The Times’s news staff more accountable.”\nHe pledged to do that by using the Web more – a smart choice, since that can provide the biggest “bang for the buck” and it is where the future of the news industry lies. He’ll use the Web to publish more letters from Times readers and to host Q&A sessions with reporters and editors. Actions like these will help re-establish the links between newspapers and their readers in an age when there are thousands of options for news.\nThis is not without risks, of course, and high among those is the idea that a paper can become overwhelmed by disruptive conversation that drowns out the civilized discourse we all want (the Ventura County Star recently shuttered its discussion forums for this reason). I agree with my colleagues who say that people want a newspaper in part because it is edited and not simply a sea of unorganized information. But the relative lack of engagement we have now, when doing more is not difficult, cannot be our choice.\nAs with phone calls and emails, we do not have to respond to every blog posting. We need not encourage or acknowledge vitriolic screeds in response to our work, and we can remove those types of postings from our own sites. But we should encourage reporters and editors to respond to well-reasoned inquiry and civil conversation on and off our official sites.\nSuch behavior humanizes us, I believe, and helps to counter the notion that journalists are unresponsive or unaccountable. Ombudsmen or reader representatives seem to be popular with readers, and we can expand upon that work on the Web.\nThere are thousands of conversations going on about who we are and what we do in the media, and we don’t participate in enough of them. Our readers want to know more about us and the process that results in a paper arriving on their doorsteps. They’re knocking on our door; we should let them in."
  },
  {
    "objectID": "thefix/the-collaboration-issue/index.html",
    "href": "thefix/the-collaboration-issue/index.html",
    "title": "The Information Gap",
    "section": "",
    "text": "First in a series of essays humbly titled “Fixing Journalism”\nSometime before I joined the Washington Post last fall, someone had the idea of installing an internal search engine that would enable people to search the contents of documents on the paper’s network, including the individual network drives of reporters and editors (who could still restrict files to their own PCs). This was and still is a non-starter in the newsroom; nobody wanted to make their information available (the idea was discussed by managers but never presented to the newsroom in general or any reporters for approval or rejection).\nCan you imagine another information-based business that permitted its employees to build walls around their information? Can you imagine it succeeding today?\nNewspapers, to put it in computer terminology, don’t scale well. Papers like the Post and the New York Times have remarkably intelligent people with extensive contacts and reams of interview notes, documents and acquired knowledge about their beats. But the larger the operation, the less likely that any one person can locate the information held somewhere in the newsroom. When reporters change beats, there is little incentive to turn over the best information and sources – after all, they might come in handy some day. The modern newspaper is the anti-Google – it keeps its best information within its own walls, and makes it hard even for those few who work there to get to it.\nIt’s not just that, as Dan Gillmor likes to say, our readers know more than we do. We don’t know as much as we could or should, given the amount of information that passes through newsrooms every day. At a NICAR conference in 2000, George Landau spoke about capturing some of the information that falls through the newsroom floor each day. Five years later we’ve done very little about it, and most of our editing and publishing systems are designed to make this task difficult at best. Landau’s company, NewsEngin, has a bunch of clients but few that, to my knowledge, use the full capabilities of the product. Mostly they’re just too busy. Others don’t even think this is a necessary task.\nIn the meantime, we’ve seen a decline in readership, a loss of trust and more competition on many fronts, particularly from the Internet. Yet most newspapers, aside from requiring that their reporters now file for the Web, have barely changed their reporting and information-management practices. We have redesigned, added color, tried to appeal to younger readers, changed the comics and shortened stories. Very little of it has worked, which leads me to believe that the real issue is not in the packaging of news but in the product itself.\nJournalism’s chief problem is not bias or its delivery method, although both deserve scrutiny. Journalism suffers from an inability to recognize the shifts in ownership of information and from an astonishingly weak response to the changes that have taken place in the way that information is gathered and disseminated.\nThe actual job of reporting the news has not fundamentally changed in the past ten years, a remarkable act of insulation despite a changing information world. With the exception of replacing typewriters with PCs, most reporters work with the same tools – the phone, the pen and a pad of paper. Sure, we’ve added tape recorders, but some reporters don’t use even those. The typical reporter’s use of the computer – word processing, the calculator and Google searches – barely scratches the surface of the possibilities.\nBetter tools and training will only improve journalism so much without a broader change. We haven’t realized that some fundamental changes – nearly all driven by the Internet and the new ways of organizing information – have taken place in our profession.\nThe first issue is one of ownership. In the past, most newspapers could report the news with the solid assurance that no one else had the information they had. Owning the story was an all-encompassing phrase: reporters and editors could make decisions confident that the only other people who might have access to the same information were the sources themselves and, perhaps, their media competitors.\nThe Web increasingly makes this traditional construct less true. On many stories, we no longer own all of the information associated with it. Sources can publish their own views on the Internet, including transcripts of interviews done by reporters. The actions of reporters online – emails, surfing habits – can become part of the story when others publish details of them. This is new, and a little unsettling to some reporters. We haven’t had this kind of broad exposure of our own work habits, and it isn’t pretty. It used to be that you didn’t want to see legislation and sausage being made. Some reporters might want to add journalism to the list, but I disagree. We can change our habits so that such exposure does not make us defensive or hesitant.\nThe second issue is cultural. Along with a reduction in ownership of information, journalism also suffers in comparison to the Web because of its tendencies and traditions. Journalists don’t share much. We’re trained to keep our prized sources, even to the point of not identifying them to our colleagues. We place a high emphasis on getting on the front page, preferably without sharing a byline.\nThe larger a news operation is, the more likely that it does not have, say, a newsroom-wide database of contacts. We don’t have one at the Washington Post, although reporters frequently ask for phone numbers over our internal messaging system. And they get them – or maybe they don’t. Only the requestor knows for sure. Let me repeat that – one of the largest newspapers in America has no regularly updated searchable list of valuable contacts. Is it any wonder that other people can locate people or information faster?\nThis matters because one of the advantages that larger newsrooms traditionally had – more reporters in one place capable of gathering more information – can be trumped by using the Web. Wikipedia isn’t journalism, but the model has to be a concern to even the largest papers – the idea that a community of contributors could coalesce around a framework without much in the way of direction, instruction or stature. And the folks behind Wikipedia are thinking about structured information – a topic that newsrooms should be thinking about, too.\nAnother important part of the newsroom culture is its temporary nature. Different things happen every day, and every day a newspaper has to essentially start over and build a new product in about 12 hours. That places a high strain on just about everybody. It really is something close to a miracle that the paper comes out every day. But this schedule is not a permanent barrier; it can be managed.\nGiven the economic constraints on most papers, hiring more staff is a great idea but hardly a likely solution. The one thing we can begin to improve right now is how we manage our information – the knowledge that reporters acquire, the documents they collect and the data they use. We can train our reporters to start thinking of their materials as part of an archive of knowledge, and give them the tools to create their own archives for their own use at first. Instant collaboration isn’t possible. But starting with, say, a projects team that already needs to share information can spread adoption of these principles and tools. Even at a paper as competitive as the Post I’ve seen this tactic work, albeit in small doses.\nThere are some great ideas out there on the Web that could be applied to newsrooms: take, for example, the concept of Flickr, in which people “tag” their photos with subjects and keywords, making them easy to organize and find. The closest thing we have to that in journalism is in news libraries, where archives sometimes get keywords attached to them. Some reporters know about this when searching Nexis, but most don’t. Too much time is wasted searching our own archives for stories.\nMake no mistake: this will not be easy. I’m well-aware that most reporters still keep much of their notes and documents on paper, not on the computer. A paper-free newsroom is a ridiculous goal – as if we could eliminate paper from society at large. We should, however, encourage reporters to use the tools available to them on their desktops. We should teach them that computers and the principles of the Internet can make sharing and searching easier, and sharing is not just a worthwhile goal, but increasingly a professional requirement.\nAfter all, this should be a golden age for journalism. Never have journalists had access to better tools for doing their jobs, been better-educated or had more ways to get information to more people. Look at the ability of news organizations to gather increasingly larger amounts of information in shorter times, the power provided by computer-assisted reporting and access to records, and it’s hard to imagine that we wouldn’t be turning out increasingly better newspapers.\nWhy aren’t we doing it? Because we don’t manage information well, we’re resistant to change and we haven’t taken advantage of the greatest communication platform ever devised. We use some of the cool Web apps like Google Maps or maybe we have a blog or online chats. But newspapers, by and large, don’t understand that those steps won’t be enough to ensure the kind of journalism that can’t be duplicated.\nThe shame of journalism today is not that it’s no good, but that it’s nowhere near as good as it could be, and that’s largely our fault."
  },
  {
    "objectID": "thefix/the-canvas-for-car/index.html",
    "href": "thefix/the-canvas-for-car/index.html",
    "title": "The Canvas for CAR",
    "section": "",
    "text": "Computer-assisted reporting and newspapers have always been a slightly imperfect match. In general, newspapers provided more support for CAR than have radio or television stations, but as newspaper space is at a premium (and getting more expensive), those stories involving a significant amount of data usually end up leaving much of it behind. Accompanying graphics help in many ways, but they can only do so much. Even when story length isn’t as much of a concern – at some magazines, for instance – the instinct is not to put a bunch of data where carefully produced words might otherwise go.\nStill, newspapers have been the best venue for CAR for many years, mostly because that’s where CAR practitioners have gone to work. CAR can be expensive: instead of a bare-bones PC and notebooks, papers need to spend money on more powerful machines, expensive software and hardware and pay for the time that doing CAR can take. And then the paper prints a summary of the CAR work; most of the rest of it is left behind.\nThe Web began to change that as soon as it became popular. Instead of relying on a paper’s database analysis alone, readers could actually search the data themselves. Organizations like the Center for Public Integrity (where I used to work) have made data a central part of their offerings; the Center’s telecom project puts its Media Tracker database front and center. Some newspapers have begun collecting their databases in a single location; the Louisville Courier-Journal is one example of this.\nOn the other hand, too many papers (my own included) take the printed paper’s restrictions and apply them to the Web. For example, the Fort Worth Star-Telegram’s site has what it calls a “Databank” of building permits and other municipal records. It’s a long list, not searchable, not related to anything. Other than the fact that we print this stuff in the paper, there’s no reason for doing this online. It’s as if we, as an industry, simply seek to do the same thing regardless of platform.\nIt shouldn’t be this way. The Web is the canvas for CAR, better than any other platform we’ve come up with as an industry. It has every advantage that should be available to the CAR practitioners, including unlimited depth, the ability to customize or personalize and the luxury of designing a database so that it will truly be useful to readers. Some papers get this, or are beginning to realize it. Think of USA Today, where that paper’s sports salaries databases not only produce stories for the paper but also help cement its reputation as a premier destination for national sports information. When bloggers and other publishers start using your site as the “standard” for that topic or piece of information, that increases your influence and reach. Go ahead: search Google for “baseball salaries”. What’s the first result?\nThat’s the sort of reach that we’ve been trying to achieve with the Congressional Votes Database at washingtonpost.com (and indeed, if you search for “congressional votes,” that’s usually the top result). Topix.net CEO Rich Skrenta writes about “appropriate discoverability” on the Web, saying that while newspapers produce restaurant reviews and movie reviews, almost none of those are easy to find on the Web. That’s because we’re not, as an industry, taking advantage of the opportunity to collect them in a database and index them properly.\nBut there are so many Web uses for CAR – not just for things like movie reviews, but for investigations and other “hard news” stories. Tracking the activities of government is something that many papers do better than most others (save lobbyists), but most of that expertise is trapped within a few reporters and editors. There might be a dozen stories in a huge government database, but usually papers have time and room for finding and reporting only a few. Opening up access to those datasets for our readers would provide them (and us) with the ability to answer the follow-up questions prompted by an investigative report. It could open new avenues of inquiry and connect us to new ideas.\nThe Web is the most natural home that CAR probably ever will have, and it’s certainly a better platform than print. While there’s much to be said for the ability to distill loads of information and data into a digestible story, why would we leave it at just that when we could keep readers coming back to our sites or staying there for longer periods of time? On many levels – economically, culturally – journalists have had a tough time adjusting to the Web. Some of those difficulties are natural and must still be worked out, and news Web operations themselves haven’t helped by not embracing CAR as they should be. Yet I can think of few other areas in which Web sites get more for less than with those people who know how to use databases. A great deal of Web data runs on open-source applications that cost less and, when properly automated, require less time and effort to maintain than a collection of text that needs to be copy-and-pasted into a CMS.\nThe question of how database content is presented on the Web is still one that has no single answer and remains a work in progress. But the more that we get the CAR people and the Web people together – indeed, the more that the CAR people and the Web people are the same people – the better we’ll get at both making data more accessible and less separate from other Web content. And CAR will finally get the home it deserves."
  },
  {
    "objectID": "then/index.html",
    "href": "then/index.html",
    "title": "What I Did Then",
    "section": "",
    "text": "Most of my previous work revolved around finding, acquiring and using political and legislative data for journalism. That meant scraping web sites for congressional data, building APIs and helping my colleagues build interactives that explore important issues. I used to appear on C-SPAN once in awhile, too.\n\nProPublica, 2015-2021\nAs a news applications developer, I both built data-driven websites and wrote articles. Read them here.\nSome of my favorite work:\n\nHundreds of PPP Loans Went to Fake Farms in Absurd Places\nNew Records Show the NYPD’s Favored Punishment: Less Vacation Time\nThe NYPD Files\nTracking PPP Loans\nHow Fundraisers Convinced Conservatives to Donate $10 Million — Then Kept Almost All of It\nHow Congress Stopped Working\nRepresent\nFEC Itemizer\n\n\n\nThe New York Times, 2007-2015\nAs a interactive news developer and later a writer on The Upshot, I was a member of The Times’ first newsroom web development team and later wrote data-driven stories. Read some of them here. I also helped produce two public APIs for congressional and campaign finance data.\nSome of my favorite work:\n\nMaiden Names, on the Rise Again\nHow ActBlue Became a Powerful Force in Fund-Raising\nLooking for John McCain? Try a Sunday Morning Show\nVoting Totals Reveal Crucial Boost From Blacks in Thad Cochran’s Victory\nA Mysterious Republican Committee in the Virgin Islands\nFreeing the Plum Book\nOutside Spending in Key House Races\nThe 2012 Money Race: Compare the Candidates\nRepresent\nToxic Waters\n\n\n\nThe Washington Post/washingtonpost.com, 2004-2007\nI started at the Post as a research database editor, contributing to stories, and then moved to washingtonpost.com (then a separate company!) to build data-driven projects.\nSome of my favorite work:\n\nD.C. Red-Light Cameras Fail to Reduce Accidents\nGulf Firms Losing Cleanup Contracts\nHill Leaders Often Take Corporate Jets\nThe U.S. Congress Votes Database\n\n\n\nThe Center for Public Integrity, 2003-2004\nAs a database editor and writer, I contributed to several projects by building databases and writing some stories. Read them here.\nSome of my favorite work:\n\nSilent Partners\n\n\n\nCongressional Quarterly, 1998-2003\n\n\nThe Palm Beach Post, 1995-1997"
  },
  {
    "objectID": "archives/2013/03/02/mobile-apps-where-the-data-lives/index.html",
    "href": "archives/2013/03/02/mobile-apps-where-the-data-lives/index.html",
    "title": "Mobile Apps - Where the data lives",
    "section": "",
    "text": "At first glance, I wasn’t sure how useful a mobile app for presidential documents would be. After all, it’s not too often that I am on the go and need to see the contents of the President’s weekly radio address.\nI had found a lot of value in the mobile version of The Plum Book, which provided the data for a New York Times story on presidential appointments. The data hadn’t been easily accessible before the mobile app appeared, instead being trapped in PDFs.\nPresidential documents, on the other hand, can be the white noise of an administration, coming off the production line day after day, summarizing a visit with a foreign leader or a speech at a school or manufacturing plant. And the mobile app offered up the most recent five such documents, not exactly a treasure trove.\nBut then I thought about Mark Knoller, again.\nWhile the Compilation of Presidential Documents (CPD), as it is officially known, isn’t a comprehensive accounting of a president’s time, it is a fairly good indicator of his public schedule and whereabouts. Presidents rarely travel somewhere and not make a public appearance. So I dug a little deeper into the app’s code to see what it held.\nQuite a lot, as it turns out.\n\nJSON Everywhere\nIf you load the mobile app in a web browser and check out its files via the console or developer tools, you’ll find a JavaScript file among them. This file powers the app and contains references to the endpoints that carry JSON from the GPO’s servers.\nThe first line of JS, just after the initial comments, is key. Here it is:\nvar context=\"/wscpd/mobilecpd\";\nThat’s the URL segment that begins each call to a JSON endpoint, and it provided a roadmap to them. I searched the file for every time that context was mentioned, and found a series of URLs, beginning with these three:\nvar dateHomeUrl =context+\"/home\";\nvar mrURL=context+\"/location/\";\nvar allcategoryUrl = context+\"/category\";\nHere’s what that first url looks like once you combine the original context variable and dateHomeUrl:\nsearchResults: [{ location: \"Washington, DC\", index: \"1\", line1: \"Remarks at the National Governors Association Dinner\", line2: \"Compilation of Presidential Documents. Addresses and Remarks. Sunday, February 24, 2013.\", president: \"Obama, Barack H.\", eventDate: \"Sunday, February 24, 2013\", packageId: \"DCPD-201300113\" }, { location: \"Washington, DC\", index: \"2\", line1: \"The President's Weekly Address\", line2: \"Compilation of Presidential Documents. Addresses and Remarks. Saturday, February 23, 2013.\", president: \"Obama, Barack H.\", eventDate: \"Saturday, February 23, 2013\", packageId: \"DCPD-201300112\"},…]\nIn other words, lovely JSON. Since this is a mobile app, there’s a maximum of five results returned over the wire, which certainly makes sense. This isn’t a pure data API, after all. But that doesn’t mean it can’t be used to build up a data collection.\n\n\nRich Data\nThe mobile app has several different options, each of which comes with one or more endpoints: date, location, category and search. The date endpoints aren’t limited to a single date but include a date range option that makes it possible to iterate through various dates or periods.\nThe location endpoints are even richer: they offer the options of passing in a state, a city and state, or a city and foreign country. And then there’s the icing on the cake - location-based distance searches. Let’s say you want to find the recent documents from locations with 50 miles of Atlanta, Ga. You can.\nHow about a keyword search, so you can find all the documents that pertain to the president meeting with the Los Angeles Lakers? Sure, why not.\nAll of these in a mobile app. A government mobile app.\n\n\nSea Change\nIf you had told me a week ago that the federal government was providing an open API for presidential documents that included searches based on location, keywords and dates, I would have been both amused and impressed at your imagination. But it exists, and part of me suspects that this is the way that government officials who support broader access to data can make it happen. Perhaps an official API and developer support is beyond the means of some agencies, but mobile apps - those are shiny, and a way for agencies to provide services for a different set of users.\nThe CPD app isn’t the most likely candidate for a mobile app, but it offers journalists a decent way to track the public schedule of the president, something that we need to get a lot better at. The geocoding of documents alone makes this data worth grabbing. That’s why I wrote the presdocs Ruby gem. Next up: a tracker app based on the CPD (AutoKnoller, anyone?), and then back to looking for the next mobile app."
  },
  {
    "objectID": "archives/2013/03/18/steve-coll/index.html",
    "href": "archives/2013/03/18/steve-coll/index.html",
    "title": "Steve Coll",
    "section": "",
    "text": "I have a ton of respect for Steve Coll, who was named dean of the Columbia Journalism School on Monday. He was the managing editor of The Washington Post when I started there, and was thoughtful, smart and not given to exercising authority for its own sake.\nWhat I remember most about Steve Coll is the first time that we met. Around 4 p.m. on October 4, 2004, I was ushered into a “north wall” office at The Post - the top editors had their offices along the north wall of the building. Inside were Steve and Len Downie, the paper’s executive editor. A joint interview was somewhat unnerving.\nBoth Steve and Len were very polite, even if they didn’t know exactly what I was being hired to do - something with data, basically. They asked me about my background and my work, and then asked if I had any questions for them. And I did, based on discussions I had with friends and colleagues and my experience as a Hill reporter. What I had heard, and had been told quite a bit, was that The Post was a “destination” place for journalism, and that I’d be stupid not to jump at an offer. But I also was told in no uncertain terms that The Post could be a difficult place to work. That some people there were extraordinarily competitive and, to put it bluntly, not very nice.\nSo gently, trying to be very respectful, I asked about this. “I’ve heard from people that I trust that The Post can be … uh, a little mean.”\nThere was a brief pause. I thought for an instant that I had blown it. And then Steve looked at me and acknowledged that it was an issue, but that “I think that you’ll find that it’s a much friendlier place than it has been in the past.”\nAnd I thought to myself, “He’s the managing editor of The Washington Post. He could have basically told me to stick it - they didn’t need me.” But he didn’t - he was honest and upbeat at the same time. He seemed to care that his paper was a good place to work.\nNow, I’m pretty sure that I would have taken The Post’s offer whatever his answer had been. But that one exchange really put me at ease, and made it possible for me to really believe that this not only was a destination, but it was a place for me. Thanks, Steve. And good luck."
  },
  {
    "objectID": "archives/2013/04/28/academy-fight-song/index.html",
    "href": "archives/2013/04/28/academy-fight-song/index.html",
    "title": "Academy Fight Song",
    "section": "",
    "text": "What’s that I hear\nThe sound of marching feet\nIt has a strange allure\nHas a strange allure\nMission of Burma, “Academy Fight Song”\nDisclaimer: I am the son and son-in-law of retired tenured university professors, and have taught data reporting skills for about 10 years at the undergraduate and graduate levels. I love teaching. I have my issues with “education”.\nThis post on journalism curriculum by University of Florida journalism professor Mindy McAdams deserves to be widely read. Not just because it attempts to deal with a growing problem in a responsible manner, but because it is honest about the situation:\n\nStudents deserve better. Regardless of what they think journalism is when they tick the box to become a journalism major (and regardless of whether they’re paying Ivy League prices or in-state tuition at a public university), they deserve to be taught skills, techniques, and ways of thinking that will carry them through the challenging times at hand and ahead. They deserve to have every teacher — tenured, adjunct or freshly pressed Ph.D. — looking at what’s new, what’s happening today, now (what happened in and around Boston last week, for example, as seen in the mainstream news and in social media) and incorporating new methods for reporting, for storytelling, and for engaging the audience into all of their classes.\n\nMindy has been teaching her students this semester about coding for journalism, and has a blog where she has described her experiences and posted her presentations. Here’s a wacky question: why isn’t this stuff (blog posts, presentations, videos) on a ufl.edu site? Maybe it makes no difference at all to students, but why wouldn’t the UF Journalism school want to be all over this stuff?\nMindy’s post is a response to an earlier one on Poynter by Tom Rosenstiel in which he argues that journalism education was, in fact, experimenting in the kinds of ways that are needed for the future of the profession. He cites, among others, Robert Hernandez at USC and Esther Thorson at Missouri as doing important work in the frontiers of journalism.\nFair enough, although I’d argue that rather than “barely scratching the surface”, Rosenstiel probably couldn’t generate a list of people doing innovative work in academia that was all that much longer than what he published. Katie Zhu, who is pursuing a double major in computer science and journalism at Northwestern, writes that she has had one professor – at Medill – who she can honestly describe as a person who “get[s] it.” Think about that. Here we have a bright, talented and ambitious student who wants to put herself at the forefront of journalism education and has to do it by herself at one of the elite journalism schools in the country. As Katie says:\n\nLet’s re-architect the distribution requirements so we can get rid of this notion that journalism and engineering (and the sciences) are at odds with each other. Because journalism needs to move forward.\n\nThe unspoken part there is that, if this keeps happening to enough people (and there will be more), journalism will move forward without journalism education, or worse, talented would-be journalists will seek to work in other professions. Katie has a job lined up at Twitter.\n\nAnti-Intellectualism’s Roots\nPart of the problem is that some of these technical areas and concepts are alien to many faculty, and part of it is that it’s sometimes hard to attract experts from the profession to full-time academic work. Part of it is that curriculum change is insanely harder than it should be, and that accreditation is so super important – to other academics, at least. Part of it is the tenure system, which results in disproportionate power within departments, power worth fighting to protect. And part of it is also that the academy has done a pretty terrible job of selling whatever academic products it creates, as well as pulling from the best the rest of the university has to offer.\nFor this, some blame (but only some) can be apportioned to the audience. As Rosenstiel writes:\n\nFor years, journalism was marred by an ugly streak of anti-intellectualism — the denial of theory, the exaltation of craft, the repudiation [sic] professional identity, ignorance of scholarship. One byproduct of the crisis in journalism is that anti-intellectualism is giving way to something better at schools where practitioners and scholars work together to create a new curriculum.\n\nA fair point, but it goes a bit too far for me. For one thing, this anti-intellectualism that Rosenstiel identifies - surely that hasn’t been helped along by educators who tolerated and even encouraged the fiction that journalists “don’t need to know math” to do their work? As prevalent as that sentiment is in newsrooms, would it have become such a staple of journalism if there had been broad pushback from journalism schools? It’s fairly telling that John Allen Paulos’ website is on the math.temple.edu server, because it sure as hell wouldn’t be found on a journalism school’s site (to be clear: this isn’t a shot at Temple specifically, but at the academy generally. Temple has actually even changed its curriculum.). The very fact that journalism schools have been looking outside their walls to address the industry’s current problems says an awful lot about how pervasive this anti-intellectualism really is, and what its sources really are.\nWant more evidence? Let’s talk about this professor of practice thing. Or maybe these “general manager”-type positions in collegiate newsrooms (I have at least two friends, both former journalists, who are in these types of positions at prominent journalism schools). People: these are teachers, just as surely as any PhD on the faculty. Yes, it might be easier from a bureaucratic standpoint to get a “professor of practice” into the department, but I’m willing to bet a decent chunk of change that in the eyes of too many “real” professors, these people are not in a similar line of work or are doing it the wrong way. The people who have tried hard to get new ideas into the J-school are (mostly) dedicated and good people who see what is on the horizon. The people who resist are (mostly) dedicated and good people who have too much power and independence to give it up without a fight. This would be one hell of a good story, if only most journalism students were permitted to write about it.\nA case in point: a few years ago I was invited to come to Northwestern for a week to speak to Medill faculty and students about the kinds of work we were doing, and to hold training sessions on basic data reporting concepts for the faculty. That I was there at all was hugely encouraging to me, and I met a number of faculty who, although inexperienced in this area, were excited about learning. And then I learned the catch: the training sessions were optional, because why would it be otherwise?. I struggle even now to imagine any journalism course in which the professor should remain ignorant of basic digital reporting knowledge. Great message for the faculty, let alone the students.\n\n\nIt’s Academic\nThe professional ignorance of scholarship thing, though, is certainly right from the perspective of someone who has worked in newsrooms since 1995 and has taught college classes for most of the past 10 years. I don’t read many academic journals, nor do I attend AEJMC - go ahead, Google it - or other journalism education conferences, and I don’t know many professionals who do. Far worse is the fact that I would find it hard to read those papers anyway, since most of them are unavailable online without a subscription of some kind. Sure, we’ve been ignorant of scholarship. But I’m not sure that scholars have done what is truly necessary to improve the relationship between trade and education.\nAbout a year ago I was involved in a brief Twitter episode involving a draft academic paper that mentioned Fech. Whatever my issues with the paper itself, there is a crazier undercurrent: one of its authors didn’t know that it was posted on the Web, and as I learned, there was little chance that anyone outside academic circles would ever see the final paper. The authors very kindly sent me a revised version, but of course I can’t share that with you, and neither can they, legally. Which makes no sense at all. Academic authors are literally only publishing for themselves and their colleagues, and we can’t understand why the industry is so separated from the academy?\nOf course the Internet is trying to undercut this closed system. There’s academia.edu, which as near as I can figure is like a Napster for journal (and other) articles, and it looks to me as if professors post their articles there even if they technically don’t have permission to do so. It would be funny if it weren’t sad, or if the news business wasn’t facing some pretty severe challenges. The kind of challenges that universities might be able to shed some light on through smartly designed experiments and research. I’d be interested in reading more about those kinds of things directly rather than looking at pages like this and wishing there were more links.\n\n\nThe Hard Part\nThe gap between journalism and journalism education seems pretty big, although I think it’s hard to measure. Chalk at least some of that up to the peculiarities of both the profession and of academics in general, but there’s one factor that looms large: most other fields of study are not facing such a fundamental upheaval. It seems like this would be a pretty good time for there to be tighter collaboration between the industry and those who study it, or at least a reshaping of what those studies look like. But where to start?\nChanging accreditation and tenure practices would be dramatic, but it’s hard to imagine either being done, both within journalism schools and across the academy more broadly. Even though I’ve never heard any journalism school graduate say that accreditation was any factor whatsoever in her decision to enroll, who wants to remove their name from the list? If one university (or department) changes its tenure practices, surely its competitors would use that against it when recruiting faculty.\nSome prominent journalism grant-making organizations have urged the adoption of the teaching hospital method, which makes some sense, although I’m not certain that the faculty exists to actually fulfill that mission at most schools, and perhaps it shouldn’t, given the mission of performing and publishing research, too. As much as I enjoy being an adjunct, a too-heavy reliance on such teachers is a bad thing for students. I know that my students don’t get my absolute best effort, owing to my day job, and I also am not a contributor to the academic life of the university beyond the class I teach.\nTwo things seem slightly more achievable to me, although every school is different: more flexibility in curriculum requirements, and more accessible scholarship. One starts to address the concerns of students like Katie and professors like Mindy. The other might start to repair the breach between the academy and the industry that serves neither well."
  },
  {
    "objectID": "archives/2013/04/24/the-itemizer/index.html",
    "href": "archives/2013/04/24/the-itemizer/index.html",
    "title": "The Itemizer",
    "section": "",
    "text": "Here’s a little secret about me: I love campaign finance data.\nOk, so that’s no secret at all, and it’s probably unhealthy in some respects. But let’s face it, campaign finance data provides a rich source of information for stories, blog posts and tweets, in rough descending order of size and scope. There’s one thing that has always bugged me about how we reference campaign finance data online: the best that most of us can do when we link to a campaign filing is to link to a particular page, whether that’s a list of contributors or a summary page. Yet often we’re referencing a single transaction or line-item.\nI started to think more about this after listening to David Cohn talk about Circa at the Journalism Interactive conference in February. Circa “is creating the first born-on-mobile news experience, delivering it in a format native to mobile devices, with an experience intuitive to mobile users,” and part of that means breaking down what we think of as stories into their essential components. Those usually are statements or facts of some kind that can readers into a larger story. This “atomization” of news, as David called it in his presentation, focuses tightly on the core elements of a story.\nFor people who work with data and love to cite it in stories, this is a good idea. It got me thinking about how we could do that for campaign finance data, where transactions are the smallest particle. What I’ve come up with, with the assistance of Aaron Bycoffe, is The Itemizer, a simple app that takes itemized records from Federal Election Commission filings and allows you to link to them. If you want to specifically reference the $1,000 that the Society of American Florists PAC gave to Nydia Velazquez, now you can.\nWhen I say simple, I mean it. It’s a Rails 3.2 app, but it could just as easily be implemented in Sinatra, given how small it is. The anchor links are based on the transaction IDs from the filings, so there’s no need to invent unique hashes or anything. Thanks to the Campaign Cash and Fech Ruby gems, the page loads today’s filings and then constructs links to receipts and expenditures (other schedules are coming). Nothing is stored in a database - Campaign Cash pulls in the day’s filings from The New York Times Campaign Finance API and Fech processes individual filings on the fly. Although the default is to today’s filings, if you know the filing number, you can plug it and the schedule into the URL and it’ll just work.\nAlthough it was relatively simple to build and deploy (thanks, Heroku!), I don’t expect the FEC to do something like this anytime soon, although I’d be happy if they did. More certainty, tighter focus, more direct references – these are good things for journalism. If you have ideas on improving The Itemizer, please join in. Or better yet, build one for a similar data set that you work with so even more people can benefit."
  },
  {
    "objectID": "archives/2013/05/21/what-they-say-about-us/index.html",
    "href": "archives/2013/05/21/what-they-say-about-us/index.html",
    "title": "What They Say About Us",
    "section": "",
    "text": "As journalism buzzwords go these days, analytics has a lot going for it. The term is broad enough to encompass a wide range of ideas, ranging from the “dark side” where the application of analytics leads to bad and banal writing to the use of analytics by reporters to unearth details of breaking news. Knowing who views our work and how they interact with it, while not a savior of the industry, seems like something we should be paying attention to and incorporating into newsroom processes.\nBut I’m also interested in another form of analytics: how other people cite, describe and otherwise use the information we publish. Yes, the number of Facebook likes or Twitter retweets that a story gets can tell you something about how popular it is. But what about the people who take the time to describe a story on social media in their own words, or pick their favorite quote? Aren’t they arguably more invested in it, maybe even personally involved?\nA few weeks ago, my colleagues in Interactive News at The Times held a hack day focusing on analytics, and built some really interesting and useful stuff that will help improve our internal knowledge of our own applications and how users are interacting with them. My own contribution was much less technically impressive, and we’ll get to that in a bit. But it scratched an itch that had started several years ago when I saw linkypedia, a site built by my friend Ed Summers. Ed works at the Library of Congress, which means he cares a lot about not just preserving information but also making it easy to reference. He also, via his work on a historic newspaper preservation project called Chronicling America, knows a bit about the news industry.\nThe idea behind linkypedia is that links on Wikipedia aren’t just references, they help describe how digital collections are used on the Web, and encourage the spread of knowledge: “if organizations can see how their web content is being used in Wikipedia, they will be encouraged and emboldened to do more.” When I first saw it, I immediately thought about how New York Times content was being cited on Wikipedia. Because it’s an open source project, I was able to find out, and it turned out (at least back then) that many Civil War-era stories that had been digitized were linked to from the site. I had no idea, and wondered how many of my colleagues knew. Then I wondered what else we didn’t know about how our content is being used outside the friendly confines of nytimes.com.\nThat’s the thread that leads from Linkypedia to TweetRewrite, my “analytics” hack that takes a nytimes.com URL and finds tweets that aren’t simply automatic retweets; it tries to filter out posts that contain the exact headline of the story to find what people say about it. It’s a pretty simple Ruby app that uses Sinatra, the Twitter and Bitly gems and a library I wrote to pull details about a story from the Times Newswire API.\nSeems to me that the more we know about how our content is cited, the more we might be able to find out about our users or how they value the journalism we produce. Maybe there are product ideas in that knowledge: if people are relying on our stories for certain purposes, it’s possible that we could create custom packages and charge for them. At the very least, such insights on social media might give us an idea of how people spread stories, and maybe we can learn from that. Tracing links are just a start; we could do the same for entire passages, appearances by New York Times staff in other media and more. Another project of mine uses the Capitol Words API from the Sunlight Foundation to track mentions of The Times in the Congressional Record. If we’re lucky, people talk and write about us a lot. We should be listening and watching for what they say."
  },
  {
    "objectID": "archives/2013/10/09/data-journalism-student-media-edition/index.html",
    "href": "archives/2013/10/09/data-journalism-student-media-edition/index.html",
    "title": "Data Journalism, Student Media Edition",
    "section": "",
    "text": "I had the privilege of speaking to students (and some faculty) at Duke University on Monday, and it was inspiring to see so many people come out to listen to a very geeky talk, to say nothing of the speaker. Afterwards, several students came up to ask how they could start doing data journalism at a student newspaper, particularly at a private university not subject to most public records requests. If I’m going to encourage student journalists to embrace data journalism, it’s only right that I try to provide some suggestions on how to do this in a university environment.\nFirst suggestion: as much as you’d like to address issues of national significance - and I totally did this when I worked at both The Pitt News and The Independent Florida Alligator - stick to what is right in front of you. Universities large and small are communities themselves, with often strange and interesting methods of governance and communication. They make for great stories. With that in mind, start with your university’s Institutional Research office. It might be called something else, but nearly every school has something like it. The IR office’s job is to collect information - data - about the school, its students and faculty, and to publish reports on the same. IR offices often conduct surveys, too, which can tell you what kinds of questions are important to the university (some even do surveys of parents).\nSome are pretty good about publishing data as data, usually a spreadsheet. Other times it’ll be a PDF or maybe even a Word doc. It doesn’t matter - you’ll want to build your own set of data about the university, and you won’t want to restrict yourself to only what IR offices are interested in. Start with a spreadsheet - maybe Google spreadsheets, so multiple people can access or edit it. Consider this a resource for your entire news organization, not just you. When you leave, make sure someone else takes over the care and feeding of it. Make it so that your student paper or radio station is an expert in the demographics of the university.\nAs you build up a collection of data, you’ll be able to write actual trend stories about incoming classes or the number of full-time faculty at the school. Instead of having to recompile the same information over and over for different stories, you can do it once and use it all over the place. And it’s not like the university can say that the data isn’t correct; they can, but they’ll need to correct it.\nSecond suggestion: Follow the research money. Higher education is now a complex combination of students, faculty, administrators and the funding that makes it all possible. Faculty members hope to bring in research money to pursue their interests and to enhance their profiles. While a not insignificant portion of that research is funded by private sources, the government pays for quite a lot of academic research. Student media outlets should know about it. After all, the occasional story about a faculty research grant is a common thing for a lot of campus newspapers.\n(Note: were the federal government not shutdown at the moment, I’d provide some links to sources of research grant information and data. But you’re journalists, you can find them.)\nThird suggestion: Decide what 2-3 things that your university does that makes it regionally or nationally known. This may include athletics. Figure out how to dive deep into those subjects, including collecting whatever data is available. If sports is on the table, then track plays or players or coaches or all of the above. If it’s a particular school or department, gather up all the details on faculty, classes, students, etc. Ask students and faculty what they’d like to know about those things, or what they think they already know. Own your home court, basically. That should be something Duke students, at least, find familiar."
  },
  {
    "objectID": "archives/2013/10/01/the-natives-arent-restless-enough/index.html",
    "href": "archives/2013/10/01/the-natives-arent-restless-enough/index.html",
    "title": "The Natives Aren’t Restless Enough",
    "section": "",
    "text": "A couple of points to start with, in the hopes of not wasting readers’ time and preparing for some reactions:\n\nThis post could be named “Get Off My Lawn” or “In My Day, We Earned It!” or some other title that would demonstrate that I am, more than ever, a cranky old guy now.\nWhat follows obviously does not apply to everyone younger than I am (42, for the record). But it does apply to a distressingly large percentage of those who I have taught at the university level (undergraduate and graduate) over the past eight years.\nThis post stems from one sentence in a Poynter summary of a MOOC I helped teach, but this is not about the author of that piece. It’s about a larger mindset that I see.\nYouth is wasted on the young. (See point #1, above.)\n\nThis post could be about how slow, feature-less and pathetic the Internet tools and sites I first encountered were compared to the ones we have now, but as we know too well, nostalgia isn’t a selling point. It could be a screed about how kids today don’t appreciate what they have, but I’ve tried that on our seven-year-old daughter with minimal results. Instead, this is about the very real situation I see in classrooms and among too many people who describe themselves as “digitally savvy” or “digital natives”.\nI really enjoyed the data journalism MOOC itself and was glad to see the Poynter write-up. It was my first involvement with MOOCs and it was a fascinating experiment. But there was one line from the Poynter piece that brought to the surface a couple of thoughts I’ve been harboring for awhile about teaching journalism to people who have grown up with the Web. Here it is:\n\nI’m a millennial and therefore digitally savvy, but I could feel sweat bead across my forehead while reading Matt Waite’s account of scraping government data to create Politifact.\n\nThe term “digitally savvy” (or, worse, “digital native”) is more flash than substance, but let me take a stab at what it is supposed to refer to: individuals who supposedly emerged from the womb with a facility for computing devices, the Internet and (especially) social media. They speak (type) an abbreviated language mystifying to their elders, have some different theories on the notions of sharing and privacy and have been exposed since an early age to what is surely much more information than any generation in humanity’s existence.\nLet’s stipulate up front that the facility with devices thing is very real and very much worth paying attention to, for lots of reasons. That our then-three-year-old daughter could unlock and control an iPhone with greater ease than my parents could has broad implications for device manufacturers and those who write software for them. It is also an example of the accumulated baggage that learned habits and expectations can bring. Those are interesting, serious issues that deserve study. But that’s not the problem that bugs me.\nThis is: too many journalism students and journalists are native users rather than actual natives. The difference is enormous, and has real implications. Actual natives can build in addition to use digital tools, giving themselves many more opportunities to make better journalism. Users can only work within the constraints that other people set.\nWhen I’m teaching classes or looking at resumes of journalism students, I see a lot of this kind of thing, usually on a hosted WordPress install:\n\nSkills: WordPress, Microsoft Office, social media\n\nThose are useful skills, to a point, but if you’re coming out of journalism school and those are your big technical skills, congratulations: you’ve just joined nearly all of your peers in almost every discipline. You can use TweetDeck? Great. Did you hand-code at least part of your “professional vanity site”? Do you actually know how the Internet works? I’ve come across too many journalism students who believe (or have been led to believe) that they are technologically adept, only to find that when it comes to how things online actually work, their knowledge is broad but very shallow. For example, I routinely find that most of my students are unaware of search engines’ advanced search capabilities, let alone that computers have powerful command-line interfaces.\nMost of my students are not what I would call digital natives. They are more like users, and while journalism needs users, it desperately needs creators. Otherwise we all end up using what other people make, and we end up making fewer great things for our readers.\nWhen people see themselves as users, they grow nervous at tinkering with the building blocks. They actually shy away from learning, because learning can be so much harder than just using a finished something handed to you. It’s ok to be nervous about new concepts, but what I’ve seen increasingly from journalism students is an incuriosity that borders on indifference. Part of this is the sweet lull of search results, something we all fall prey to. You know about this: you search for something and if it’s not in the first page or two of results, you presume it’s either a) unknowable or b) too hard to get at. The end result is a feeling of powerlessness, like this from the Poynter piece:\n\nWhat I hated about week four (sorry Jeremy, you’re still amazing) was how paralyzed I felt. These data journalists and news developers seemed so far ahead. How were the rest of us with limited coding skills going to grapple with foreign languages like Ruby on Rails?\n\nI’ve talked to journalism instructors who have experienced this and it seems to be one of the most frustrating things out there. Maybe the worst moment I’ve had as a journalism teacher is when a graduate student asked me, in the middle of class: “How are we expected to learn if you don’t teach us?” My first thought, which I safely kept unspoken, was: “How did you make it this far?” Because I don’t think that sort of approach is likely to lead to a journalism career that is successful or enjoyable. I’m pretty sure that if journalism ever needed incurious people, it surely cannot survive with them now.\nThe best journalists (and students) I know force themselves into uncomfortable situations when it comes to learning. They are not content to be handed information and just accept it. They want to explore, to figure things out, even when there’s some risk of failure there. There are examples from the “digital native” generation, folks like Greg Linch, who wasn’t content to use WordPress but dove into ways to extend it and fit it to journalism’s needs. Or Michelle Minkoff or Heather Brady, two former students of mine. I’ve never known any of them to say, “That looks hard and I don’t know anything about it, so I’ll just avoid it.” Paralysis ensures that you’ll stay where you are. The very worst that could happen if you choose to dive in is that you fail and, in failing, learn something.\nMany journalism students might rightly point out that their curriculums didn’t exactly demand this kind of approach, and to the extent that’s the case, that’s not their fault. But it is still an excuse, in the sense that a flawed curriculum can’t literally stop you from learning. So what can you do? Try things. Some suggestions:\n\nIf you use WordPress, try to write an extension. Or take a theme you like and build a flat HTML site of your own.\nIf you know how to use Excel, try learning a database like SQLite (it’s in your phone and the Firefox browser).\nOpen the Terminal on your Mac laptop. Learn some commands in order to do things like connect to the web. You can do much of the same stuff on Windows, believe it or not.\nSpend more time reading the source of web pages. Copy the parts you like or don’t understand, and study them.\nDownload your Twitter archive and analyze it in Excel or SQLite.\nLearn how to connect to the Twitter or Facebook API and retrieve information from it, even in a web browser.\nPick a journalism problem you have (maybe a site you check often doesn’t have an RSS feed) and try to solve it.\n\nNotice that I didn’t say, “everyone must learn to code.” I don’t advocate learning to code for its own sake, or for everyone. Have a problem in mind, or a story you want to tell. If you need code to do that, then there’s your opening. For the sake of your own career, if not the state of the industry, what you should fear are points in the process of doing journalism where you have to say, “I don’t know how to do that” and hand your story - the story you care about - off to someone else with no understanding of what they are about to do. That fear should motivate you to learn, and learning is contagious. More importantly, you should want to learn, to be excited about learning. Be curious. Newsrooms are filled with restless people looking for great stories, wondering how to explain the world. How can someone be “digitally savvy” and yet filled with dread about exploring the digital world?\nBuilding isn’t just more useful than being a user, it’s also more empowering. Of course there are times when building and creating is filled with frustration, but in most cases that frustration is your own. You have a better chance of resolving that feeling than if you sit around waiting for someone else to solve your problems for you. Journalism needs problem solvers and creators so that we can make users out of more people. Next time you encounter a new and unfamiliar technology or skill, take that nervous energy and use it to improve yourself. We’ll all be better off if you do."
  },
  {
    "objectID": "archives/2013/07/02/what-good-is-dat/index.html",
    "href": "archives/2013/07/02/what-good-is-dat/index.html",
    "title": "What Good is Dat?",
    "section": "",
    "text": "Max Ogden, a one-man band of interesting civic (and other) coding, has a new project called dat. Dat is “a new initiative that seeks to increase the traction of the open data movement by providing better tools for collaboration.” In practical terms, dat will consist of a set of tools to “store, synchronize, manipulate and collaborate in a decentralized fashion on sets of data, hopefully enabling platforms analogous to GitHub to be built on top of it.”\nGreat! More data tools! Or: Great. More data tools.\nGithub’s Ben Balter falls into the latter camp, asking why more tools for government data are needed: “It’s the culture that we’ve go to worry about. Will the solution be adopted by the agency? Will existing habits subvert the cool thing you just made? Will the dream live on once you’re at your next gig? If you ignore culture, even the most elegant of technological solutions can be relegated to a dusty shelf as nothing more than the right solution to the wrong problem.”\nBen has worked inside government and outside it. He’s not uninformed when it comes to this stuff. And he has a fairly accurate assessment of the whole government data scene:\n\nI watch day in and day out as many of my former colleagues fight tooth-and-nail trying to convince well-meaning government bureaucrats to toss a scrap of government data over the firewall. It’s a tiring process. After all, whack-a-mole is, by definition, a loosing (sic) game. But the answer’s not Yet Another Mallet, nor is it to give up and build our own mole management solution. We should be making it dumb-simple to do the right thing. We should be building really, really boring stuff. The more boring the better. In many cases, we probably shouldn’t be building anything at all. This is one of them.\n\nMax provides a response that addresses why git and Github don’t fit the bill for this project, but his response isn’t particularly relevant to my interests anyway. Ben’s right: the technology isn’t the issue. But that’s why I think dat is a good thing: because the best part of it, the thing that makes it exciting to me, is boring but essential and not really about the technology. It’s not the tools to store or synchronize the data. It’s not even the bit about transforming it into more useful formats.\nIt is the simple idea that if enough people use dat to explore government data, they will find the best and worst of it, document it and ultimately produce better things. This has been the case in my experience with campaign finance data, which requires solutions for storage, synchronization and transformation. Those of us who work with it have our own approaches, but the best things happen when we collaborate on our methodologies. We reduce the chance that any of us display or report figures that conflict with each other, and crucially, we learn more about the data in doing so.\nThat’s what I’m looking forward to in dat - a true platform for collaboration that enables users of government data to annotate it and to share their experiences in the hopes of creating a set of best practices for individual datasets. Maybe that won’t happen, or maybe the technical aspects will prove more popular than the collaborative ones. But the more opportunities for collaboration, the better, and I’m glad to see another one underway."
  },
  {
    "objectID": "archives/2013/07/16/why-develop-in-the-newsroom/index.html",
    "href": "archives/2013/07/16/why-develop-in-the-newsroom/index.html",
    "title": "Why Develop in the Newsroom?",
    "section": "",
    "text": "Nearly a decade ago, I sat with a group of Python developers around a table at a Vietnamese restaurant in Washington, D.C.’s Cleveland Park neighborhood and told them what it was like to work at a newspaper.\nEvery day, I said, we go out and collect as much information as we can about possible stories, but only some of that information actually turns into stories and only some of those stories get published. Then we wake up the next day and do it again, largely from scratch!\nThey were horrified.\nNews organizations are inefficient, often far too cavalier about the raw materials that provide their lifeblood and can seem to lurch from story to story without a whole lot of reason. Many journalists have a bizarre fear of math and computers, and guaranteed a secret ballot, a decent portion might opt for a return to the days of typewriters and afternoon editions.\nBut all of this hides a beautiful opportunity, an underbelly filled with ever-changing stories and challenges, and a chance to make an impact beyond the web. For some developers, especially those who are engaged with the world beyond Github, newsrooms are quite a natural fit, even if you’ve never thought about it.\nWhy? Because we are storytellers. When you’ve solved some tricky bug in your code, what do you really want to do? Tell someone about it. So you blog, tweet or even walk over to someone else and begin the story: “You won’t believe this thing I just had to deal with!”\nWe’re seekers. You find something on the web that’s amazing or tragic or wonderful or confounding, and you want to share it: “Hey, you have got to see this.” And then you want to find something else.\nNewsrooms also attract misfits, in the best possible sense of the word. You’ll find people willing to ask questions until they find answers, who want to understand things and try to pry the facts out of sources, whether those sources are other people, documents or data. Finding interesting things and telling people about them is at the very heart of what we do.\nHere’s where you come in, though: we have too many things to look at, too many places to find information and a variety of audiences to satisfy. We need the help, whether that’s in making it easier for reporters to find the news in reams of text or providing a way to track what keeps readers coming back. We do quite a bit of our work in the open, as you’ll discover once your name is in a byline. Our users react.\nIf you’re interested in social applications, consider that newsrooms have been collecting birth, wedding and death announcements for years. They have been piling up information about crime, schools, elections and sports team for decades. Sure, a lot of it that data will be unstructured, or worse. But imagine being able to tell the story of a landmark in a new way, or being able to create the app that settles all arguments about your favorite sports team.\nAnd if you’re interested in contributing to our shared civic life, where we learn about the issues that define us and our future, there are few better places to be. We are not campaigners in the usual sense, but our mission is a better-informed and active citizenry, and newsrooms have a built-in platform for driving that effort. We do things that are not popular in the conventional sense but are necessary for a free society or shed light on an important issue. Newsrooms are about war and peace, laughter and pain and every aspect of our world.\nLook, newsrooms have issues to spare. I won’t pretend that many of them aren’t prone to hesitation and at times resistant to new ideas. They have loads of debt, of the monetary, legacy and technical varieties. Our audience is being replaced while we publish, with few guarantees that today’s readers and listeners will be there tomorrow. But newsrooms are also a gateway to a wider world and a broader understanding of who we all are together.\nIf you have solid web development skills, chances are you’re not hurting for employment opportunities. There are plenty of organizations, large and small, established and new, that want to build things for the web, tablets, phones and more. So why come develop in a newsroom? Sure, we need you. But you can be a witness to and a participant in extraordinary events.\nInterested? Check out the Knight-Mozilla fellowships, which place talented developers in newsrooms that want and need your help, and consider applying."
  },
  {
    "objectID": "archives/2013/08/22/teaching-hospitals-journalism-education-and-a-hatchet-job/index.html",
    "href": "archives/2013/08/22/teaching-hospitals-journalism-education-and-a-hatchet-job/index.html",
    "title": "Teaching Hospitals, Journalism Education and a Hatchet Job",
    "section": "",
    "text": "Donica Mensing and David Ryfe from the Reynolds School of Journalism at the University of Nevada, Reno, have published a paper that attempts to argue that recent attempts by foundations and others advocating for a “teaching hospital” approach to journalism education, while well-intentioned, is actually some sort of “back to the future” plan that would enshrine the journalistic norms of the 1960s and (I am not making this up, I swear) “could actually slow the response to change” by journalism schools.\nThree questions come to mind: First, could anything really slow the response we’ve seen so far from academia? Second, is that what the teaching hospital model is really about? And third, this is scholarship?\nThe paper is not purely a critique of the teaching hospital method, although it surely approaches that task with fervor. It also advocates for an “entrepreneurial model” of journalism education, saying that “the groundswell of grassroots activity in civic entrepreneurship, social innovation, user centered design and related practices represent a reservoir of potential energy that could invigorate journalism education.”\nAh, buzzword bingo, that hallmark of well-grounded research. Tell me, exactly, how “user centered design” and these other concepts cannot be taught through doing, only learned through the entrepreneurship process?\nThe easiest way to win an argument is to make the other side so unappealing that no rational person would choose it. This is done through a variety of methods: the use of straw men, unsupported claims and other tactics common to, say, Lee Atwater. An important part is to attribute things to your opponent that sound awful but possible. That’s what Mensing and Ryfe do in this paper.\nIn describing the teaching hospital model of journalism education, they write that it:\n\n“implies that journalism is a settled profession with clear boundaries that needs only to be practiced more rigorously, instead of a field with its most fundamental premises unraveling”\n“makes it hard for students to think differently”\nargues that “the core of journalism is sound and the problems faced by the news industry are primarily technological and economic”\n“forecasts a future for journalism that is infused with the practices of the past”\nassumes that the answers to the question of how to add value to democratic community are “fixed and eternal”\ncalls for “saving the journalism industry by turning journalism schools into production facilities staffed by industry professionals who have left an industry in deep distress”\n\nJust two of those claims has a cited reference (one is a paper by one of the authors). The reader is led to believe that, having surveyed the teaching hospital model, the authors have found that it is regressive, inflexible and hopelessly tied to the past.\nThat sounds pretty bad, right? Is there some real evidence for those assertions? Did the authors conduct a survey of journalism school practices and outcomes? Unearth a statement by the “teaching hospital advocates” that plants a flag in the soil of regressive education practices?\nNo, they didn’t. Here’s what Eric Newton, an author of the teaching hospital open letter linked above, had to say about the teaching hospital model:\n\n“Teaching Hospital” Model: A model of learning-by-doing that includes college students, professors and professionals working together under one ‘digital roof’ for the benefit of a community. Student journalists provide news and engage the community in innovative ways. Top professionals support and guide them. Good researchers help design and study their experiments.\n\n\nIf it’s old school, it’s not a teaching hospital. If it doesn’t include research, it’s not a teaching hospital. If it doesn’t engage the community, it’s not a teaching hospital. If professors and professionals don’t set aside their long-standing arguments and work together, it’s not a teaching hospital.\n\n\nTeaching hospitals help invent and test new things. That’s why a group of funders said at this year’s journalism education convention that we are working on a micro-granting competition. The grant program, to be announced later this year, will support “live news experiments.” We hope that good examples will clarify what good journalistic teaching hospitals should do.\n\nAt this point, I should say that a project that I co-founded is a recipient of money from the Knight Foundation, where Eric works. I’ve never met Eric, nor have we communicated on the subject of journalism education.\nIf you read that definition (and the open letter) it might strike you as not at all what Mensing and Ryfe describe. Yet somehow they’ve cast it in this rather negative light. As Carrie Brown-Smith, a professor at the University of Memphis, put it: “I’m just unclear who or what is actually being rebutted here.” You and me both, Doctor.\nThere are no reasons why teaching hospital models cannot be “entrepreneurial” in the manner described by the authors. Indeed, there are examples of this that combine the best ideas of both models. I am not aware of anyone advocating for a regressive teaching hospital model the authors describe. If they are, they do not cite any in the paper (and if they know of some, why wouldn’t they cite it?).\nTo the extent that journalism schools are failing to keep pace with the very real changes in the practice of journalism (professional or not), it’s very hard to see how the involving more professionals in the process is a harmful thing. But that’s not what the authors argue against - they write that teaching hospital advocates are seeking to replace existing faculty members with professionals “who have left an industry in deep distress”.\nLet’s be very clear about this: there certainly are people who have left the industry to teach who are not well-suited to prepare students for doing journalism today. But if anything, a program in which such people actually worked alongside students and other faculty members in producing modern journalism would give them an opportunity to refit their skills and mindsets. The authors suggest that an entrepreneurial approach would attract non-journalism instructors to the process, but it’s not clear that current faculty would accept this or that such outside experts really want to teach journalism.\nIn a comment discussing the paper on Nieman Lab, Mensing writes that “In the end, it’s not an either/or argument. It’s an AND argument – we need teaching hospitals AND other educational models.” But that’s not at all what the paper says. The paper characterizes the teaching hospital model as a relic from the past with a few redeeming values (“a handful of well-funded schools may benefit,” they concede), but the system they describe is not the same one that its proponents have outlined.\nI learned journalism by doing, with some useful assistance in the classroom. Almost all of my important lessons about journalism came while practicing it, and almost all of my creative learning happened that way, too. There is nothing wrong with injecting some entrepreneurial life into the teaching and doing of journalism. There is something very wrong with claiming that the teaching hospital model would be harmful for students without actually verifying that claim.\nThis paper sets up the teaching hospital model as a straw man standing in the way of a more flexible, creative and successful model of journalism education, and proceeds to cut that straw man down in order to make the other model look better. The authors write that they wish to “build on this conversation” surrounding the future of journalism education, but instead the reader is treated to a lesson in how to win an argument by stacking the deck.\nMensing, to her credit, seems to pull back a bit from the aggressive critique of the paper in her comment on Nieman Lab, writing that “I guess the next logical step is to stop arguing and actually study what j-schools are doing so we learn what really is most effective for teaching.” Wait, you and Ryfe wrote the paper before studying what j-schools are actually doing? That’s not scholarship; it reads more like a hatchet job."
  },
  {
    "objectID": "archives/2013/01/23/magic-removal/index.html",
    "href": "archives/2013/01/23/magic-removal/index.html",
    "title": "Magic Removal",
    "section": "",
    "text": "Few may remember this, but Aaron Swartz kickstarted Django’s magic-removal effort years ago. Django is 1000x better thanks to his feedback. — Adrian Holovaty (@adrianholovaty) January 12, 2013\nAfter Aaron Swartz’s death earlier this month, my friend Adrian wrote that on Twitter as a way of explaining what kind of software developer Aaron was. The ticket that Aaron filed that led to removing the magic from Django is classic Swartz:\nMy biggest issue with Django right now is that it’s so hard and heavy to get started….Imagine how many more people would just start using Django, instead of putting it off or giving up. Imagine how people would start using bits and pieces of Django everywhere, instead of just their website.\nNot all of Aaron’s suggestions made it into the “magic-removal” branch of Django, and eventually into the main codebase. But his goals - simplicity, a closer understanding between programmer and tool - were enormously influential, as Adrian says.\nMy friend and colleague Dan Chudnov, a library hacker who met Aaron back in 2001, brought up the magic removal example today at an event he organized to talk about Aaron’s life and work. It struck me, sitting there, just how appropriate the phrase was. When you think about it, much of Aaron’s work involved removing the magic.\nWe live in a complex time, with more complicated structures and procedures and relationships. There are so many things that we do not understand that we could, if the barriers in the way could be taken down. If we could only remove the magic that keeps us from understanding how things work. I think Aaron tried to do that, by making as much information as possible public information, and by making it easier to organize and share that information.\nI’d like to think there’s a parallel to journalism, too. Good journalism - especially good public service journalism - is all about magic removal, about putting the reader in a position to understand why and how decisions are made, and to be able to put herself squarely into the conversation. The best feature writers can demystify just about anything, no matter how unknowable it seems on the surface. I think Aaron was working on the same thing, albeit in a different way.\nTo the list of descriptions we’ve seen of Aaron in the past two weeks: hacker, writer, thinker, activist, prodigy, friend and son, let’s add one more: magic remover. And let’s think about how we can remove some of the magic that remains."
  },
  {
    "objectID": "archives/2013/01/13/on-aaron-swartz/index.html",
    "href": "archives/2013/01/13/on-aaron-swartz/index.html",
    "title": "On Aaron Swartz",
    "section": "",
    "text": "After trading some emails with Aaron Swartz in 2004 and 2005 about building a congressional votes database, I began to entertain a ridiculous fantasy: maybe, just maybe, I could coax this brilliant young man into a fledgling world of data-driven journalism on the web.\nA fascinating dinner at a Palo Alto diner in April 2005 put paid to that idea. If there was any kind of work system that could appeal to Aaron, I was pretty sure that it hadn’t been invented yet. He visited The Washington Post, where I then worked (he wore a shirt labeled “Unemployed”), but he wouldn’t have lasted more than a couple of days had he actually been an employee. The whole process of journalism, leaving aside any ideological issues, was unfit for his interests, ethics and habits. I hugely admired him for the clarity he had, and the desire to live out his ideals. We need people like that to remind us that what may seem impossible is not.\nAnd yet, as Danah Boyd has written, there is a tension here, because to mythologize Aaron is to make the work he believed in seem more remote or unachievable. That he chose to involve himself in politics, perhaps the messiest and most human of activities, seemed a little odd to me at first. “He’s such a talented programmer, I don’t get it!” Politics (and governance) is a complicated, maddening, compromised business. Books and speeches can make it sound simple, but in my experience those are mostly ideals, not reality.\nHis death is first and foremost a terrible tragedy for his family and friends. Those who knew him less well, myself included, and the general causes he fought for and believed in, also have lost an incredible source of talent, inspiration and curiosity. But as with most movements, Aaron was one of many working for open access to information and a more responsive public sector. He accomplished more in less time than anyone, perhaps, but that’s not what left the biggest impression on me.\nDespite the penchant of many (including my employer) to refer to Aaron as an “Internet activist”, we should not forget that he was also deeply involved in the offline political world. I remember him volunteering to help Bill Halter’s Senate campaign in Arkansas in 2010 (Halter lost the Democratic primary to incumbent Blanche Lincoln). He spoke at rallies about SOPA & PIPA. He could have chosen the “web-only” role, and for someone who had a complicated history of social interaction, that might have been an easier route. But I think he learned that, whatever his talents with a computer, politics needs - demands - people on the ground, face-to-face. I also imagine such activities took their toll on him; a spotlight lays bare our strengths and weaknesses.\nI think what I admire most about Aaron is that he was willing to step out of the hacker role (and I use that term in the best and true sense of the word) and into a system that was not of his creation. Most political/government/media types who are unfamiliar with people who create things using computers, particularly on the Internet, struggle to understand how these people can do what they cannot: invent a new reality if they don’t like the one they have. Online, if you don’t like a piece of software, you can write your own. If you disagree with a community, you can leave and set up your own. Such options in the offline world can have enormous costs.\nSimilarly, many of my friends who build software wonder how anyone can tolerate a political system that seems to work fitfully, if at all, and are amazed at what progress of any kind requires. Aaron was one of those who didn’t need to work in both worlds but chose to. I used to wonder why he would even care to talk to people like me, who could teach him nothing about technology and only a little about other topics. But he loved ideas and what you might call “social studies”. I think he knew that the amazing platform provided by the Internet could only bring about real changes if people were willing to put them into action. That documents and data and records posted online would need to translate into action or impact offline. It’s something those of us who work in the web journalism space need to keep in mind, too.\nI have met few people like Aaron Swartz, and no one exactly like him. To me, the main difference between Aaron and almost everyone else was that he seemed far less willing to make many of the compromises that the rest of us do as we do the messy work of building a life. Many others have written about the frustration of maintaining a dialogue with someone who holds himself and others to very high standards. Our longest email thread, about the Washington Post’s coverage of Jack Abramoff, has some of that.\nMaybe no one could have saved Aaron. Maybe he alone could have. We’ll never know. But there are millions of people with a desire to improve the world who see both an online world full of possibilities and an all-too-human system of governance and politics capable of almost hideous brutality. How to reconcile the two may not be the most important problem right now. But I feel like it’s a problem that Aaron was working on."
  },
  {
    "objectID": "archives/2013/06/04/lessons-from-data-crunched-democracy/index.html",
    "href": "archives/2013/06/04/lessons-from-data-crunched-democracy/index.html",
    "title": "Lessons from ‘Data-Crunched Democracy’",
    "section": "",
    "text": "Last Friday I traveled to Philadelphia for Data-Crunched Democracy, a conference drawing together political consultants, data analysis and targeting professionals, academics and journalists to discuss the impacts of “big data” on elections. This cross-disciplinary approach made for some very interesting discussions and I met a number of fascinating people (including, as it happened, the prodigious Xenocrypt). Rasmus Kleis Nielsen has a good overview of the day, but I wanted to write a bit about the lessons for journalists covering campaigns that engage in the use of data for targeting and predictive analysis.\n\nThe Limits of Obama vs. Romney\nMany of the moderator and audience questions at the conference were focused on last year’s presidential campaign, and with good reason, but journalists need to absorb the important lessons and move on. Here is a quick list of relatively unimportant questions about data use from that campaign:\n\nHow many people worked on the data team?\nWhat percentage of the campaign budget was devoted to targeting? (If you want to be a wiseass, the correct answer is: as close to 100% as possible, depending on the definition of “targeting”)\nWhich commercial datasets were the most useful to the campaign?\nDid you really track whether visitors to the campaign sites also visited pornography sites? (this still annoys the Romney folks)\nHow often did you pull my Facebook graph and Twitter feed?\nHow much data did you have?\n\nI wished that the organizers of the conference - who did a very good job - had blocked out 30 minutes at the start for an “Ask Me Anything About the 2012 Campaign” session, and then prohibited 2012-specific questions for the rest of the day, in order to draw the discussion towards the implications of such use and the lessons for all of us. There were times when we had that discussion, but there were other times when it kept circling back to some version of “How great, in your opinion, was the Obama campaign’s data use” or “Why didn’t the Romney campaign do X?”. As Ethan Roeder, an Obama campaign data staffer, put it on Twitter: “Obama campaign wasn’t all that and a bag of chips, bro.”\nThe first thing we can do as journalists is to focus on the broader lessons and questions of the campaign, not rehash it.\n\n\nNot Exactly Like Selling Orange Juice\nYes, there was some excellent journalism done about the campaigns’ use of data, particularly from Sasha Issenberg, Alexis Madrigal and Lois Beckett of ProPublica. And it’s logical that much of the focus from the media and academics in this area has been on the 2012 presidential race; it featured two campaigns with massive resources and plans to engage and target voters in sophisticated ways. There are some really innovative techniques that we learned about, including the use of data recorded by television set-top boxes. But even that Gawker piece seems to me to focus on the utility of such a tactic in the wrong way. As Carol Davidsen, who described the process at the conference, said, “behavior data is more interesting than consumer data,” and the campaign was looking to spend its television ad budget wisely. So running ads on a specific cable network during a specific time period becomes less about targeting specific individuals - Davidsen pointed out that while TV viewership hasn’t slacked off, people are doing other things while the TV is on - and more about getting a better value for the campaign’s ad budget. Yes, it’s targeting. But what I heard from the people who do campaign targeting is that a lot of what’s possible is fuzzy at best. It’s very hard to get down to targeting individuals precisely using data from social media or other sources beyond a voter file.\nThere are too many occasions when the press has conflated political data work with the most common use case: selling stuff to consumers. Elections are not like selling orange juice; rather, they are like selling orange juice to people who drink it infrequently, are often ambivalent about it and can change their minds based on what may seem tangential issues. In other words, it’s hard, and that difficulty matters both to the process and how journalists explain it. Political campaigns have not and cannot just take corporate data mining practices and plug them into a political campaign; it’s a different process, one that actually has less data.\nListen to Alex Lundry of TargetPoint Consulting, who worked on the Romney campaign: “What we’re doing is building data bridges, but to data islands. There are no individuals, only aggregates. There are bridges, but our ability to work on those islands is limited.” Some of the academics accused the targeting folks of playing down the effectiveness of their techniques, as if they were saying that this stuff doesn’t work. I don’t believe that’s the point they were making. The point, which journalists in particular need to understand, is that integrating data sets is a messy, unreliable and hard business. I usually tell students of mine that at least 75 percent of the time they spend with data is doing stuff before the analysis: acquiring, cleaning, standardizing, figuring out the limits of it. Lundry put that number at north of 90 percent at the conference. So there are no magic bullets here, and the really valuable part is the voter file, not the other data. It would be convenient for journalists if the story of data use in the 2012 campaign turned on a single tactic or dataset, but it’s not true. We need to be honest with ourselves and our readers about what campaigns are trying, but especially what works, even if it’s not thrilling.\n\n\nKeep Calm and Write Sensibly\nThere was some pushback from campaign staff towards the media for its reporting on data and privacy, that we’ve gotten alarmist and are scaring readers. I think they have a point, in that the world we describe too often sounds like, as one panelist put it, “a Tom Clancy novel” where everyone assumes that they are being followed all the time. I’m not saying there is no danger of abuse or misuse of personal data - a campaign staffer could disclose personal information about voters in a number of ways - but I do think the journalists need to recognize that most campaigns are built on a simple model: do what’s necessary to win. That often means that big plans that you and I might dream up for massive digital surveillance don’t happen because campaigns don’t have the time, staff or incentive to conduct them. It doesn’t mean that campaigns are always good actors, however, and journalists need to be able to understand the technology enough to tell the difference.\nDuring the presidential campaign, the Times (and I presume other outlets) was pitched a story by one side about the Internet practices of the other, with the implication being that the other side wasn’t playing by rules barring coordination among separate committees. While it was technically possible that such skullduggery was afoot, there was no proof (or even very solid evidence). I imagine it would be tough for many reporters to be able to judge the situation, considering that it turned on how Web analytics were being used and identified. Ultimately, we did not publish a story, and when I asked one of the original sources of the pitch about it after the election, this person laughed in a fairly sheepish way. They tried: it was a campaign, after all. You want to win.\nSo yes, the media has its problems in accurately writing about data use, privacy and the Web components of modern political campaigns, but let’s not forget that the campaigns have an interest in painting each other as bad actors, too, and journalism has a role in trying to figure out such claims. Which means that journalists need to understand how sites track visitors. Your own analytics department would be a good place to start. And then we need to write calmly about the tactics and implications. This approach, perhaps best exemplified by ProPublica’s Message Machine work, tries to pull apart the campaign’s techniques without making them sound like they are some science-fiction tale. Campaign emails are a great place to start, because lots of people receive email, and it’s so cheap to send that there’s no incentive for the campaigns not to experiment. When that approach becomes possible with television advertising (an area Lundry calls “ripe for innovation”), we need to be ready for it, as difficult as it may be to track.\n\n\nFocus on the Basics\nBecause a lot of this data stuff seems like rocket science to most journalists, there’s the temptation to believe that data science can do anything, and that anything is possible. But when it comes to campaign data, it all starts and ends with the voter files - registration and voter history information. That’s the core data asset, and everything else is built atop it. You can’t really start with, say, a file detailing magazine subscriptions and build an effective campaign. Lundry called the information gathered by volunteers or paid canvassers “solid gold” to a campaign, because it is collected by a person who is able to talk to potential voters.\nFrom that point, the workings can get a little obscure, as campaigns try to integrate data from the voter files with information they have collected. Part of that is the commercial data, but only part, and its predictive value doesn’t always rival that of voter history files and partisan leanings (particularly in the past few elections). For journalists, the lesson is clear: we should at least have what campaigns start with in terms of voter data: voter registration, voter history and political geographies. In many states, these are freely available (although, as Kevin Collins notes, that’s not universally true). The campaigns were updating their voter data monthly, then weekly, then daily as the election neared. Are we doing anything similar?\nI hear the cry: “But what about social media?” Here’s what Rayid Ghani, the Obama campaign’s chief data scientist, said at the conference: “So far there hasn’t been a good way to connect that with the voter file.” The piles of Facebook Likes, the retweets and reblogged Tumblr posts - they are all interesting things, but how relevant are they if a sizable chunk of your social audience either lives outside the United States or isn’t eligible to vote for some other reason (age, for example)? The most interesting question from the 2012 election, to me, is how the Obama campaign managed to find additional voters and turn them out. Some of that is due to predictive modeling. But some of it is likely due to really diligent in-person field work. We should strive to understand all the parts of the effort.\nOne way to do that is to follow the people who did targeting work as they now go on to other things. Many are still involved in politics at the national level, including folks like Lundry. Others, like Obama’s former Wisconsin field operations lead Hallie Montoya Tansey, are looking down the ballot. She and a friend have founded The Target Labs to provide prediction services to campaigns. Why? Here’s what she says about working for Obama in Wisconsin:\n\n“The reason we were reaching out to voters outside of the Democratic strongholds in Madison and Milwaukee is because we were using data analytics to build models that used all the information we had about voters, including demographics, geography, and past voting history, to make extremely accurate predictions about how likely they were to support us. These models made it possible to find ‘our voters’ on every block in every region of the state. In 2004, John Kerry won Wisconsin by 10,000 votes. In 2008, our margin of victory was 400,000. Although I’d worked on many campaigns before, this was my first experience using predictive modeling, and it convinced me completely of the potential of these techniques, adopted at scale, to change the way campaigns are run.”\n\nNews organizations should be doing post-mortems using 2004, 2008 and 2012 voter history and registration data to figure out where the “new” voters came from, who they might be and how solid they are in terms of voting behavior. Although no one at the conference was certain that micro-targeting would become a staple of every campaign anytime soon, I think we all acknowledged that it is coming to an election near you. That warning applies to journalists, too. The way to start is to begin valuing information about voters as much as campaigns do, and to try and emulate the data efforts of the campaigns, so that we can more accurately assess their work."
  },
  {
    "objectID": "archives/2014/05/22/how-it-starts/index.html",
    "href": "archives/2014/05/22/how-it-starts/index.html",
    "title": "How It Starts",
    "section": "",
    "text": "Tomorrow is Aron Pilhofer’s last day at The New York Times.\nAron joined The Times in 2005, first working with the computer-assisted reporting desk then headed by Tom Torok. In the summer of 2007, I was working at washingtonpost.com doing things that other newsrooms weren’t doing. And then Aron started talking about this team he would be building at The Times. It was a compelling pitch, even though washingtonpost.com was ahead of the pack. “You should have a blast,” he wrote in one email.\nThanks to Aron, it has been one hell of a ride.\nIt’s instructive to look back at the beginnings of Interactive News at The Times, when we had five or six people, a charge of developing web applications and literally no infrastructure of our own. Our initial 2008 presidential primary app ran off a Windows box belonging to CAR. One person could “login” to it at a time and if you think that Rails runs slowly on Windows now, you should have seen it then. Sometimes, the Windows box needed to be rebooted by CAR. We used the same user names and passwords because a) there were so few of us and b) it was easier to create one account than five. Which led to emails like this one from Ben Koski in June 2008, during a presidential primary: “Tonight my newsweb password expired promptly at 8pm, perhaps one of the most inopportune moments for a password change.”\nWe were parasites. We squatted on whatever servers we could get access to, which once resulted in a box we used being literally unplugged and moved to a different data center - a totally scheduled thing - that we were completely unaware of until we tried to ssh to it. The migration took a couple of days. We cooled our jets, and developed a plan where we could publish election results off Ben’s laptop if need be. It was not a frivolous plan; there was a non-zero chance it would be needed.\nAron “acquired” some hardware we could actually see - the late, lamented server he called “jeffvader”. The prototype for Represent first lived on jeffvader. Lots of things lived on jeffvader. We ran Django on jeffvader, for goodness sake. Jeffvader was in use until at least mid-2010.\nBut Aron kept pushing and pushing for us to get access to Amazon’s cloud offerings, kept pushing for us to have access to ports that were not widely available, pushed to make sure that, whatever the obstacle, we’d figure it out. It was rarely a thing a beauty, but then anyone who makes something new knows that not every day is a work of art.\nOnce we got access to EC2 servers, we just started spinning them up for stuff. We’d get emails from Aron periodically asking, “Does anyone know what is running on (random Amazon IP address)?” It wasn’t a scolding thing; it was basically housekeeping duties. He was (usually) patient as we all stumbled around in the dark, trying to figure this thing out.\nWe switched from using Subversion to using Git, saw each other through Rails upgrades, database meltdowns (mostly caused by me) and near-calamities on production servers. Only once did I feel any sort of anxiety from Aron; on election night in 2008, he warmed us up by smiling and saying, basically, “If this doesn’t go right, don’t come in tomorrow.” We had that sort of nervous laughter, but if it had failed - if we had failed - it might have been the end of INT, which was barely a year old. But we didn’t fail.\nAll of this is worth mentioning not just to recognize what Aron has done at The Times in building Interactive News, but to suggest that this sort of change doesn’t come easily and isn’t handed to you. You have to make a play for it, and that involves sticking your neck out and trusting that your colleagues will do the same. It requires selling everyone - perspective employees, current colleagues and higher-ups - on something that isn’t quite there yet.\nNone of us knew what we were doing - not in the way we do today, when there are habits and practices and traditions. We had no idea where we might end up. But we had a guide, and we found the way. Thanks, Aron."
  },
  {
    "objectID": "archives/2014/11/17/were-all-publishers-now/index.html",
    "href": "archives/2014/11/17/were-all-publishers-now/index.html",
    "title": "We’re All Publishers Now",
    "section": "",
    "text": "Last weekend I had myself a proper Twitter rant.\nIt began with this: “We need to talk about newsroom efficiency, a phrase right up there with ‘cruel kindness’ or ‘deafening silence’ in its oxymoronic quality.”\nNewsroom efficiency is a little bit of a white whale for me. I know most newsrooms can never be made completely efficient, nor should they, for the kinds of inspiration needed for doing journalism isn’t available in predictable forms. I have no illusions about the messiness of what we do.\nThere was a time when efficiency didn’t matter as much, or even much at all. Our timeframes were elongated, not compressed. Our budgets were fat, not lean. Perhaps most important, the duties of reporting and editing the news were tied to its production process only by a printing and delivery schedule. If the presses stopped working, there was very little reporters could do about it. Production - publishing - was someone else’s problem.\nThose days are slipping away fast.\nVirtually every journalist today is closer to the tools of publishing the news than at any point since the hand-pulled press of colonial times. We can push buttons and our words, images and graphics appear on the Web. A story’s afterlife - what happens when it has been published - is something that we can revise and extend. These are hugely important changes; they alter what is possible for all of us.\nToo many of us ignore or downplay this. We don’t know the means of production, and don’t want to know. It’ll get done by someone, right? That much is true - if we continue to be detached from the tools that publish our journalism, it will get done by someone else. It’s just that that someone is likely to be literally someone else.\nWe cannot remain strangers to the means of publishing. In other words: for your own sake and that of your co-workers, learn to use your CMS.\n(I’ll pause here for an obligatory eye-rolling.)\nYes, the CMS. The software we love to complain about, the deleter of edits, the thing that crashes when you can least afford it. To the extent that reporters and editors think about their CMSes beyond epithets, it usually involves something like this: “If I could make one Christmas wish to developers, it would be for a better content management system.”\nWell, you can’t spell “better content management system” without “you.” I mean, you can, but not in practice. Reporters and editors need to be better, stronger users of their publishing tools, not just to be effective at their jobs but to shape and inform the growth and improvement of those tools. You can summon all the developers you want, but if you don’t actually use what they build, they won’t know how to make something useful for you.\nI’ve had to use a number of CMSes in my career, and very few of them could fairly be called “great.” But even the worst ones - the ones I dreaded opening and tried to avoid - contained surprisingly useful features. If you, as a user, participate in the development of your CMS, you’re gonna end up with a more useful CMS. If you don’t, you’re holding yourself and possibly your colleagues back.\nYou might ask why this matters so much. Who cares if I file a story using email, Word or Google Docs rather than my CMS? Here’s why: unlike publishing systems of the past, the modern CMS publishes directly to the Web. That means that you are a publisher. And that means that you can become a better publisher, knowing how to add links, where to place photographs or graphics, or how something will look to a reader on a phone.\nPerhaps you didn’t sign up for this. You wanted to be a writer, an editor, a photographer, a graphics editor. And you can be. Now you’re also a publisher, too, and the longer we shrink from that responsibility, the more problems we’ll all have.\nNo CMS is perfect. That’s why we need to use them, so we can help shape them and use them to put our best journalism in front of our readers and know how they are reading it. Choosing not to use your CMS is choosing not to be a part of that process, and unlike the past, when someone else did the publishing for all of us, it’s our responsibility.\nNot everyone will become a power user. Not everyone needs to: any piece of software with a reasonably broad set of users will need to accomodate them. But if you never know how it works, you’ll never be able to make it better for yourself, your organization and ultimately your readers. You can’t be a tourist visting your own workflow.\nHere’s what you can do: start using your CMS if you don’t. Then, find out who works on it and tell them what you like and don’t like. Chances are they would love to hear this kind of feedback. Ask questions: Why can’t I do this? Would it be possible to do this other thing?\nI started this rant with “newsroom efficiency.” Here’s how that factors in. Right now, the people who don’t use the CMS are being supported by people who are doing that for them: they are copying and pasting, clicking buttons on a story that isn’t theirs and will never be as important to them as it is to its creators. Imagine what they could be doing, the work they could be creating, if they didn’t have to do this. Maybe they would think of a new way to tell stories online. Maybe they’d have the room for that inspiration to strike, the kind that leads to amazing stories.\nBut this isn’t a zero-sum game. You’ll be better off, too, having more control over what you publish and a bigger role in building our future. You’re a publisher now."
  },
  {
    "objectID": "archives/2014/07/08/lightning-strikes/index.html",
    "href": "archives/2014/07/08/lightning-strikes/index.html",
    "title": "Lightning Strikes",
    "section": "",
    "text": "On November 19, 2009, Jaimi Dowdell of Investigative Reporters & Editors sent an email to more than a dozen of us asking about some ideas for advanced sessions for the 2010 CAR conference in Phoenix. Here’s part of my response:\n\nI think the ideas are great. A couple of possible suggestions:\n\nA session of “lightning talks”, in which people present a single idea or technique in a short time. I think it would lessen the pressure on presenters and expand the universe of them, too. http://en.wikipedia.org/wiki/Lightning_Talk\n\n\nEvery once in awhile, I have an idea that pans out.\nLightning talks, a popular feature of many a conference, were almost completely unknown to the journalism community. A few of us had seen them at technical conferences we’d been to, but mostly it was a foreign concept, particularly for a conference that had always featured 45-minute sessions with talks by the very best data reporters and editors around.\nThe first session in Phoenix was in a pretty standard room. There were 10 talks that year, and half of the audience consisted of the presenters. Here’s the lineup:\n\nGoogle Charts. Easy, Clear, Indestructable. (Ben Welsh)\nLike Snowboard Cross, but With Data (Scott Klein)\nThe new free visualization tool (Sarah Cohen)\nEasy peasy due diligence (Danielle Cervantes)\nData Manipulation or Graphics with R (Bill Alpert)\nEssential Queries for SQL Server (Anthony DeBarros)\nUsing an API with Excel (Derek Willis)\nEasy interactive charts with Open Flash Charts (Tim Henderson)\nHello, Newsroom! Build out a GIS-enabled web app in < five minutes (Brian Boyer)\nThe hidden power of Javascript (Michelle Minkoff)\n\nPicking Ben Welsh to lead off was, in hindsight, a rookie mistake on my part, but the session was pretty well-received, and we all had fun doing it (all 25-30 of us). Fast forward to earlier this year, and here was the crowd for the lightning talks in Baltimore:\n\n\n\n\n\nA big crowd\n\n\nIn five years, lightning talks went from being a small session competing with other sessions for attendees to a standing-room only event in the biggest ballroom the hotel had. I’ve been privileged to help run it, along with Aron Pilhofer and the folks at IRE who constantly encouraged and supported it. My favorite part of the last few NICAR conferences has been seeing new speakers (and some regulars) wow and entertain their colleagues.\nAnd that’s why it’s time for me to hand over the lightning talk reins: because we have so much talent in our community, and five years is long enough for anyone to be heading up a showcase of our brightest. Starting this summer, Sisi Wei, who gave a remarkable talk in 2013 about making games for news, will be leading the session at next year’s conference in Atlanta. Besides being a smart and funny and good person, Sisi is also someone who has worked very hard to bring new people into our community, and that’s one reason she’s such a good person to lead lightning talks.\nWe’ve already talked in general about some of the changes that lightning talks needs now that it’s such a popular event. Mainly, these changes involve the talk submission and voting process, which in the past has favored people who pitched earliest and those who are well-known in our community. We’ve grown too much for that to continue. For many first-time NICAR attendees, lightning talks are perhaps the one place where they could see themselves being able to contribute; after all, it’s just five minutes as opposed to an hour. Working with the IRE staff, Sisi will be mindful of the need for lightning talks to be a place where we get to hear from a broad range of speakers with different levels of experience. It might also be time to retire the submission site that Aron hastily put together years ago.\nNone of this would be possible were it not for the openness of IRE, including Mark Horvit, who has been incredibly supportive of lightning talks, and of the IRE staff. All of us owe a debt to them and to that first round of speakers who tried something new, and made it a success.\nWitnessing the growth of lightning talks at NICAR has been a joy for me. I have always looked forward to presenting the lineup of awesome presenters and watching as the audience were blown away by their passion and creativity. Now I get to experience that myself, and we all get the benefit that IRE has always provided: dedicated volunteers helping to make our profession that much better. I can’t wait to see what Sisi has for us in Atlanta."
  },
  {
    "objectID": "archives/2022/12/31/welcome-back/index.html",
    "href": "archives/2022/12/31/welcome-back/index.html",
    "title": "Welcome Back",
    "section": "",
    "text": "It feels like a good time to start writing again, but this isn’t a totally fresh start. As I’ve tried to de-emphasize the time and attention that I’ve given to Twitter over the past few years, I realized that I still have things that I want to talk about. I don’t have a newsletter, or a podcast. But since 1998 or so, I have had a blog, and that’s what I’m coming back to.\nI’ve rebuilt this site to combine what used to be “here” on thescoop.org and some of what had been sitting around in other places. There are and will be more personal posts here, but the focus will be on what I like to write about: the combination of data, journalism and learning. Since joining the faculty at Merrill College, I’ve had so many questions and discussions with my colleagues near and far about teaching: what works and doesn’t, and what we could try. So there will be a fair bit of that here.\nThe move to academia also has given me a bit more freedom in terms of what I can say about this profession and how it is changing. I’ve spent the past year learning R and with this rebuild I’m able to share code in a way that hopefully is more useful, too."
  },
  {
    "objectID": "archives/2015/07/29/my-favorite-things/index.html",
    "href": "archives/2015/07/29/my-favorite-things/index.html",
    "title": "My Favorite Things",
    "section": "",
    "text": "As first reported on the tweets, I have a new job, at ProPublica, where I’ll be working on news applications, investigations and other incredible stuff with the News Apps team.\nLeaving The New York Times wasn’t easy. There are so many good people there, so many really smart journalists. So as a way of saying goodbye, here’s a list of some of my favorite things from my seven-and-a-half years at The Times (my longest time in any job, btw):\n\nRepresent. Even though it didn’t last, this was the most rewarding thing I worked on in terms of coming up with an idea and seeing it to completion, thanks mostly to my then-colleague Andrei Scheinkman, now at FiveThirtyEight. Represent was a departure for The Times into personalization and feed aggregation at a time when both were still new ideas in the newsroom. It also was, to the best of my knowledge, the first Django app at the NYT. We stopped maintaining it a few years ago, but it lived on in other projects, including the Districts API and “The Other Races” from 2013.\nToxic Waters. One of the first large projects I worked on at The Times, this involved both obtaining and loading EPA data as well as helping to decide how we’d present records of inspections and violations for water pollution permit holders. I didn’t do the really amazing visual aspects of this project, but really enjoyed digging into a new dataset and being able to help readers find out more information on water pollution where they live.\nFech. This started as an intern project with Michael Strickland and Evan Carmi; the goal was to create an internal toolkit to process electronic campaign finance filings from the FEC. Since releasing it, we’ve gotten contributions from people outside The Times and Fech has become something of a standard, inspiring a Python version, too. It also powers Itemizer.\n2012 Election Coverage and results. Elections are a huge deal at The Times, an all-hands-on-deck affair. I was really proud to be a part of the process for elections beginning in 2008, but for me 2012 was a high-water mark. We were able to build individual campaign finance elements like this one with Jeremy Ashkenas and another with Kevin Quealy, plus contribute to exit polls and other parts.\nCampaign Finance and Congress APIs. These were somewhat accidental projects in the sense that from a publishing standpoint they weren’t strictly necessary. But both APIs provided information to the public that otherwise would have been difficult to obtain. Even though the FEC now has an API and the United States project has expanded the congressional data available, they have been used in academic projects, silly projects and serious projects.\nUpshot Mississippi Senate primary coverage. The 2014 GOP primary and runoff in Mississippi gave me a chance to use precinct-level results data in near real-time. Pairing with Nate Cohn, we examined the impact of black turnout in two stories that provided evidence instead of just anecdotes. I also was able to delve into the murky world of outside spending in that race.\nVIGOP This is a crazy story, still ongoing, about how a small party organization is trying to raise its profile through some controversial and fairly unorthodox methods. The artwork makes it even better.\nSunday Morning Talk Show Guests. One of my favorite things to do is to try and confirm or debunk the conventional wisdom about a topic, and there are few things more conventional wisdom than being a guest on the Sunday talk shows. I worked for several weeks standardizing data on talk show guests (which we released) and then helped build the table that displayed the most frequent guests.\nActBlue. One of the biggest stories around political fundraising has been the rise of the small donor, and ActBlue, a Democratic conduit organization, is right in the middle of that. I used ActBlue filings to show how their users have expanded and grown as repeat donors, to the benefit of Democratic candidates and organizations up and down the ballot.\nMaiden Names. When I came to The Times, I half-jokingly said that one of my goals was to combine the wedding announcements with people mentioned in DealBook to create a network of New York’s elite. We never did that, but before I left I was able to finally do an analysis of wedding announcement data with my Upshot colleague Claire Cain Miller and get a Sunday Styles byline, to boot. This one was a lot of fun, and a great way to end my time at The Times."
  },
  {
    "objectID": "archives/2015/06/06/civic-data-and-journalism/index.html",
    "href": "archives/2015/06/06/civic-data-and-journalism/index.html",
    "title": "Civic Data and Journalism",
    "section": "",
    "text": "A solid foundation of publicly available, consistent civic data - think of election results, voting information and other political records - is more important than ever for journalism. And while we’ve made a good start, we have a long way to go.\nOn June 5 I had the privilege of being on a panel at the Personal Democracy Forum in New York with Luciana Lopez of Reuters, Jonathan Capehart of The Washington Post and Jenn Topper of the Sunlight Foundation. The title was “How Civic Tech is Changing the Way Newsrooms Cover Elections,” and our discussion was wide-ranging, thanks to a) good panel selection and b) expansive definitions of civic tech.\nMy basic theme was this: newsrooms need access to a basic level of civic data and technology now more than ever, for two major reasons. First, the days when most newsrooms had the budget and staff to maintain their own versions of important civic data (like voter files or campaign contributions) are gone. Instead, the most common options are seeking out such data from third-party providers or not doing it at all. While there are some excellent third-party providers, including the folks at Sunlight and the Center for Responsive Politics, this is still a system that makes it harder for journalists to stumble across potential stories on their own.\nSecond, consensus on how to describe some political situation is harder to achieve when the number of publishers multiplies. Maybe we don’t want total consensus, but I’d argue that it’s desirable and important that we at least are able to have a common understanding of the key data that informs our civic life and how we report on it. Along with methodological transparency, it’s a way that we can be upfront with readers and each other.\nTwo projects have helped make these points clearer to me. The first, the United States project, is the result of conversations between Eric Mill (then at Sunlight), Joshua Tauberer of Govtrack and myself that came to a head in August 2012. We all agreed that it made no sense for each of us to maintain separate scrapers for legislative data, and that beyond duplicative work, it created the possibility that different choices made in isolation would result in different data. No one wanted a situation in which three providers of congressional data disagreed with each other - and we had a few cases of that, inadvertently.\nThe project grew from an initial focus on Congress to include executive branch data, including a central repository of inspector general reports. It now has a number of contributors, and decisions are made by rough consensus. The result is a stable foundation of legislative data. If, for example, The New York Times Congress API that I maintain were to be discontinued, something very similar to it could be re-created in a matter of weeks, if not days, using this data as its primary source. In less than three years, a group of volunteers has made it possible for virtually anyone seeking bulk legislative data to not have to spend weeks or months reinventing the wheel. That’s a huge accomplishment for civic technology, and for journalism.\nThe second project is OpenElections, which I’ve been working on along with Serdar Tumgoren and a host of volunteers since early 2013. This effort will go on for years, but its goal is to remove the high barriers to journalists and developers seeking to work with historic election results data. To make it easier for people to understand how elections have played out in their towns, counties and states over the past 15 years (and further if we can). Most newsrooms, never bastions of efficiency during good economic times, now face pressures that make it all but impossible to embark on months-long efforts to collect and parse data. If they were able to do so, OpenElections wouldn’t exist.\nWhat makes these efforts worthwhile for me is the idea that someone, somewhere will use the data we’ve gathered and published to make something I’ve never considered. Or to look at issues or trends I’ll never look at. This is public data; it should have a public home and a public understanding. There’s still a huge role for journalism in civic technology, none bigger than providing context and understanding from our shared foundation. And there’s still a lot of work to do. We have too many silos of civic information about politicians and candidates. We duplicate each other’s work too often to make sense.\nThe United States project and OpenElections could have been owned by a single newsroom and turned out ok. But I don’t think that’s the best scenario, because then the data that provides a common foundation is subject to a single organization’s control. We literally, and figuratively, can’t afford that.\nSo here’s what you can do: if you’re interested in congressional or election results data, consider contributing to these projects. Or consider filling the other gaps we have, especially at lower levels of government. But whatever you do, try to ensure that your work doesn’t become someone else’s repetition. Our common foundation, and our ability to deliver the kind of journalism that our civic life demands, can’t afford it, either."
  },
  {
    "objectID": "archives/2012/05/28/finding-new-money/index.html",
    "href": "archives/2012/05/28/finding-new-money/index.html",
    "title": "Finding New Money",
    "section": "",
    "text": "My colleague Raymond Hernandez has a story about an unusual aspect of New York Democratic congressman Charlie Rangel’s fundraising: after years of raising millions to help elect other candidates, the 40-year veteran is now taking money from some of his House colleagues for the first time ever.\nRay did 95% of the reporting work here, including getting lawmakers to talk about their financial support for a legendary politician. Vermont’s Peter Welch: “Charlie never asked me directly,” Mr. Welch said. “I don’t remember if I heard from another colleague that Charlie needed help. But I was not going to make Charlie come to me.”\nMy part was pretty simple: On May 18, I spotted a contribution to Rangel’s campaign that seemed unusual to me. Why would it be unusual for Steny Hoyer’s leadership PAC to give to Rangel? First, Rangel is the kind of guy who had never needed much help raising money. He was a high-ranking member of the Ways and Means Committee and had his own leadership PAC to raise money to distribute to others. That Hoyer’s PAC would give him $5,000 – the maximum contribution – made me look a little deeper.\nThanks to the Federal Election Commission’s bulk FTP data, it’s relatively easy to research the history of giving between a committee and a candidate. I looked back to 1979 and found no evidence of Hoyer’s PAC ever giving to Rangel prior to this year. Then I wondered: if the minority leader is doing this, who else is giving Rangel money?\nThere are three different ways to find out using FEC data. The first is to look at those FTP files, which are updated every week. The second is to look at electronic filings as they come in, which I also do (and that’s how I spotted the initial contribution). The third is to watch for electronic filings of large ($1,000+) contributions during the 20 days before a primary or general election. These must be filed with the FEC within 48 hours of the contribution.\nThe FTP data is very easy to use, but it is only updated once a week. The electronic filings can be more timely, and they can help give you a picture (a potentially incomplete one) of a candidate’s fundraising by looking at reports from committees who give money. Many of those committees file monthly reports, and they will report giving money before Rangel’s campaign reports receiving it. That was key here: Rangel’s last campaign filing was in April, and his next is due in July.\nWe load the expenditures from each electronic filing into a database table, and while it’s not as clean as the FTP data, this expenditures table was the key to finding the other donations referenced in the story. (An aside: you’ll note that we didn’t report a grand total for contributions from Rangel’s colleagues. That’s because there’s no way of knowing for certain until all of them have filed reports. The fact that more than a dozen had done so for the first time ever made it news enough; the actual amount isn’t terribly important to me.)\nI needed to check each of the new Rangel donations against the history of committee contributions, but the FTP data makes that pretty simple. Having both the electronic data and the FTP data in the same database makes this kind of story much easier to do. Since the FTP committee contributions to candidates data is in The Times’ Campaign Finance API and Fech handles electronic filings, this isn’t just something we can do, either. Now anyone covering a House race can see interesting and newsworthy transactions and act on them immediately. Little by little, we can get closer to telling the story of elections as they happen, not just after they end."
  },
  {
    "objectID": "archives/2012/05/01/our-mark-knoller-problem/index.html",
    "href": "archives/2012/05/01/our-mark-knoller-problem/index.html",
    "title": "Our Mark Knoller Problem",
    "section": "",
    "text": "My colleagues at The Times (and other folks I know who cover the White House) tell me that Mark Knoller, the CBS Radio reporter who reports on the president, is a genuinely nice man and someone who has always been extraordinarily generous about sharing what he knows with other news organizations. Knoller is such a fixture at the White House – he’s covered every administration since Gerald Ford’s – that he’s moved beyond simply being a reporter into the realm of providing a public service: he’s very often cited in other outlets’ stories about presidential travel. A sample:\n\nCBS’s Mark Knoller, who keeps detailed notes on Obama’s travels, recently told The New York Times that since the president filed for re-election, he’s taken 60 domestic trips and 26 of them involved fundraisers.\nBut Mark Knoller of CBS, the unofficial keeper of presidential work schedules, reported that President George W. Bush had taken more time off than Obama at this point in his first term.\nAccording to presidential watcher Mark Knoller of CBS, George W. Bush, at this time of his presidency, had made 30 visits to his Texas ranch spanning all or part of 220 days. The Obama’s vacation day count is less than half of that.\n\nThis isn’t about Knoller as a person or as a reporter. It’s just that this situation – where one person has become the official source of public knowledge about the travels of the President of the United States – is far from ideal. Forget that the government is occasionally off-base on presidential travel statistics; how is it that other news organizations, including my own, have relied on a system in which one person – however diligent and generous – holds such important information?\nFrom an information management standpoint, having Knoller be the keeper of presidential travel information is not only inefficient – what happens if Knoller is on vacation, or busy? – but makes it harder to regularly review the data or incorporate it into other inquiries. In reality, this is our problem, not Knoller’s, and his generosity has enabled us to carry on as if we’d been collecting this information all the time. But we haven’t. It’s easy enough to just ask Knoller, especially since we don’t use the information all that often.\nWe’re not talking about uncovering classified information here, but the daily whereabouts of the President of the United States. And yet somehow, every other news organization has decided that it’s perfectly ok not to have this information at its fingertips. It probably won’t happen as long as Knoller remains in his job, but what happens if someday CBS decides not to share that information anymore? Or Knoller decides he’s tired of doing this and retires? In the “weak link in the chain” scenario, the rest of us are the weak links, not him. He’s doing his part. Why are we shirking ours?"
  },
  {
    "objectID": "archives/2012/05/13/lost-in-the-weeds/index.html",
    "href": "archives/2012/05/13/lost-in-the-weeds/index.html",
    "title": "Lost in the Weeds",
    "section": "",
    "text": "The indefatigable Alex Howard posted a link today about a draft academic paper on open source and journalism by Nikki Usher of George Washington University and Seth Lewis of the University of Minnesota-Twin Cities. Alex’s tweets are worth a look, so I pulled up the paper and began reading.\nAlthough I didn’t finish graduate school, I have written my fair share of academic papers, so I’m not a complete novice in the area. And despite the fact that it is a draft, as Usher points out, the idea that you would post even a draft on the web and then profess surprise when someone links to it or reads it is a little off, particularly for someone writing about a) journalism and b) open source. If you weren’t expecting critics, maybe it’s a good idea not to let them see the material. Update: in fairness, Nikki Usher was not aware that the paper was online at all. (I should also note that criticism is my full-time non-job; I probably should have sought work in the field, not due to any real talent on my part but based purely on personal enthusiasm.)\nThere is a good bit to like in the paper, which will be presented this month at the International Communication Association’s annual convention. There is a tendency for those working at the intersection of technology and journalism to focus on the tools – the stuff that actually exists now – rather than systemic changes to journalism itself (there are ahem, some exceptions to this tendency). In part that’s because systemic changes are a hard problem too easily pushed to the background by the demands of doing journalism today. But it’s also in part because incremental changes, as the authors note, can be valuable in changing the whole. But, ok, I get it, and in many ways agree (except for the utopian one-open-source-CMS-to-rule-them-all idea – can we just kill that fantasy and focus on making information portable?).\nThen I read the part about Fech. As a contributor to the project, I am admittedly biased in its favor, but I do not think my reaction is solely or even mainly based on that fact. Here’s what they wrote about the project:\nThe New York Times developed Fech, a tool that helps journalists crawl financial disclosures by political candidates just by knowing a filing number (Strickland, 2011). Just as the discourse around open source tools emphasizes their pro-social benefits, Fech’s creators note that more access to these filings will lead to better journalism. But Fech also gives one more tool to journalists eager for the horse-race style journalism that is divisive and counterproductive for democracy (Patterson, 1993; Cappella & Jamieson, 1997). There is another problem with what could be a strength of Fech: While the source code is posted on Github for other developers, the tool has been built to help people in the newsroom, not to encourage participation by ordinary people.\nWhere to begin? First, and perhaps least important, it vastly understates what Fech enables journalists and developers to do, particularly in regards to what can be done programmatically with disclosure data. I do not think that it is a stretch to say that the easier it is to examine and search campaign finance disclosures, the easier it will be for reporters and the public to discover interesting and useful pieces of information. Indeed, the use of Fech by news organizations like ProPublica, Reuters and The Associated Press – to say nothing of our use at The Times – has borne that out.\nBut here’s the line that got my back up: “But Fech also gives one more tool to journalists eager for the horse-race style journalism that is divisive and counterproductive for democracy.” The evidence for that? There isn’t any. This is pure speculation; I would argue that it is refuted by the examples I just cited. Can the authors — or anyone — cite an example of where Fech has been used to enable more horse-race style journalism (in its pejorative sense, which is what I assume the authors meant)? I’m 17 years out of graduate journalism school, but I’m pretty sure that assertions like that need a bit more than a citation to work that says horse-race journalism is bad for democracy. In theory, Fech could be used to run nuclear reactors, I guess, but since there is no evidence of that actually happening, I’m going to discount that as a possibility.\nAnd finally, the authors write: “While the source code is posted on Github for other developers, the tool has been built to help people in the newsroom, not to encourage participation by ordinary people.” Well, yes and no. I would be hard-pressed to describe those people interested in campaign finance data as “ordinary,” but we open sourced Fech so that it could be used in newsrooms and in any other situation. I don’t understand how exactly the authors presume that we built it only to help people in the newsroom, or to discourage participation by non-newsroom folks. The fact that the contributors to Fech come mainly (but not exclusively) from newsrooms who cover campaigns is understandable to me (and to Jeff Larson). I’m not entirely clear how what we’ve done makes it harder for non-newsroom people to participate. I’d love to read about that, but there are no examples or further discussion in the paper (nor was any user of Fech contacted by the authors, from what I can tell. I wonder if they installed it and tried it themselves).\nUsher, to her credit, offered several explanations as to why this passage was in the draft. They include the fact that this paper, like many, undergoes blind review. Fair enough, but it’s worth asking whether the reviewers are able to evaluate these claims, since most of them could be debunked by reading posts on Open. It also, like many of my own projects, seemed to have been a bit of a rush job. I know all about that, but was it really so difficult to talk to anyone involved in Fech? Finally, my objections, however valid, don’t damage the overall point of the paper, but reflect the possibility that I “may be getting lost in the weeds.”\nThat’s the tricky thing about journalism, data and even open source. Weeds matter. If you get the weeds wrong, the eventual result usually suffers. If I’m lost in the weeds, maybe the garden needs some attention."
  },
  {
    "objectID": "archives/2012/02/04/on-legislative-data-transparency/index.html",
    "href": "archives/2012/02/04/on-legislative-data-transparency/index.html",
    "title": "On Legislative Data Transparency",
    "section": "",
    "text": "This week I was honored to speak at at the Legislative Data and Transparency Conference put on by the Committee on House Administration. If you’re so inclined, the videos of the presentations are online at the conference site, although I must warn you that they contain heavy doses of XML references and other fun stuff. What follows is not my presentation, strictly speaking, but most of it along with some other thoughts.\nBeing a former Congressional Quarterly staffer, I have an innate fondness for House Admin, one of the lesser-known committees but one with a large influence over what kinds of information the public can see about the House side of the legislative process. The committee has jurisdiction over the Library of Congress, which by extension means Thomas, the online home of so much Congressional information.\nThere are many other posts about the general desires of what those folks committed to transparency want when it comes to Congress, but Daniel Schuman of the Sunlight Foundation sums them up pretty well: “To the maximum extent possible, legislative information must be available online, in real time, and in machine readable formats.”\nI don’t disagree, and I am sympathetic to complaints that Congress has been slow to address the availability of bulk data. People such as Josh Tauberer have been screen-scraping Thomas since 2004, and I joined in the process a year later at washingtonpost.com. In 2012, we’re both still doing it, now joined by Sunlight, OpenCongress and who knows how many others (speaking of OpenCongress, if you want a less patient restatement of Schuman’s thoughts, OC’s David Moore has a stem-winder of a post for you).\nI, too, long for the day when I don’t have to wonder when my HTML parsers will break after a seemingly innocuous change to Thomas’ styles, or when I don’t have to enter three different IDs for a new Senator (Bioguide, LIS and Thomas’ own unique sequential number). But my presentation on Thursday concentrated on a more fundamental need. Before bulk data can become really useful, it has to be more consistent, understandable and accurate. Right now, if you’re not willing to put in a lot of time studying the quirks of Congress, you will always face the likelihood that your data, however lovingly collected, has plenty of errors.\nFor example, in the Senate it is possible for the Majority Leader and Minority Leader to alter the rules of math when it comes to how many senators constitute a three-fifths majority. The death of Sen. Ted Kennedy in 2009 reduced the number of Democrats in the chamber at that time to 59, and the total number of senators “duly elected and sworn” to 99. For votes requiring a three-fifths majority (thanks, Malcolm), a 99-member Senate would need 59.4 senators for passage, or at least 59. But the party leaders agreed to keep the three-fifths threshold at 60 votes throughout the period when the Senate had 99 senators, not 100. For much of that period, any two-thirds vote displayed on nytimes.com had the wrong number of votes required for passage, because I was relying on math. I could not find any place in the Congressional Record or anywhere else where this was documented.\nAn edge case, you might say. But when it comes to Congress, there are loads of them. A reporter called me several weeks ago to ask why a seemingly simple question about three members of her state’s delegation was maddeningly hard to answer. All three had been elected to the House the same year, and had served since then. But each of them had a different number of total votes he or she was eligible to vote on. How could that be?\nIt took me a little while, but the only explanation I could find was that their dates of service had to differ in some way, and my guess was that not all of them were sworn in for each session on the same day. It happens. Unfortunately, neither Thomas nor the Clerk of the House provides an easy way to find out when a particular member was sworn in, despite the fact that it is a basic element of what makes someone a Member of Congress. At the conference, I heard someone say that it would be possible to provide a list of swearing-in dates for every lawmaker. That’s good, and needed, but it’s not good enough. I need, and the data demands, timestamps in this case. That’s the only way I can be sure of what votes a member was or was not eligible to vote on.\nYou might think that you could find the total number of House votes for a given year by looking at the Clerk’s votes site. In 2011, the last vote was roll call 949. Alas, officially, there were 948 votes that year, because roll call 484 was vacated and replaced by vote 485, and thus never really happened.\nIn my presentation I cited a few other examples, but they mostly boil down to this: unless we can make congressional information easier to use and understand by people outside the small circle of legislative wonks, bulk data access by itself won’t solve our problems. Today the most creative uses of congressional information, such as Sunlight’s Capitol Words project, suffer from this limitation. I love Capitol Words, but right now the Congressional Record – the source for it – cannot reliably tell me in a machine-readable form whether a particular word or phrase or speech was even spoken out loud on the floor of the House or Senate. That’s kind of a big deal, for reporters, historians and the public.\nIf we can’t use congressional data to answer what should be straightforward questions, or can’t agree on what the answers should be, providing immediate access to that data in bulk form may not be as helpful as we would think, and in some cases risks adding to the confusion. It may expose more of those problems, which is of some usefulness, but if the ultimate goal is not just access but understanding, we need to address the fundamental issues of accuracy and consistency before we switch on the firehose."
  },
  {
    "objectID": "archives/2012/11/27/how-i-got-here/index.html",
    "href": "archives/2012/11/27/how-i-got-here/index.html",
    "title": "How I Got Here",
    "section": "",
    "text": "I meant to finish this by Thanksgiving, as a way of acknowledging the key role that female mentors have played in my career. Better late than never.\nA resume is a fairly terrible way of summing up professional experience, particularly since it attempts to impose a uniform structure over stories of wide variety. If you look at mine, it would show that I started in journalism at The Palm Beach Post, moved to Washington, D.C. to join Congressional Quarterly and then onto The Center for Public Integrity. From there, I was fortunate enough to work at The Washington Post and now The New York Times.\nMy resume, like others, might suggest that I have made my own “luck”, as it were. There’s no place on a resume to assign a percentage of personal responsibility, say, or a figure to the help of others. Perhaps there should be. When I consider the start of my career - the steps that put me on the path to where I am now - there is no escaping the conclusion that at key moments, when someone had to take a chance on me and see something that wasn’t entirely there, five women did so. I don’t for a moment believe this is a coincidence.\nGainesville, 1995\nTowards the end of my graduate work at the University of Florida, I began sending out applications for reporting internships to various papers, mostly in the state. I had done some high school sports stringing for the Jacksonville and Orlando papers and had assisted the Orlando Sentinel with coverage of the Danny Rolling trial. I covered Florida sports teams for the Alligator and occasionally wrote other pieces. But my clips were not particularly impressive. No one offered an internship (I still have most of the rejection letters, of course), and I began to worry.\nAt the journalism school one day I saw a flyer advertising an internship at The Palm Beach Post as a research librarian. I had been serving as sort of an unofficial librarian for the Alligator (I actually enjoyed cutting clips) and thought, maybe I could do that. It was something. I was not a library student, nor was I even sure what a research librarian did beyond “look up stuff”.\nThen I met Mary Kate Leming, who ran the library at The Post. She came up to Gainesville (a non-trivial trip) to interview me for the internship. Whatever I might have thought about the image of librarians, Mary Kate wiped away almost instantly. The way she described it, research librarians at The Post hunted down the news. They were part reporters, part detectives. It sounded pretty cool to me, and I got the internship. Mary Kate took a chance on me, and set me on my course. She’s now the executive editor of The Coastal Star, a local newspaper for beach communities in southern Palm Beach County.\nWest Palm Beach, 1995\nAt The Post I was introduced to Michelle Quigley, a librarian, and told to learn how to do what she did. Michelle spent most of her time fielding questions from the newsroom, ranging from “So-and-so got arrested, his DOB is such-and-such, can I get a criminal history and an address for him?” to “My source mentioned a court case in Nebraska from 5 years ago, I think. Can you find it?” and beyond. In the paper’s internal messaging system (yay, ATEX!), the person who got all the questions was known as “REF”. So, you messaged REF with your query, and at some point you hopefully got an answer.\nA former law librarian, Quig was the best researcher I’ve ever seen. Reporters loved her, because she thought like a reporter; indeed, sometimes she thought better than they did. I could not imagine a better mentor. This was still in the early days of general Internet access, so search engines weren’t all that useful and certainly not for what we’d call “real-time” results. I learned how to construct queries on dial-up systems like Dialog and a dozen others. We had a microfiche collection of Florida driver’s licenses. It was like working in a candy store, except the candy consisted of facts. The problems changed every day. It was awesome.\nThere’s one story I’ll never forget: a sports reporter had gotten a tip that the Florida Marlins would be hiring a new coach (it might have been the manager, Jim Leyland, or an important position coach, I’m not sure a former colleague says it was the hiring of Leyland, and our reporter, Dan Graziano, was looking for a club official to confirm it). The reporter was trying to get a phone number for the guy, and had struck out. Could REF help? Michelle established that the fellow lived down a rural road in Ohio and had an unlisted phone number. Seemingly a dead end. But then she found the numbers of his closest neighbors, and the reporter convinced one of them to run over to the guy’s house and ask if he’d get on the phone. Now that’s some creative problem-solving.\nAnd yet Michelle and Mary Kate must have known at some level that I wanted to get out at some point and do more reporting. Still, they trained me and spent hours answering my ridiculous questions. And then they let me go, off to build a newsroom intranet and work on CAR projects and cover some small towns. They took a risk on me, one that had limited benefits to them in the long run, and changed my life.\nMichelle still works at The Post, hunting down the news. I’m not sure where I’d be without her example. I knew I wasn’t a great writer, but in her I saw how you could have a huge impact on the news without writing every day.\nWashington, 1998\nI came to D.C. to work at Congressional Quarterly at the end of 1997, ostensibly to work on a reference book on political action committees and to do some CAR stuff whenever I could find the time. Kinsey Wilson and Dan Gainor hired me, and did much to get me integrated into the organization, but I worked for the “New Media” department, which didn’t have a lot in common with the traditional side of CQ. Maybe a month into my work there, CQ held its annual meeting at a nearby hotel. We all went to listen to various executives talk about the company. It wasn’t the most exciting thing.\nThen I heard a speaker mention my name. It wasn’t Dan, or Kinsey, or anyone else that I really dealt with much. It was Martha Angle, a veteran journalist and editor of CQ’s “Pulse of Congress” column, which sought to bring to light exclusive inside baseball-type stories. Martha is a D.C. reporting institution, a fighter and someone who loves the job. And she had just singled me out as someone who she thought would help CQ. Then she helped make it happen.\nI moved to the politics group after the book project was done, then to working on CQ’s annual vote studies and finally to covering House leadership and writing Pulse pieces for Martha. I knew next to nothing about how the House actually worked, but I learned from the best. Martha championed my work to the point of encouraging me to set up a Blogger.com account (this was 2000 at the time) that got i-framed into cq.com, which I think was one of the first examples of a D.C. publication using a blog for its coverage. She was a demanding editor but one who would go to the wall for her reporters. I still don’t know exactly what she thought she saw in me, but she, too, took a chance on me and changed my life. She made it possible for me to take my interest in data and my interest in Congress and make a thing of it.\nWashington, 2004\nI was doing a training at the National Press Club on campaign finance data (what else?) in the summer of 2004 when one of the “students” stayed around after one of the sessions to ask some questions. These were mostly D.C. reporters and some editors who were trying to figure out how this data stuff could help with election coverage, or reminding themselves where to find things on the F.E.C. website since the last time they looked.\nBridget Roeber said that she ran the library at The Washington Post and that they might be looking for someone to do data work for reporters, some training and other tasks, and that I might be a good fit. Me: “….uh…yeah!” We had a lunch soon after, followed by an interview lunch and meetings with some of the other researchers. It all seemed way too fast to me; I couldn’t believe that this is how you get to The Post. I was at a non-profit, The Center for Public Integrity, spending my days buried in data on 527 political committees. I remember asking friends, “Should I do this?”, which was a ridiculous question. I was probably trying to ask, “Is this actually happening?”\nStill, this was a risk for Bridget. The Post was losing Margot Williams, an incredibly talented and hard-working researcher who had credentials and experiences far beyond mine. As far as I could see, it wasn’t an even replacement, except that they would be getting someone much geekier. Margot was a known quantity to the newsroom; I spent a single year as a professional researcher, and that was in 1995-96. It would have been easy for Bridget to hire someone else with more experience and a library background. There was no shortage of candidates. Still, she hired me, and made me feel like she had no doubts about it.\nBridget and her successor, Lucy Shackelford, encouraged me to do things beyond my job description, even when it wasn’t in their interest to do so. Lucy once drove me to a Special Libraries Association conference and introduced me at the start of a 7:30 a.m. session, saying something like: “I’m not totally sure what Derek does,” which is what every employee longs to hear from his boss. But she defended me to other managers more times than she should have had to and made my job easier. That’s how I wound up at washingtonpost.com and now at The Times.\nThere were no shortage of men who helped me, encouraged me and smoothed out mistakes of mine along the way. Some of them took some risks, too. But I am so incredibly grateful to these women who made my career possible, even when I didn’t always see it or in spite of my own efforts. Each of them, in different ways, are the kind of role model I want for our daughter."
  },
  {
    "objectID": "archives/2012/11/09/what-ive-been-up-to-lately/index.html",
    "href": "archives/2012/11/09/what-ive-been-up-to-lately/index.html",
    "title": "What I’ve Been Up To Lately",
    "section": "",
    "text": "A link dump of election-related work that I’ve contributed to as part of The New York Times’ coverage of the final months of the campaign. In every instance these involved collaborating with at least one other person, and usually more. For most of the stories, it was Nick Confessore and Jo Craven McGinty; for interactive work, it was Jeremy Ashkenas and my colleagues from the Graphics desk. This collaborative effort is what makes working at The Times so challenging and rewarding; we push each other to bring out our best, and we often save each other from our worst. Election Night Presidential, Senate and House Exit Polls (with Josh Keller) Interactives Outside Spending in Key House Races (with Kevin Quealy), Oct. 25 2012 Money Race: Compare the Candidates (with Jeremy Ashkenas, Matt Ericson & Alicia Parlapiano) Super PAC Pages (with Jeremy Ashkenas) Independent Spending Totals (with Kevin Quealy) Party Fund-Raising in Battleground States (with Mike Bostock & Kevin Quealy) The FiveThirtyEight Forecast (the right column of the blog, with Jeremy Ashkenas & David Nolen) Stories Last-Minute Money Pours Into Senate Races, Nov. 4 Obama, Romney and Their Parties on Track to Raise $2 Billion, Oct. 25 Romney and Republicans Began Month with $34 Million Advantage, Filings Show, Oct. 20 GOP Congresswoman in Fight To Retain a Hudson Valley Seat, Oct. 18 New Super PACs Alter Landscape for House Races, Oct. 8 In Iowa and Beyond, Redrawn Districts Test Favorites of Tea Party, Oct. 4 New York State Becomes a Focus in the Fight for Congress, Oct. 3 ‘Super PACs’ Finally a Draw for Democrats, Sept. 26. Cash Low, Romney Striving to Find New Large Donors, Sept. 20 And that’s just the stuff that you can see. Behind the scenes, working with my Interactive News colleagues, we’ve built systems to handle the storing and delivery of loads of political and electoral data. Some of that you’ll see more of via Times APIs in the future."
  },
  {
    "objectID": "archives/2012/10/15/congressional-data-github/index.html",
    "href": "archives/2012/10/15/congressional-data-github/index.html",
    "title": "Congressional Data on GitHub - A Way Forward",
    "section": "",
    "text": "Two years ago, there was a round of blog posts touched off by Clay Johnson that asked, “Why shouldn’t there be a GitHub for data?” My own view at the time was that availability of the data wasn’t as much an issue as smart usage and documentation of it:\nWe need to import, prune, massage, convert. It’s how we learn.\nTurns out that GitHub actually makes this easier, and I’ve had a conversion of sorts to the idea of putting data in version control systems that make it easier to view, download and report issues with data. The biggest factor in this change is the work that Eric Mill, Joshua Tauberer and I (in order of effort put forth) have done on scrapers to collect legislative data from the Library of Congress THOMAS site.\nWhen we first started discussing the idea, it seemed like a great way to avoid duplication of efforts; all of us scrape THOMAS for various reasons and information, and we all have gained some expertise in doing so. That we should then maintain our own systems, or at the least not share our experiences, makes little sense. (Yes, I agree that in an ideal situation none of this should be necessary, but it is.)\nBut the very act of collaborating on this project has led to really useful things. Josh produced a YAML file of every member of Congress that included an ID used by THOMAS that I had used in my scrapers at The New York Times but the others had not. In scraping bills from years that I had chosen not to tackle, Eric found some quirks in legislative data that I never imagined, much less encountered. The sum of the whole, then, is indeed greater than its parts.\nScraping is, for me at least, an activity that can encourage an “ends justify the means” mentality – since few people will ever see the code, it’s too easy for me to settle for the “good enough” marker. But having more eyes on both the scrapers and the resulting data means that people can spot inefficiencies or edge cases that can be the bane of scraper maintenance. And it enforces a bit more discipline; even if I do continue to maintain my own scrapers, they need to produce at least the same data as these files do. It’s good to have a standard.\nEric and Josh deserve a ton of credit for spearheading this effort, and I’m excited to see this repository grow to include not only other congressional information from THOMAS and the new Congress.gov site, but also related data from other sources. That this is already happening only shows me that for common government data this is a great way to go."
  },
  {
    "objectID": "archives/2012/08/03/share-your-knowledge/index.html",
    "href": "archives/2012/08/03/share-your-knowledge/index.html",
    "title": "Share Your Knowledge",
    "section": "",
    "text": "Unlike my friends at The Chicago Tribune, I have no logo to display, but my message here is as important as “Show Your Work”: Share Your Knowledge. In fact, it’s hard to do one properly without the other, so consider them a package deal.\nIn the more than seven years since I wrote “The Information Gap“, segments of the journalism community have gotten a whole lot better at preserving some of the things that we value and make our work better. Some of this is thanks to plentiful and cheap storage, some of it to code-sharing sites like Github, and some thanks to outfits dedicated to building, preserving and sharing best practices and groan-inducing foibles.\nWhat we’re less good at — still — is setting down the tasks and background information that doesn’t make it into the news but makes getting the news possible. But the same tools that make it possible for us to create an awesome tool for processing federal campaign finance filings also make it easier to collect and organize the secret sauce that makes our journalism remarkable – or just possible. All we need is the willingness to share our experience and the ability to keep it up.\nThere are some examples of this in action, including one by my former Washington Post colleague Nathaniel Vaughn-Kelso, a cartographer now with Stamen Design. Nate has been working on an open source GIS how-to that explains how and why to get started with open source mapping solutions. The section on setting up PostGIS alone makes a visit worthwhile.\nNow imagine if we had similar efforts by journalists experienced in various subject areas like food safety, or the energy industry or the criminal justice system. I’m not asking for, or expecting, people to voluntarily surrender key information like sources or other information very specific to a local beat. Instead, I’m thinking about the common materials of a beat or topic, and the right and wrong ways to make use of them.\nOne of the issues with this approach is that it works best if you’re able to gather a number of people to bring their experience and vantage points to bear. More hands make for a better result, and collaboration hasn’t always been the strong suit of journalists at competing news organizations. Let’s see if we can’t change that, in just one instance.\nAlong with Huffington Post developer Aaron Bycoffe, I’ve been working on a developer’s guide to working with Federal Election Commission data. This is geared towards people working with the data for web applications, but it also will include tips on using them for reporting in general, since our web apps are products of journalism. If you work with FEC data, or are just getting into it, please join us on the wiki. If you’re feeling a little shy, you could fork the entire repository and add a page or edits and we’ll get them back into the wiki.\nAnd if you’re not into campaign finance, then maybe you have other expertise to share. Your colleagues, to say nothing of journalism in general, will be better for it."
  },
  {
    "objectID": "archives/2012/12/03/thoughts-on-newsfoo/index.html",
    "href": "archives/2012/12/03/thoughts-on-newsfoo/index.html",
    "title": "Thoughts on NewsFoo",
    "section": "",
    "text": "In NewsFoo style, a brief introduction to this post:\nI was somewhat skeptical. I had an excellent time. There could be improvements.\nIf you’re looking for name-dropping and dish, this isn’t the post for you. I did meet all sorts of people who I hope to keep in touch with, and some very kindred spirits. You can probably figure them out via my Twitter feed if you want to. This is a mostly structural post, and I don’t want to leave people out, either.\nFirst, the skepticism. Invite-only gatherings in general make me kind of itch, regardless of the intentions of the conveners and the scope and format of the gathering. There’s also quite a bit of noise in “future of journalism” type events, in which the natural tension between those working at news organizations and those working to improve on or replace them gets magnified in ways that bring out the worst in both. I was worried that I would not find enough sessions to interest me, or that they would have too much New York-Silicon Valley representation (why yes, I do live outside those areas!) and thus be unrepresentative of issues and concerns that affect the news industry. Finally, I work for The New York Times. We’re often cited as examples of this trend or that mistake, and that comes with the territory. But it’s dangerous to extrapolate too much from what The Times does or doesn’t do. My skepticism didn’t entirely disappear (more about that in the suggestions section), but it was tempered quite a bit by the program put on by O’’Reilly, Knight and Google.\nFinding interesting sessions was no trouble at all. I took the advice of others and tried to hit up discussions with people I did not know, and that worked out quite well. These sessions were invariably led by passionate and smart people who truly wanted to test their theories and engage in a discussion about them. Circulating among the meal times and before/after sessions yielded lots of good introductions and conversations, even after nearly everyone asked whether I knew Nate Silver. We spent a decent amount of time talking about politics and political journalism, and the presence of Harper Reed from the Obama campaign did much to make those discussions serious and productive (there was some of the victory lap stuff that I had worried about, but more on that later, too). But we also talked about generating story ideas, about the beat system and about whether the filter bubble was a harmful thing, and what could be done about it.\nThere was a heavy Google influence on many of these sessions, which provides both a refreshing counter voice to those of journalists and a curious sort of reverence, as I mentioned on Twitter on Saturday. While it’s true that many of journalism’s issues, procedural and structural, cannot be solved by algorithm, hearing the perspective of people who care about news but don’t work in it was really valuable. Journalists, whatever their experiences and views, should always welcome such participation from people who care about what we do.\nFrom what I know about the first NewsFoo, it wasn’t exactly an optimist’s convention due to the changes in the industry. This one had a pretty positive vibe, although some of that I think was masked by other factors. If President Obama had lost re-election, I’m pretty confident that NewsFoo would have had a much different feel, thanks mostly to the ideologically homogenous nature of the attendees (more on that in a bit, too). People enjoy talking about successes, but it’s necessary to talk about why success is sometimes very difficult and what success even means in certain contexts.\nNow, the suggestions. Should I be invited to another NewsFoo (and I would gladly go again), these are the things that I would like to see changed or added:\nA problem primer. I would suggest an introductory session on Friday evening in which people in the news business (and some of their technology peers) lay out an honest but not maudlin list of serious problems that journalism has. Our conversations should be informed by this background, because while there are many “solutions” to some of these issues, there are also many structural problems that prevent an easy fix. For our part, journalists should listen to problems that others have in approaching and working with us. Then let’s get to work on finding answers.\nMore local, more international. I’ll venture that not many folks leaving Phoenix traveling within the U.S. had to switch to smaller planes to reach their final destinations. Too many NYC, DC, SF folks, and I’ll be happy to give up a spot if it means getting bright people from Colorado, Kentucky or Minnesota. Local journalism & technology have issues that national journalism does not. Same goes for international representation.\nIdeological diversity. I don’t believe there was a single person who voiced a political preference for anyone other than President Obama (to the extent that people did so). If there were, I’m not sure those persons would have felt comfortable doing so. This should be unacceptable for any gathering seeking a wide variety of input and voices. It would also help make conversations more thoughtful instead of enabling people to fall back on slogans and caricatures. The irony of hearing about filter bubble issues from a 99.999% crowd was a little jarring. Surely we don’t have to agree on politics to discuss substantive issues.\nSport. Let’s have some more sports people, and more discussion of sports and journalism’s role in covering it. Same with culture.\nGive back. It would be nice to see NewsFoo attendees try to do something to benefit the area that hosts it. Is there something we could contribute to journalists in Arizona that would help them do their jobs?\nRemind blowhards like me to shut up once in awhile. It’s usually the men who need this."
  },
  {
    "objectID": "archives/2012/12/26/the-data-driven-congressional-reporter/index.html",
    "href": "archives/2012/12/26/the-data-driven-congressional-reporter/index.html",
    "title": "The Data-Driven Congressional Reporter",
    "section": "",
    "text": "Washington is full of reporters who excel at finding and building sources, or at knowing which documents to look for and when. Those are skills that take time to develop and hone, but plenty of congressional reporters don’t have tons of experience under their belts. I didn’t have any when I arrived in 1998.\nThe best training I had for covering Congress was the one I received while working at Congressional Quarterly. The number of CQ alumni working in the Washington press corps is pretty formidable, and for a good reason: these folks understand how the House and the Senate work, and often are experts in particular policy areas such as agriculture or the budget process.\nBut there’s one area where I feel that most congressional reporters operate on roughly the same level, and that’s on the ability to find stories using data. Part of this situation is due to the overall lack of data skills among journalists and part of it comes from the wide range of ways that Congress produces data (some of them fairly complex). But two things are clear to me: there is so much more data about Congress than reporters can reasonably absorb without some level of automation, and virtually none of the existing press corps has a good approach to this problem.\nEven at its most basic level – simply the recognition and reporting of discrete “data points” – it’s pretty clear to me that the organizations that would benefit most from a programmatic approach to data don’t take advantage of it. I see this in some of my campaign finance tweets, many of which are derived from an internal dashboard that highlights specific filings or circumstances. You might argue that as I’m fairly obsessed with campaign finance, I should be able to spot this stuff. But there’s no way that I – or any reporter – can look at every filing to check for newsworthiness (or weirdness). There’s no way that every congressional reporter, or even most congressional reporters, are reading the Congressional Record for odd things, either. But our internal Congress app also alerts me to various changes or appearances (committee assignments and close votes among them).\nIt’s a legitimate argument that some (most, even) of these items may never lead directly to a story. But they can help fill in the gaps of a reporter’s awareness and understanding, and provide a certain range of context, too. They also can help build out a news organization’s own resources, so that when a reporter has a question, it’s possible that she can answer it right away rather than having to seek an external authority.\nAny congressional reporter should be to answer questions such as “How often do two members of the Senate vote together?” or “When is the last time that a specific group of Democrats voted for a Republican rule in the House?” Furthermore, the reporter should be able to answer these questions without paying someone else to do it – this is public data, after all, and there are people working to make it easier to obtain.\nNews organizations can, of course, pay for legislative and campaign finance data; it’s one of the ways that CQ makes money. But there are plenty of good reasons to develop in-house resources, not the least of which is that doing so makes also develops in-house expertise. More importantly, doing so broadens the number of ways in which you can find stories. Maybe you don’t have time to read the Record every day; wouldn’t it be great if you could set some simple rules for things of interest and have a computer do it for you? Wouldn’t it make sense that a computer could find the exception to the rule among a series of House votes that occurred while you were out interviewing people?\nThis isn’t some fantasy talk; if your news organization has people who can setup and maintain web publishing software, it’s pretty likely that they would know how to construct a system for alerting reporters to significant events in data. For campaign finance data, the basic pieces are in place. Congressional data is a little trickier because it has a number of different sources, but GovTrack is a good place to start.\nAll that’s really needed is a recognition that this stuff can lead to valuable discoveries and a willingness to consider it an important part of the reporting process. In a town like Washington, where who you know has long been considered as important as what you know, this requires a certain amount of change. I’m betting that the news organizations able to handle that transition will reap the rewards."
  },
  {
    "objectID": "archives/2012/12/15/on-vengeance/index.html",
    "href": "archives/2012/12/15/on-vengeance/index.html",
    "title": "On Vengeance",
    "section": "",
    "text": "I was 23 years old when I saw someone die, and my first thought about it is that I don’t recommend the experience.\nThe second thought I have about it is that maybe I do recommend it. I witnessed the execution of Roy Allen Stewart - a name I will never forget - because I was writing a story about the death penalty. At the time Gainesville, Florida, was gripped by the upcoming trial of Danny Rolling, who ultimately confessed to the murders of five students in 1990. The death penalty seemed a likely outcome for Rolling (he was put to death in 2006), so I took the opportunity to witness the electrocution of Stewart in order to better understand the process.\nWhen people ask, as they often do after murders, how anyone could bring themselves to kill another person, I think I might know an answer, and only because I was watching when Stewart was put to death. The entire event, from pre-dawn briefing at the state prison in Starke to the mid-morning breakfast at Hardee’s with Associated Press reporter Ron Word, was so clinical it seemed as if I was watching a movie, not an actual event. The coldness, the removal of humanity, was so overwhelming that it was not only possible, but likely, that you literally begin to think that it really wasn’t another person being killed that day. At least, not a person like you.\nThere was little public outcry about Stewart’s case; the victim, Margaret Haislip, had no immediate family and the only people connected to the case who were at Florida State Prison were two officers from the Miami-Dade police who worked the case. Stewart was not a sympathetic figure; he made no statements nor begged for mercy. He showed very little emotion; a priest who had been with him told reporters that Stewart had been more prepared for his own death “than I am for mine.” As he spoke, a truck drove by the prison and a man leaned out the window, shouting: “One less!”\nIt was an entirely depressing and disturbing day. Not because Stewart wasn’t guilty or didn’t deserve punishment, but because of what the process of death does to everyone involved.\nWhat I took away from Stewart’s execution is that even when we cry out for retribution, for vengeance, sometimes it will elude us. Sometimes what we get is relief, which is not the same idea at all. Sometimes we get nothing.\nI cannot imagine what those families in Connecticut are feeling, and I would not blame them at all for wanting to exact a price on anyone connected to the murders of their loved ones. I didn’t live in Gainesville in 1990, but after seeing the pictures from the murder scenes of Sonya Larson, Christina Powell, Christa Hoyt, Tracy Paules and Manny Taboada, I had no doubt that Rolling had forfeited any right to live. But his death brought no feeling to me at all, and I am not proud of that.\nI remain ambivalent about the death penalty, but I believe this: whatever we do as a society to make it easier for one person to kill another - and there are many ways of doing that - will lead us away from being the kind of people we like to see ourselves as. Whatever we do in the name of vengeance, we do to our collective conscience.\nThe trouble is, I know that I’d have a lot of trouble adhering to my own counsel were it my child, or my wife. This is why we need each other, and why there are no simple fixes."
  },
  {
    "objectID": "archives/2008/05/24/on-bomb-throwing/index.html",
    "href": "archives/2008/05/24/on-bomb-throwing/index.html",
    "title": "On Bomb-Throwing",
    "section": "",
    "text": "Note to visitors coming via Jay Rosen’s Twitter feed: Nowhere here do I say that Curley and his team were “not effective” at WPNI. Not effective as they could have been is a better reflection of my thoughts.\nSo: Curley and Co. to Las Vegas, one of the non-secret secrets of the Web journalism world. Among the reactions?\nPatrick Thornton: “Now the real fun will begin at WPNI. He is taking his whole team with him.” Steve Outing: “Big ouch for WashPost, eh?”\nWell, yes and no. I say this as a former WPNI employee who knows Curley and his team (we weren’t best buddies, but friendly enough). Let me also say that he and his team were always nice to me, and I admire them for doing what they do.\nBut to respond to Steve’s assessment, it would be a bigger ouch for WPNI if Curley’s team was more a part of the organization, but it was clear from early on that they were, if I can borrow the phrase, a pluggable app rather than deeply ingrained in the organization. Curley puts it this way: “I love The Washington Post and all that it stands for, but I probably wasn’t the best fit with the organization.”\nI agree. Here’s my take: Curley probably figured out quickly that being a bomb-thrower (in the best sense of the phrase) was gonna be tougher at the Post than it was at, say, Naples or Lawrence. He couldn’t get rid of the IT staff (at WPNI, his unit was entirely separate in every way, which was the best he could do). He couldn’t make people at the paper do his bidding, although certainly they sought him out to build things, and his team did some great work. But there was a lot of pushback; some of it for the wrong reasons, some of it entirely justified.\nAnd from the perspective of good portion of the staff of the Washington Post newspaper, this Curley guy talked a great game, but some were always a little skeptical. It’s a natural enough reaction, but at an organization that size, that’s a tough situation for anyone to overcome. So I think Curley and his team did what they could and didn’t get too attached. I can totally understand that choice, but the end result is that Curley’s departure won’t spell doom for WPNI, as his group was never really a part of the team. Will it reduce WPNI’s ability to produce some cool stuff? Absolutely. But it would hurt a lot more if circumstances were different.\nCertainly there was resentment at his arrival and the attention paid to it, which Curley couldn’t have done much to mitigate. And it’s not in his nature to do so, from what I saw. He’s a very smart guy, and there’s no need to apologize for that. He had to realize that some folks wouldn’t be excited to see him, or would be jealous of his resources and ability to pick and choose projects, so his team mostly kept their heads down and did their work.\nThat’s the thing about bomb-throwing: it works best when you control the environment around you (think Lawrence and most likely the situation awaiting Team Curley in Vegas). Throw a bomb when you’re surrounded by people who aren’t already your allies and you risk alienating more people than you intended. I say this as someone who routinely chucked bombs during my career. It may have been slightly more effective at smaller organizations; it certainly didn’t work at the Post when I tried it, and I was wrong to do it as often as I did. I didn’t pick my spots well, and as a result I alienated people who could have been my allies.\nI say this not just because Curley’s leaving, but because I see a lot of this behavior in the journalism blogging community. It’s not done with bad intentions, but it seems terribly counterproductive to me to condemn people who are facing an uncertain future and don’t know how to respond to it as people who “cannot be helped.” Shall we take them out back and shoot them, then? Once you start dividing your newsroom into people with a future and people without, imagine how much fun that’s going to be for everybody. Imagine the impact on your workplace culture.\nYeah, it sure feels good, in that revolutionary zeal sort of way, to toss some grenades over the wall. But who does it help? So much of journalism blogging is preaching to the choir that you get the sense that if rest of the industry just would get out of the way already, everything would be fine. Things are more complex than that, and it’s time we started being serious about that complexity.\nMay’s Carnival of Journalism question is this: “What should news organizations stop doing today, immediately, to make more time for innovation?” Which is a great academic question - because that’s pretty much what this exercise is: academic. Imagine, if you will, walking into a typical newsroom, picking out a person who has worked there for, say, 5-7 years and saying: “What you’re doing? Stop it. It’s worthless.”\nWhat, that’s not what you meant by the question? I’m sure every newspaper veteran will see it that way, too. My response was to suggest that we work on improving existing processes rather than trying to tell people who already fear for their futures that we don’t need them to do the job they’re trained to do anymore. These aren’t abstractions that can be neatly tied up in an all-encompassing blog post; these are real people. Just because someone is not willing to get fired doesn’t mean they’re not of value. I’m sorry, but if you’re 25 and single, you have yet to understand why losing a job would be a very bad thing for you and the people who depend upon you. Let’s not approach our colleagues with a raised middle finger when there are more productive avenues.\nWhile we’re at it, let’s not all pretend that we’re the cool kids rebelling against the man. Let’s not all assume that replacing 15-inch stories with blogs works in every circumstance. Let’s not all assume that the answers are so obvious that anyone who disagrees “just doesn’t get it.” There’s a gap, people, and talking about it only helps so much. I’d love to see more creation, more building of tools that will help us get to a better place. Advice is cheap, and worth the price in too many instances.\nMy intent here is not to shut people up; that’s not a good thing for anybody, either. It’s to encourage all of us to think a little longer before issuing sweeping pronouncements like “no more meeting stories” or “everyone must blog/twitter/facebook/pownce/whatever or else they’re useless.” Those are not fixes to real problems; they’re revolutionary slogans. And as revolutionaries the world over have found, revolution doesn’t always translate well once you’ve attained power. Then you have to make things work."
  },
  {
    "objectID": "archives/2008/07/22/the-birth-of-quadruplets-or-understanding-the-process/index.html",
    "href": "archives/2008/07/22/the-birth-of-quadruplets-or-understanding-the-process/index.html",
    "title": "The Birth of Quadruplets, or Understanding the Process",
    "section": "",
    "text": "My friend Dave Gulliver had a fascinating piece in his paper on Sunday about the birth of quadruplets in a Sarasota hospital. It’s a great story, but what makes it greater is that it was written by somebody with a certain amount of expertise on the subject of difficult premature multiple births. I hope Dave doesn’t mind, but I’d like to use that story as an example of why understanding the use of data is increasingly important for large swaths of journalism.\nThere’s a tendency among some folks in the industry to see CAR and other technological tools as just that - blunt instruments. Helpful, sure, but not ultimately necessary to the task of creating journalism. And for a segment of what journalism does, that’s probably ok. When we report on people and institutions that aren’t using technology to guide their decisions or actions, then an understanding of how data is used or certain technologies isn’t a necessity.\nI suppose a music critic needn’t understand much about databases, for example, but reporters covering government, business, college or professional sports, to name a few, should be able to assess their subjects the way that people inside those sectors do. And increasingly, that means understanding the use of data. Many local governments base their police staffing - who covers where - on a non-stop flow of crime data. Sports teams pour over tape, logging their opponents’ tendencies in preparation for upcoming games. Businesses are all about the numbers, too.\nAnd then there’s politics. Winning elections these days is very often about putting together enough voters to crack 50%. There’s microtargeting based on consumer data and door-to-door canvassing so that volunteers can input demographic data into centralized servers. They’re not doing that just for fun - it’s valuable information. But if journalists can’t really grasp how organizations are using data, we’re liable to miss the effects, and thus miss some fuller explanations of events. Yes, we can rely on people to tell us what’s happening - and we should - but if data plays a big part in the life of an organization, the reporter covering it should have some basis to evaluate that role.\nSo how does that relate to Dave’s story about the quads? Well, after reading it, I noticed that there were some subtle bits of detail that I never would have thought to include or been able to describe as well - about how the NICU operates, the details of the births. That’s because Dave has been there with his twin boys. A parent of a child born without complications or a single person would have been hard-pressed to write as good a story. I sure wouldn’t have been able to do so.\nIt’s the same idea when it comes to understanding the basis for decisions that come from, at least in part, the collection and consumption of data. It’s can mean the difference between telling a story and telling a better story. I’m sure plenty of organizations that we cover would be happy to have reporters who are in the dark about these things. But that doesn’t help our readers any.\nSo, technology and data as a tool? Yes. But when the tools become a crucial part of the world we cover, understanding how they work and being able to use them makes us better journalists."
  },
  {
    "objectID": "archives/2008/09/21/the-difference/index.html",
    "href": "archives/2008/09/21/the-difference/index.html",
    "title": "The Difference",
    "section": "",
    "text": "One of the things I try to stress to students in my computer-assisted reporting class at GW each spring is the difference between a story based largely on anecdotes or temporal observation and the same story with the addition of a definitive analysis of data. The LIRR story in today’s Times by Walt Bogdanich, Andy Lehren and a host of contributors is a great example of the latter.\nThere’s little doubt that a good reporter who looked hard enough at the LIRR would start to learn that disability payments for retirees were more frequent than one might expect. Tales about people retiring and getting huge disability payments, as the story details. You’d be able to get the pictures of folks receiving large disability payments playing golf, as the reporters on this story did. But nailing the story - removing any doubt that this isn’t a handful of isolated cases - takes the kind of analysis that Andy Lehren did. The kind that leads to a reporter being able to write this:\n“Virtually every career employee - as many as 97 percent in one recent year - applies for and gets disability payments soon after retirement.”\nCase closed. Airtight. Dare I say George Tenet might trot out “slam dunk” again for this one. My only quibble is that Andy didn’t get a byline. Congrats to Andy, Walt and the rest of my colleagues who worked on this. And for showing the difference that good data analysis can make to journalism."
  },
  {
    "objectID": "archives/2008/09/23/white-house-beat-feature-request/index.html",
    "href": "archives/2008/09/23/white-house-beat-feature-request/index.html",
    "title": "White House Beat Feature Request",
    "section": "",
    "text": "Ok, I love the fact that CBS Radio’s Mark Knoller keeps such good tabs on presidential travel, but can somebody please come up with a backup plan in case, heaven forbid, Knoller gets hit by a bus or something? Is this too much to ask of WH reporters? This is the kind of thing where somebody out there is going to do a better job of gathering and displaying this data on the Web. And it should be us, cause it’s our job."
  },
  {
    "objectID": "archives/2008/06/29/caspios-lessons/index.html",
    "href": "archives/2008/06/29/caspios-lessons/index.html",
    "title": "Caspio’s Lessons",
    "section": "",
    "text": "Been awhile since I wrote about Caspio, and since then they’ve only gained more media clients, which I suppose could be a lesson for me. But I think not. Rather, I hope what we’ll see in the months and years to come are the lessons that Matt Wynn offers from his experiences using Caspio. Here’s your nutgraf: “My conclusion on Caspio is that they do one thing very well. But other, cheaper alternatives do it just as well. Further, to learn to make it do otherwise seems pointless, especially seeing as we would be paying for the luxury of learning to hack it.” (The emphasis is mine.)\nCaspio’s David Milliron spoke at this year’s Special Libraries Association conference at a panel organized by SLA’s News Division, which includes many newspaper and broadcast librarians. It’s easy to see why: a lot of these folks are being asked to do new things, to be more involved with their organization’s Web sites, and to do it with fewer people. Seems like a pretty good opportunity for Caspio, and I don’t fault them for recognizing that. The problem I have is that the promise of Caspio is in the short-term; no matter how many features they add (my personal favorite being the Data Sheet Find and Replace one: “You no longer need to export your table outside of Caspio Bridge for this type data modification.”), you’ll never get the flexibility and control over your apps that you do when you build your own stuff. Despite what Milliron says, there are very real and serious differences between Caspio and Web application frameworks.\nMaybe that’s the real lesson that journalism folks need to heed: that the costs of learning Caspio go beyond the monthly fees and the potential cost of switching to another tool (having to re-do your existing apps). Caspio is, as Matt says, good at doing some pretty basic stuff when it comes to putting data online. But if you want to go beyond Ye Olde Data Ghettoe, you’ll have to learn some programming anyway. So why learn something that can only be used on a closed system that you have to rent? Matt’s alternative happens to be PHP/MySQL based, but he’s not going to be paying for using either of those. And if suddenly MySQL decides to charge corporate users or something equally far-fetched, he can switch to Postgres or SQLite without starting from scratch.\nI realize many, many folks in newsrooms can say, “Um, pardon me, but we don’t have a Matt Wynn.” Or maybe you do, but he’s insanely busy all the time. That’s a very common situation. But the real long-term question is this: if your organization is never going to want to do anything more than put up isolated search pages serving up content that no search engine can reliably find, you’re still gonna pay every month for that privilege by using Caspio. And if you hope and plan on doing more someday, even if that’s not today, then you’ll have almost nothing to transfer to that effort by using Caspio, since one of their chief claims is that you don’t have to learn any programming to use it.\nSo if learning more is a part of your plan, why not spend the time learning a system that doesn’t charge you for that time? By adding Caspio experience to your resume, what real skills have you gained aside from the ability to point and click?"
  },
  {
    "objectID": "archives/2008/06/19/the-future-of-news-libraries/index.html",
    "href": "archives/2008/06/19/the-future-of-news-libraries/index.html",
    "title": "The Future of News Libraries",
    "section": "",
    "text": "At the recently-completed SLA conference in Seattle, Nora Paul led a session on the “future of news libraries” that asked the attendees to imagine 2012, when librarians (or news researchers, or whatever you want to call them) are recognized as leaders of innovation in newsrooms, and then to explain how that came to pass. It was an ambitious and worthy session, and I’d like to see more of them among the News Division crowd. But to be honest, some of the answers worried me. I didn’t see the future unfolding the way we’d all like when I heard some of the responses to Nora’s questions.\nThe problem isn’t the people. I go to the SLA conference even though it has a tangential relationship (at best) to my current job because that’s where I find a group of really smart people that span nearly every field: news, law, government, engineering, technology, health, you name it. The session topics are eclectic, interesting and well-attended. People don’t wander off much.\nBut the problem isn’t the business climate for news, either. At least not totally. It’s a complex situation, in which a combination of factors keep a lot of news librarians anxious about their jobs and their futures. There’s a whole new set of content to archive, dwindling staff resources to deal with and a main set of consumers - reporters and editors - who remain by and large too ill-informed about the best way to find and manage needed information.\nSo there’s a tendency to think that if news libraries continue to provide what newsrooms want, and do it well, that things will be ok. Yes, duties will change, and people will adapt, but fundamentally it’s pretty much the same goal: finding information and turning it into knowledge.\nExcept that now we as news organizations are competing with, well, pretty much anybody who wants to be a guide to information, and on multiple fronts. And while most of these new competitors should also be our consumers, they are not bound by some of the ideas that have shaped how libraries have worked.\nThe first, and most important in my mind, is freeing ourselves from near-total dependence on vendors. Vendors will always have a role, as there are some non-core tasks that we should rightly hand to them, and some core resources that only they can provide. But I cannot imagine how news libraries will becoming engines of innovation if they do not control, or seek to control, the tools of their trade. They can no more outsource the future than the rest of the paper can, and it’s time to consider how news libraries can produce both better tools that can lead to better products.\nDuring a session that Jessica Baumgart and I had the privilege of speaking at on Monday, I got a question from a woman in the audience who wanted to know how it was a good thing for the industry if news librarians picked up programming skills and then got better, higher-paying jobs in IT. I answered that such an occurrence was the industry’s problem, but this is what I should have said: I encourage librarians to develop these skills precisely so that they will be better librarians, better researchers. So that they can better and more efficiently manage the ever-increasing flow of information. So that they can take control of their own futures in a way that they will not be able to do without those changes. Some will leave the news library; I did. But if we stay in the news business, we’ll still be interested in solving the problems of the newsroom. And we’ll be able to contribute in even more meaningful ways (and fail sometimes along the way).\nI wrote in November 2005 about ways that news libraries could actually become innovation centers, but I didn’t go far enough. Newsrooms desperately need people who can generate and then execute new ideas - for improvements in the news process and for products, to name two areas. I heard at SLA about papers forming employee committees to discuss and propose new ideas, which is great, but what if the library was the place where you could prototype and even build out some of those ideas that involve the existing content of the paper? Those committees aren’t going to be permanent, and most news organizations can’t afford to hire an R&D department.\nNews librarians regularly solve problems that vex 30-year veterans of the newsroom, and they often are the source of last resort for the people who need good, accurate information. But until they can be less reactive and until they start developing their own tools, getting to Nora’s 2012 scenario will be tough.\nSo let’s get started. SLA has created an innovation section on its site, and it’s worth taking the time to explore. Or try this effort by a fellow librarian and developer, Daniel Chudnov of the Library of Congress. Try Python, or Ruby or another programming language. Think of a concrete task - maybe some repetitive work you have to do that you’d like to automate - and see if you can’t solve it. Maybe not the first time out, or the second, but failure is a part of innovation. Not trying is the only real failure."
  },
  {
    "objectID": "archives/2008/12/19/represent-and-geodjango/index.html",
    "href": "archives/2008/12/19/represent-and-geodjango/index.html",
    "title": "Represent and GeoDjango",
    "section": "",
    "text": "For more details on Represent, see our post on the NYT’s Open blog.\nToday (well, technically last night) at work we launched a beta Web application called Represent that helps New York City residents keep tabs on what their elected officials are doing. It’s the product of an idea that my colleague Andrei Scheinkman suggested when we sat down in the late Spring to kick around ideas for the Times’ internal technology challenge. We ended up being a finalist with our entry, which became the app you see today, thanks to design work by Stephan Weitberg and advice from a lot of folks.\nSince Andrei and I both liked Python, and the contest asked for a working prototype, we built Represent using GeoDjango, the part of Django that makes spatial work easy enough that even I can manage it. This is my first time really dealing with GIS data, and probably the hardest part was getting all the dependencies installed the first time (after you do it once, the install becomes fairly routine, and on the latest Ubuntu version it’s simple).\nSo if you’re thinking about going the GeoDjango route, or trying it out, I’d offer these suggestions:\n\nGo with Ubuntu Linux as your OS. It’s just easier. You can run GeoDjango on Windows, but I think I hate the Postgres Windows install and configure process more than just about anything I’ve tried to do on Windows.\nWhen installing, do what the docs say. These are written by people who have already installed it. Once you start freelancing, your chance of success goes down dramatically.\nMake sure that when you install Postgres, you test it out via the psql shell.\nWhen in doubt, try the #geodjango IRC channel on FreeNode\nWhen things just work, do not be surprised or await a message saying they worked.\n\nOnce you get GeoDjango installed - and it might take awhile the first time - you’ll be really impressed with how easy it is to use. I’ve written before about the power of LayerMapping, which is a huge gift to people who want to move from using Shapefiles to using GeoDjango. And there are other goodies, too, like the ability to plug into the GeoIP library to place your site visitors on a map (or configure their content based on that information). For Represent, we needed to produce KML files of each district that would display on the results pages. Turns out GeoDjango does that, too. To my eternal shame, I never really got the hang of working with Arc. Now I have a tool that suits me better."
  },
  {
    "objectID": "archives/2007/05/29/shoot-the-google/index.html",
    "href": "archives/2007/05/29/shoot-the-google/index.html",
    "title": "Shoot the Google",
    "section": "",
    "text": "Pieces like the one Neil Henry, formerly of the Washington Post, wrote in today’s San Francisco Chronicle, annoy me to no end. They are written by passionate, intelligent people who have become so disheartened by challenges to their beloved institutions that they feel the need to lash out at their perceived villains. First it was Wall Street. Henry picks Google, because if there’s anything that has limited the flow of information (which would be a primary goal of journalism), it’s Google, right?\nHenry’s main argument begins well: he summarizes the various economic pressures brought about by the Internet. Not much dispute there - we’ve got lots of pressure on us. Then he runs off the rails: “As a result, newspapers such as The Chronicle must make staff cuts to survive — and increasingly it is highly skilled professional journalists committed to seeking the truth and reporting it, independently and without fear or favor, who must go.” Or instead of just cutting, and this is just me being crazy here, maybe they could, you know, do things better? Maybe people like Google and craigslist because they are easy to use and don’t require irrational steps (I’m looking at you, Newhouse newspaper sites). How is it that the industry that dominated classified advertising didn’t see craigslist coming and was not able to preempt it? Because to us, innovation has meant color photos and putting navigation rails on the left side (or wait, the right!).\nThe other thing that galls me about Henry’s piece is this gem:\nI see a world where corporations such as Google and Yahoo continue to enrich themselves with little returning to journalistic enterprises, all this ultimately at the expense of legions of professional reporters across America, now out of work because their employers in “old” media could not afford to pay them.\nNow, I’m not sure what other non-news corporations have returned to the journalistic enterprise, outside of ad revenue and the occasional PR job for reporters. IBM didn’t have to give back to the typewriter industry because it made typewriters before pivoting in favor of personal computers. I guess Henry’s argument is that corporations should just go back to being advertisers only: hand over the money and leave journalism alone. But if there are tasks out there that Google can do better - and there are some - why should they pay a fee to the industry that has mostly declined to do them?\nGoogle has always maintained that it is not trying to kill newspapers. That doesn’t seem to matter to Henry: “While that may be true, the time has come for corporations such as Google to accept more responsibility for the future of American journalism, in recognition of the threat ‘computer science’ poses to journalism’s place in a democratic society.”\nIn other words, someone has to pay. Some questions come to mind:\nThe “threat” of computer science? Would this be like the threat of the telephone and the threat of the television? The computer is a tool. How you use it (or whether you do at all) is up to you. It’s pretty clear how most of the journalism industry uses that particular tool, and what some of the consequences are for doing so. Besides, what kind of independent press demands that a corporation that indirectly competes with it also subsidize its existence?\nHenry also wonders why Google can’t support journalism education. Maybe because journalism education has been even less innovative than the industry itself, a remarkable achievement. If you ran Google, would you hire more talented and ambitious programmers who know how to solve problems, or would you give the money to places that pretty much since the decline of the two-paper city have done their level best to put their hands over their ears and shout “I can’t hear you!” in response to the changes in technology and society?\nHenry asks if Google could not “somehow engage and support the traditional news industry and important local newspapers more fully,” although he doesn’t describe what this would mean. Google probably would be happy to “engage the news industry” if said industry was loaded with people who really understood and could make the best use of its talents. Or maybe he just means a nice fat check.\nPeople who have worked in the media for many years probably have a hard time recognizing the industry they started out in. Heck, I have a hard time, and I started 12 years ago. That has to be jarring and even distressing at times. But change happens in every industry, and has happened before in ours. Henry’s “solution” to the “problem” posed by the greatest communication platform in the history of the world comes from the heart - I’ll give him that. It also has some of that “blame the messenger” reaction journalists have heard once or twice.\nWe cannot goad or guilt companies like Google into saving journalism when there is much about our own processes that we need to improve. If we used the information we collect in more useful ways and figured out how to present information that makes it harder to ignore, maybe we’d have a better financial outlook. But that is our problem. It existed before Google and craigslist. That we didn’t tackle it then is not the fault of smart engineers who took advantage of our weaknesses.\nAfter the people who run and staff news organizations accept their share of responsibility for the future of American journalism - and then do something about it that doesn’t involve hand-wringing and finger-pointing - then maybe then we can ask Google to shoulder whatever is left."
  },
  {
    "objectID": "archives/2007/02/04/why-the-web/index.html",
    "href": "archives/2007/02/04/why-the-web/index.html",
    "title": "Why The Web",
    "section": "",
    "text": "Friday was my last day as an employee of The Washington Post newspaper. As I told my newsroom colleagues, it’s sort of a ludicrous thing for me to contemplate, since I barely imagined even getting to a paper like The Post. I was lucky to attract the attention of my former boss, Bridget Roeber, who worked hard to create a position for me and to recruit me (not that that took much). And I owe my next job, as database editor at washingtonpost.com, to my time at The Post and the people there who let me explore my web tendencies. For a large media company, it is remarkably willing to try new things, and that’s gives me a lot of hope for the future.\nThat said, there have been several of my colleagues who, when informed of my plans, responded with a single query and often an uncomprehending look: “Why?” To many of them, the creation of the journalism is the most important work, one that somehow gets lessened the further away from the newsroom one goes. It’s a cultural thing, and I’m sure many website employees know what I’m talking about. But it doesn’t make it any less urgent that we as an industry tackle this gap. It’s one of the things I hope to be able to do at washingtonpost.com – be an example of how you can mix the newsroom and the website in a way that sustains and encourages both rather than takes from one to grow the other. Maybe the staffing ratios will grow increasingly even over time; maybe websites will have more of their “own” editorial employees. What’s important is that the entire operation recognize that we succeed or fail together. It’s not “the paper or the web,” but the journalism everywhere. And for me, that job starts tomorrow."
  },
  {
    "objectID": "archives/2007/11/06/innovation-belongs-in-the-newsroom/index.html",
    "href": "archives/2007/11/06/innovation-belongs-in-the-newsroom/index.html",
    "title": "Innovation Belongs in the Newsroom",
    "section": "",
    "text": "There’s quite a bit of useful discussion about the recent big moves in the social networking space and what it means, if anything, for the news industry. Steve Yelvington’s advice is particularly good: “Given the magnitude of the change in Web consumption behavior brought about by social networking sites, newspaper companies need to think about how their content, tools and services might interoperate with these standards.”\nBut to even start that process, newspaper companies need to think of something more fundamental: how to put their employees into position to think of, sketch out and implement good ideas in the social network space and elsewhere online. As an industry, we suck at that. Not because we’re inherently stupid, but because the newspaper is such a different product - communal in the sense that it’s published for a mass audience, but generic because of the same reason. Niche ideas? Good luck - perhaps an inside section could be the place for a popular niche feature, but any good reading of what’s been going on with papers shows that niche sections have been killed off.\nBut on the Web, niche is king. It’s what enables papers to go a mile deep on some subjects while mostly ignoring others. It’s what makes college sports fans return to the local newspaper’s site even when they wouldn’t read it for anything else. And it’s the most likely destination for good, new ideas for the news business.\nSome folks have suggested that the newspaper industry needs to cooperate on technical processes and innovations that could be used industry-wide. This is a crutch, and poorly-constructed one at that. The problem is that when you take the role of innovation out of the newsroom and put it with some consortium or committee or association, it dilutes the urgency and the applicability of the result. On a more practical level, when has journalism education really been the source of innovation for the industry?\nNews innovation needs to be in the newsroom, and individual papers have to figure out how to make that happen. For some, it’s giving the keys to a smart and idea-filled staffer, hoping that other employees will follow. For others, it’s hiring people to write the software that makes a news site better and also brings in additional revenue. But the pressure to innovate needs to come from within, not from some consortium that will allow hesitant publishers to keep their distance from whatever recommendations are issued. This is too important to be left to anyone else.\nThe other fallacy behind the consortium idea is that no other way of collecting and distributing this kind of knowledge exists. If this were true, then the open-source software model would not work, but it does. People who are innovating within the industry are sharing that knowledge already online; it’s not hard to find this information and adapt it to your own needs. No committee, however well-managed, will be able to provide the same kind of speed and flexibility.\nIn a special report on innovation two weeks ago, The Economist published a section entitled “We are all innovators now.” A key excerpt:\nStewart Brand, an internet pioneer, has famously argued that “information wants to be free.” So surely the knowledge worker, the creator of that information, also needs the same freedom. Companies and governments can find an innovator inside everyone; they just need to liberate them. Moreover, the rising tide of inventions that make one country wealthy benefits others that bring those clever ideas to market or simply make use of those products, processes and services.\nNews organizations: find your innovators and liberate them inside the newsroom. Steal the best ideas from the other innovators out there. Just don’t punt this to a committee."
  },
  {
    "objectID": "archives/2007/11/15/of-the-web-vs-on-the-web/index.html",
    "href": "archives/2007/11/15/of-the-web-vs-on-the-web/index.html",
    "title": "Of the Web vs. On the Web",
    "section": "",
    "text": "News organizations spend a lot of time talking about what they’re doing “on the Web,” but there’s another phrase that’s more important from a long-term perspective: “of the Web.” There’s a significant difference, although it can be abstract to folks who don’t spend a lot of time thinking about online. A simple difference between the two phrases can be illustrated by news organizations that simply put their content online and those that do go beyond that and use the capabilities of the Web through links and other resources. (Jacob has a related example for software).\nWhich brings me to CARMA. Today, the Post and other news organizations published stories about CARMA’s efforts to track carbon-emitting power plants worldwide. The Post published a small map with the story in the paper, while other news organizations alerted readers to the existence of the site. This is “on the Web” stuff - valuable, but not much more so than what’s in the paper.\nA couple of days ago, in a time-honored tactic, CARMA sent out advance notice to journalists and offered a sneak peek at the database. Out of curiosity I visited the site, which is nicely done (although running a bit slow today, likely a result of the traffic). And then I saw the bit that the press release didn’t talk about: the API. In short, for those folks who want to use the data in some way that CARMA didn’t think of, there’s an option that delivers information over HTTP. The API, and the options it provides, is definitely “of the Web.”\nAPIs are something don’t come naturally to news organizations, WPNI included, but they should be a larger part of our thinking. Consuming data via API when available is a no-brainer; it replaces the time-consuming and often repetitive tasks of requesting and processing data. You change your data processes when the API changes. Producing APIs, on the other hand, is a slightly tricker business, since there are plenty of news folks who get nervous when you talk about providing published content to others with no inclination of how those people will use it.\nIn some cases we’re already doing APIs, of a sort at least. Take RSS feeds, which can be consumed and transformed for different purposes. But few of the database applications produced by news organizations have such feeds. The current widget rage is nice, but it hides the data from these widgets behind Flash and other wrappers. The appeal is control - we know exactly what people are doing with our widgets, because we control their actions. The loss is the uses we haven’t even thought about.\nWhat really struck me about CARMA’s API is that the press release to news organization didn’t mention it at all. That’s because it isn’t intended for us, and that should worry news people. Think about it: the groups that have useful and interesting data are building alternate routes around the news media, maybe because they were told that APIs are the next big thing, or maybe because they know that getting their information in front of as many people as possible requires a certain amount of letting go. Either way, Web users are going to find CARMA’s data - some of them driven from a news Web site, but some of them without ever having visited one. What’s connecting those users and that data is HTTP - the “lingua franca of our age,” as Jacob puts it. Being “on the Web” - particularly for database applications - is not enough. We must strive to be “of the Web.”"
  },
  {
    "objectID": "archives/2007/10/16/the-times/index.html",
    "href": "archives/2007/10/16/the-times/index.html",
    "title": "The Times",
    "section": "",
    "text": "I originally tried to write something about this with the weightiness I felt it deserves, but it turned out to be, as a friend accurately assessed, “corny.”” So I’ll just say it: I’ve accepted a job with nytimes.com, where I’ll be joining a talented group of developers to build Web apps.\nThis move says less about washingtonpost.com, where I have had the privilege of working since February (and in collaboration with since 2005) than it does about the opportunity in front of me. I have been deeply impressed with the recent moves that the Times has made to further embrace the Web, and I will be fortunate to be able to call upon more than a few exceptionally talented colleagues. Resources being what they are these days in the news business, this chance was too good to pass on, and I’m very excited about what lies ahead.\nWeb development at WPNI remains quite alive, thanks to the efforts of people like Peter Harkins and Alyson Hurt. I won’t particularly enjoy having to compete with them. Web development has the support of the newsroom leadership and I would not hesitate to recommend WPNI to anyone who wants to test the possibilities of journalism on the Web. I cannot say enough about my colleagues and their dedication, particularly Adrian Holovaty, who encouraged me and gracefully endured scores of questions.\nSome other details: yes, the Times will be a Rails shop, which means I won’t be using Django professionally, but I’ll still be using it for my own stuff. I’ll be working from home, which is a very nice arrangement. And one of the great aspects is that after a three-year hiatus, I’ll be reunited with my good friend Aron Pilhofer, who works on the Times’ CAR team. My first day is November 26. Wish me luck."
  },
  {
    "objectID": "archives/2007/07/11/the-original-and-future-facebook/index.html",
    "href": "archives/2007/07/11/the-original-and-future-facebook/index.html",
    "title": "The Original (and Future?) Facebook",
    "section": "",
    "text": "So the people who run newspapers are probably looking at Facebook, which is enjoying traffic growth that surely has advertisers lining up to try and get a piece of that action, and saying to themselves, “Why can’t we do something like this?” (Warning: irony approaching.)\nMany of my kindred spirits, notably Adrian, have for quite a while now tried to drive home the point that news organizations, especially newspapers, aren’t doing enough with the information they collect. There’s a portion of Adrian’s usual presentation on this stuff where he breaks down the details of a crime - it has a victim, a date, a location, a time, etc. - and then applies the same concept to marriages and obituaries.\nThe basic lesson is one I’ve said before, but probably is worth repeating: we collect a ton of information and only think about how it will help shape a smallish blob of text in tomorrow’s paper. And then we forget about it and move on.\nNow here’s the ironic bit. If you think about it, newspaper wedding announcements and obituaries (in particular) are the original social networking application: people volunteer or even pay to have these announcements printed, they hand over personal information that could be quite interesting or relevant in other situations and they really, really like to see those announcements in the paper. They see long-lost friends or former co-workers and it brings back a surge of memories and feelings. Obits are like crack for a certain set of readers, and I dare say I’m not the only one who enjoys perusing the numerous descendants of Mayflower passengers who seem to turn up in the New York Times Sunday Styles weddings section each week.\nYou might be thinking, “Ok, but … the bit about data? Where does that come in?” Well, imagine if you could browse weddings and obits like you browse Facebook. Just because you’re no longer single or alive doesn’t mean you don’t retain networks and connections that people find interesting. And this has journalistic implications, too: if the newspaper in a place that hosted, say, a factory that worked with toxic materials databased its obits by cause of death, home address and employer, it might discover some patterns and trends before the public health authorities did. Or if a newspaper databased its wedding announcements it might better understand trends in social life, such as changes in the age when people marry. Most obits and wedding announcements provide employment information, family members and educational backgrounds. All of that could be used to locate people or get inside networks of former employees or school graduates.\nThink of it this way: have you ever asked your newsroom colleagues a question like this? “For a story, I need to talk to people who attended Lincoln High School between 1997 and 1999. Can anybody help?” Um, yes, your wedding or engagement announcements probably can. That is, if you let them.\nSo sure, the bad news is that one of newspapers’ social networking apps is for people who are no longer alive. But as any person who has ever written an obit knows, that doesn’t make them less important or interesting. Any good paper wants to reflect and understand its community. Why are we throwing away information that would help us with those tasks?"
  },
  {
    "objectID": "archives/2007/07/31/django-ical-vobject/index.html",
    "href": "archives/2007/07/31/django-ical-vobject/index.html",
    "title": "Django, iCal and vObject",
    "section": "",
    "text": "For one of our Django applications at work we received a request to add iCal feeds to accompany the RSS feeds available for each candidate’s page (example here). I first thought about doing this using a hard-coded template, the way described in March on boomby.com. So I did and it worked - the directions are great. Since I happened to be at OSCON, I showed it to Jacob Kaplan-Moss, who immediately recommended that I check out vObject, a Python library for generating iCal files.\nWhy use vObject? Well, for one, it eliminates the need for a template file, and the iCal spec being a slightly cranky thing, that’s for the better. How cranky? Well, it doesn’t like carriage returns in template, for one thing. It also means that Django treats iCal feeds much the same as RSS feeds in terms of generation. So using vObject means you set up the file in the view, call the serialize() function to fill out the standard stuff and return an HttpResponse, setting the mimetype to text/calendar.\nSeems pretty simple? It is, with one exception. Internet Explorer (of course). To ensure that Outlook correctly handles the iCal files, you need to add a few lines to the calendar object and to each event. Specifically, you need to do the following inside your view:\ncal = vobject.iCalendar()\ncal.add('method').value = 'PUBLISH'  # IE/Outlook needs this\nfor event in event_list:\n    vevent = cal.add('vevent')\n    ... # add your event details\nicalstream = cal.serialize()\nresponse = HttpResponse(icalstream, mimetype='text/calendar')\nresponse['Filename'] = 'filename.ics'  # IE needs this\nresponse['Content-Disposition'] = 'attachment; filename=filename.ics'\nOutlook seems to require three datetime fields for each event: DTSTART, DTEND and DTSTAMP (even if you don’t have values for all of them). Other consumers of .ics files do not, and what works on Windows with Firefox and Outlook may not with IE and Outlook. But it does work if you follow the steps, and vObject makes it much easier.\nThe only other thing you need is to set up the url in Django’s urlconf and have it call the view. No template needed! Here’s a sample iCal file listing Mike Huckabee’s upcoming visits.\nThanks to Jacob, Malcolm Tredinnick and Simon Willison for their advice on this. Any mistakes are mine, however, and comments or suggestions are welcome."
  },
  {
    "objectID": "archives/2007/09/07/outsourcing-database-development-or-the-caspio-issue/index.html",
    "href": "archives/2007/09/07/outsourcing-database-development-or-the-caspio-issue/index.html",
    "title": "Outsourcing Database Development, or the Caspio Issue",
    "section": "",
    "text": "Updated: Caspio’s David Milliron responds in the comments.\nThe good news is that there are plenty more databases served up on newspaper Web sites than there used to be. Some papers are organizing entire desks around data. The bad news - and people can disagree on this - is that in some cases, the papers aren’t really doing much in the way of Web database development. They’re outsourcing much of the work to Caspio and its Bridge application.\nThis can’t be such a bad thing, right? I mean, more databases online is a good thing, and of all people, I should be encouraging any way to get the stuff up. Unfortunately, it’s not that simple. By leaving the work to Caspio and reducing database development to the safety of point and click, news organizations are far more likely to end up with a bunch of cookie-cutter apps that go just far enough to satisfy the “hey, we need some databases” crowd but not nearly far enough to hold the attention of readers and provide a real service.\nGive Caspio this much: it spotted an opportunity in an industry that’s trying to do more on the Web with less, and the company has managed to sign up clients including the Indianapolis Star, the Arizona Daily Star, the Palm Beach Post and the Atlanta Journal-Constitution. The pitch is attractive: Web databases in hours, not weeks, and all you have to do is load the data and decide what how to display it. Indeed, Caspio boasts “No programming skills required” to use it. (I should say that I have not seen Caspio Bridge in person and have only had it described to me.)\nSo what’s the harm? I see several. First, Caspio’s product is an abstraction, built atop a database server (SQL Server, in this case) without giving users all of the power of that DBMS. It can’t, because in order to make things simple, it has to limit the ability of users. That translates into, for example, two choices for storing text: a maximum 255-character VARCHAR field and a maximum 4000-character VARCHAR field, even though SQL Server supports more. When you have a text field that always will be, say, 5 characters or less, it makes no sense to use a 255-character field. A small app won’t be affected in most cases, but larger ones could see a performance hit, especially on searches using wildcards. In addition, Caspio says it supports importing from text, MS Access databases and something called Caspio Bridge XML, which means that “only XML files previously generated by Caspio Bridge can be imported. XML files from other sources are not supported.” It also says, with no apparent irony, that “Text files are the least-desired import file formats because they cannot contain field data type information. If possible, import your data in one of the other formats. Caspio Bridge assigns a Text (4000) data type to all fields in tables imported as text files, unless you choose to appended (sic) or replace an existing table.” So it creates excessively large fields when importing text files, can’t deal with most XML and encourages the use of Access, which in practice means that very large datasets are going to be fun to deal with. Oh, and your organization (or Web site) doesn’t run on Windows? You may be out of luck.\nSecond, the data isn’t on your servers. I’m not sure what news organization would put a critical and exclusive data application on servers not under its control. That leaves room for plenty of potentially interesting data apps, like one charting Cleveland Browns’ games since 1946 from The Plain Dealer, but the lack of flexibility and control involved is ultimately going to be frustrating. If the PD wants to expand the Browns app, it has to add data to its database on Caspio’s servers. Can one Caspio app use data from another? It’s not clear from the docs. What about serving data to other types of apps like Flash? Nope. Want to use your Caspio app to build a Google Maps mashup? That’ll be an extra charge.\nThird, Caspio makes it sound like programmers and developers only make things worse. They play on the ignorance of people who don’t know what good programmers can do. An example, from the online help: “A scripted function written by a programmer for you is handcrafted, and may not meet the quality and reliability standards that your application or web site demands.” Um, sure, if you have your neighbor’s 13-year-old write your database apps. Or this: “Hand-coded scripts are difficult to edit and maintain because they contain hundreds of lines of code that may or may not be properly documented. The original programmer may no longer be available, or the logic behind the script may be forgotten.” Apparently Caspio hasn’t heard of version control or documentation or, gasp, even Web frameworks that abstract many of the simple functions that it describes as potentially being convoluted. Caspio’s sales pitch seems to be: “Hey, you don’t know anything about Web database development, and you shouldn’t trust anybody who does.”\nLastly, I believe that anybody who has done any kind of work with data realizes that you learn more about the data by being more involved in its use. Some of the best features on the Congress Votes Database came to us only after we had spent time doing the Web development. So while speed-to-deployment is a very attractive feature, many times it results in a one-and-done approach: just slap it up and move on. This would be less risky if there weren’t smart people outside the media who can and will get their hands on useful data and do a better job with it then we are doing. News organizations can’t afford to rely on an approach that limits their choices and encourages quick but shallow development.\nCaspio may have an upside; news organizations may come to realize that the value of putting up a database requires that they invest more time and effort, not less, and that it works better if they have the highest degree of control and flexibility. So maybe Caspio is a stepping-stone, an actual bridge rather than a crutch that people will be using rather than expanding their skills. But right now it sure doesn’t feel that way to me. News executives seeking to be able to tell the bosses that “we have some databases on the site” will find lots to love about a product like Caspio Bridge. The rest of us should take no comfort in that."
  },
  {
    "objectID": "archives/2007/09/30/teaching-data-on-the-web/index.html",
    "href": "archives/2007/09/30/teaching-data-on-the-web/index.html",
    "title": "Teaching Data on the Web",
    "section": "",
    "text": "Matt’s advice (the latest in the series kicked off by Paul Bradshaw) is excellent: “Learn how to put data on the web.” But check out the next paragraph, where he suggests some venues for accomplishing the actual learning. Notice anything missing? You know, like the journalism school?\nLet me begin by saying that I’ve been an adjunct instructor at several schools during the past seven years and enjoy it very much. So I do have a small financial interest in this question of how data skills get taught to would-be journalists. But if large private schools like George Washington University, where I currently teach a class, don’t have someone on the faculty who can teach this stuff, what are the chances that most journalism students who are even interesting in taking Matt’s advice will have the opportunity to do so within a journalism structure? The comic strip Bloom County once had a strip devoted to school teacher pay that ended with the fraction diddly/squat. That’s about right in this situation, too.\nOf course, Matt’s advice can still be acted upon, and probably would have some beneficial side effects for the student (meeting students from other academic disciplines being chief among them). But until and unless journalism schools show that data has any sort of importance to them, most journalism students only will be exposed to the possibilities, but not the actual process, of working with data on the Web.\nThis is an eminently fixable problem, especially when you consider that less than 10 years ago, the number of new media faculty was pitifully small. Now we’ve got experienced, talented folks who teach not just the basics of online but the best ways to use digital media in a news context. Which makes it all the more odd that CAR, which has been around for several decades now, has had so few advocates in the academy.\nLet’s face it; despite my parental background (both English majors and teachers), I’m not going to challenge anyone in a writing contest. And I’m design-impaired to the point where I still think it’s cool to draw two-dimensional boxes during boring meetings. So while my journalism classes were usually interesting from an intellectual and academic point of view, they did little to actually prepare me for doing the kind of journalism I like to do. I can’t be the only person to experience this. Put another way: it can’t be good for journalism education if the best data-oriented journalists owe little or nothing to their journalism classes.\nWhen I went looking for an intern to do mostly technical tasks for us at washingtonpost.com, I didn’t even bother with the local journalism departments. I found a computer science major instead. And that works - until the point where compsci majors realize what news organizations pay, or when they discover that either they aren’t that interested in journalism, or journalism isn’t much interested in them."
  },
  {
    "objectID": "archives/2007/09/12/on-trials-software-and-otherwise/index.html",
    "href": "archives/2007/09/12/on-trials-software-and-otherwise/index.html",
    "title": "On Trials, Software and Otherwise",
    "section": "",
    "text": "So in response to several commenters on my previous post, I went to caspio.com to see about a free 14-day trial in order to test things out. Then I read the Terms of Service, which contains this sentence: “In addition, you may not access the Service for purposes of monitoring its availability, performance or functionality, or for any other benchmarking or competitive purposes.”\nSo I’m not sure whether I can get that trial, as I have no intention of becoming a customer. I mean, I’m pretty sure I could, given the kind invitation by David Milliron, but the ToS seem to indicate otherwise. Anyway, there are two larger points that deserve more treatment than I gave them in the first post.\nThe first is that part of the appeal of a service like Caspio is, as several folks have pointed out, the ability to bypass a reluctant or otherwise clueless IT department. This is not insignificant, and in this sense Caspio is providing a way out for some organizations that might otherwise have little or no option when it comes to publishing data online. That is, compared to the alternative, not a bad thing. The potentially troubling aspect is that, having avoided a fight with IT over server access and other issues, newsroom managers may be content to leave the situation at that. Some of the folks who had kind words to say about Caspio also said that it was not a complete solution; hopefully their managers see it that way. I don’t blame people for using Caspio or any other product if they have limited options. It’s the limited options situation that’s a big problem for many news organizations.\nThe second point, however, is all about a Caspio limitation, one that I mentioned in the comments. The fact is that, for a Web database app, Caspio has a significant problem in that many of its DataPages cannot be found using a search engine. So while an Indianapolis Star database of school suspensions has a page for the Carroll Jr-Sr High School, a Google search for “‘Carroll Jr-Sr High School’ suspension” doesn’t find any pages on IndyStar.com in its results. Now maybe you’ve got a the kind of audience loyalty that means you don’t have to worry about search engines; if so, count yourself extremely lucky. (The Indianapolis example, btw, is not the only one where that happens, so this isn’t to bash the folks there.)\nSome of the comments seemed to suggest that other solutions were only within the reach of larger news organizations. If that were true, then the folks at the Lawrence Journal-World, where Django originated, would be surprised to hear it. Other newspapers have built their own tools, too.\nEvery framework is going to have its problems or limitations; this isn’t to suggest that there is a holy grail. What it is meant to suggest is that when you promise ease of use and a point-and-click Web database publishing experience, there usually are some tradeoffs. Whether those tradeoffs are worth the experience is up to the users to decide, but it’s to the better that people know there are real options out there, and many of them aren’t beyond our capabilities."
  },
  {
    "objectID": "archives/2007/06/12/finding-enterprise-reporting/index.html",
    "href": "archives/2007/06/12/finding-enterprise-reporting/index.html",
    "title": "Finding Enterprise Reporting",
    "section": "",
    "text": "During our train ride this morning a colleague and I talked about a common problem that has some real implications: the difficulty in finding enterprise and public service journalism done by news organizations, particularly newspapers. For example, when you visit Newsvine or other aggregators you get a wealth of stories from sources that you may not have known existed, but how do you find the really notable original work, especially by smaller papers? There is a way, but it’ll take some industry cooperation and collaboration with search engines.\nI started tracking investigative stories on this site back in 2002, and eventually IRE took over the compilation when I narrowed my focus to CAR stories. Both of these efforts, however, involve a good bit of manual labor – the time spent scouring newspaper websites and other sources for stories and then inputting them into a database or blog software. It’s the same problem faced by the folks who compiled the All-Star Newspaper back during the boom, although that effort skewed towards the larger papers.\nThe idea is that Web aggregators should have a way to display public service and enterprise reporting as its own category, particularly when it comes from smaller outlets that don’t get national attention. My response is that this is too messy of a problem for search engines to solve via algorithm, but it’s something that most newspapers can do by themselves.\nHow? The industry, with IRE or ASNE or some other organization taking the lead, develop a metadata standard for stories that papers want to showcase as examples of public service or enterprise reporting. By embedding the code in stories on their websites, news organizations can signal Google or other aggregators to the existence of a certain class of content which then could be displayed separately from other types. We already do search engine optimization, so modifying a CMS to produce this code shouldn’t be too difficult, and definitely isn’t a costly thing.\nThe question is, would papers voluntarily adopt the standard, and if so, would some of them intentionally or not bring down the value of the standard by including stories that shouldn’t be in there? There’s no way to know unless we find out. But some organization should sit down with Yahoo!, Google or anybody else and start talking about this standard, and then get to developing it. We often talk about the value of original journalism. Here’s a chance to actually show the world what we mean, and why newspapers are an important part of civic life."
  },
  {
    "objectID": "archives/2007/06/14/the-new-competition/index.html",
    "href": "archives/2007/06/14/the-new-competition/index.html",
    "title": "The New Competition",
    "section": "",
    "text": "Back when we launched the Congress Votes Database in late 2005, it had only a few contemporaries, including the excellent GovTrack. Now the field is getting pretty crowded, and that’s a good thing for readers interested in federal legislation and how Congress works. The Sunlight Foundation in particular is responsible for jump-starting activity in this arena by funding individuals and organizations and also building its own tools. The latest, LOUIS, is an aggregator of federal legislative documents, which is a fairly robust challenge to those businesses that provide legislative tracking. If this kind of thing doesn’t worry journalists, it should.\nWeb apps like these should cause some consternation among editors and reporters, since ultimately we’re competing for readers’ attention in niche areas. I know Sunlight’s work has pushed us to add features to the votes database, and that benefits readers in the end. But news organizations shouldn’t forget that they have advantages of their own: expertise and knowledge among staffers and the ability to integrate quality content with data. We just have to recognize the competitive threat and respond do it.\nFor awhile now I’ve been telling folks that apps like Facebook – as non-newsy as it can be compared to a paper – are now part of our competitive space, and not just for hits and eyeballs. They are not the only competition, and we shouldn’t ignore other news outlets or journalistic efforts. But it’s way past the time for us to be dismissing niche Web apps as irrelevant to our jobs."
  },
  {
    "objectID": "archives/2009/03/25/no-really-show-us-the-data/index.html",
    "href": "archives/2009/03/25/no-really-show-us-the-data/index.html",
    "title": "No, Really, Show Us The Data",
    "section": "",
    "text": "When it first appeared I was really excited to see Show Us The Data, which gave visitors a chance to list and vote for their “Most Requested Documents” that should be more readily available from the federal government. Sure enough, there were plenty of strong choices for the top 10 list. And then people starting voting, and the results were not quite what I had hoped to see. Yes, the items that comprise the Top 10 List (irony alert! it’s a PDF) are worthy documents, but some of them (the Supreme Court website?) reflect a lack of familiarity with the government information that’s truly buried.\nWhat follows is my entirely subjective, data-heavy and document-light version. It’s Congress-heavy, because the executive branch has done much, much better in many ways. No, really, show us the data:\n\nCongressional committee votes. As far as I know, only commercial companies like CQ possess this information in the aggregate. Most committees publish them in committee reports - House Judiciary is one of the better ones - without a standard format and in such a way as to make their gathering prohibitively expensive. And yet these are some of the most telling public actions lawmakers make.\nEarmarks. If you don’t think the Appropriations Committees have a database of earmarks, you’re naive. Of course they do – it’s valuable information. Now, about sharing it in anything but an image PDF format… well, let’s just say that Keith Ashdown and the folks at Taxpayers for Common Sense probably aren’t going away soon.\nForeign Travel Reports (Codels). The House publishes PDFs and text files of this data, but they are formatted for reading, not analysis. It would not be hard to change this.\nLegal Defense Funds. It’s utterly ridiculous that while House members now file their campaign reports electronically, legal defense fund reports are still filed on paper. This is a no-brainer.\nSenate Votes in XML. Go ahead, view source on this page. See where the HTML comment says “****** vote_111_1_00110.xml …”? They already generate these files; but the public can’t have them. They’re only for the use of Senators. There’s absolutely no reason the Senate cannot join the House in doing this, so why won’t they? Update: they have!\nSenior Executive Service. This one is particularly egregious, in that the information on senior-level political appointees in the executive branch previously was made available in database-friendly formats, but now is only available via PDF. So OPM chose to make the information less useful.\nHigh-Level Diplomatic Visits. Another “I can’t believe it’s not a database” entry. The State Department offers a list of visits by foreign leaders and lists of visits by the president and secretary.\nThe CIA World Factbook. Oh, you can download the PDF, but (and I am not making this up): “the search software resides on our server and cannot be distributed with the World Factbook.” Thanks!\n\nThat’s eight, and I can already think of some more. What’s on your list? Actual federal data, please, as opposed to documents that are valuable for their full-text content. I’m sure I’m missing some that should be on here."
  },
  {
    "objectID": "archives/2009/11/29/the-future-of-ire-training/index.html",
    "href": "archives/2009/11/29/the-future-of-ire-training/index.html",
    "title": "The Future of IRE Training",
    "section": "",
    "text": "Anyone in journalism who knows me knows how much of a debt I owe to an organization called Investigative Reporters and Editors. Sure, I liked playing with data before I found out about IRE, but the knowledge and support that I’ve received from IRE training, conferences and members has been the single most positive influence on my career.\nThe trouble is that IRE is a non-profit organization tied to an industry that increasingly has cut back on spending for training, travel and other “luxuries”. So while attendance at this year’s IRE conference in Baltimore was very strong, a lot of folks there were paying their own way. Same deal for the annual computer-assisted reporting conference in Indianapolis. That’s simply not sustainable, given IRE’s current orientation towards providing hands-on training and data services to news organizations. My IRE membership will be pried from my cold, dead hands (and you should join, too), but attracting new members and offering them the kinds of training and services they’ll need will be increasingly difficult.\nIf you ask me, IRE needs to reorient its training and service offerings to take advantage of the distributed nature of the Web and the broadening of acts of journalism. Yes, hands-on training must continue, but do trainers need to travel all the time? What about video-based training? Yes, IRE should collect the best expertise of its members, but the age of the tipsheet alone is gone. We have so many other options: screencasts, podcasts, YouTube – hell, even Google Wave – to deliver the kind of knowledge that is the lifeblood of IRE.\nCrucial to that effort is the recognition that as the potential base of members and users of IRE services expands, so too the need for individual training modules. Yes, IRE should still offer a 5-day bootcamp on computer-assisted reporting. But it also should offer half-day refreshers on SQL, or 10-minute screencasts on a useful Excel function. Look at what PeepCode does – and I don’t think IRE would need to have such high production values to be valuable – and you’ve got an idea of what I’m talking about. IRE members are some of the leading experts in journalism on subjects such as the Census, mapping and various obscure datasets. Yet the only option for purchasing audio from conferences is all or nothing.\nSimilarly, the Resource Center needs a good update. IRE books, which are tremendously useful but can have a short shelf-life, need to be sold in print and revised online to remain attractive and lower the costs of doing new editions. Future tipsheets should be digital-only, and categorized not just by keyword but also by speaker. If I want to overdose on Paul Overberg’s Census material, I should be able to do that without searching (they should also be sold to journalism schools for classes). Nearly all training exercises should be available online – this would require greater standardization, but that’s not a terrible thing – and members or paid users should be able to schedule video/chat time with an IRE trainer or volunteer as a follow-up.\nWhile we’re at it: Uplink is, well, I don’t think even IRE knows for sure, but it definitely isn’t working. Simplify it. Give it a narrow mission. Make it easier to find the expertise that currently is spread across email, blog posts, tipsheets and tutorials. The value of the organization lies in the ability of its staff and volunteers to intelligently organize and disseminate the unique and valuable information it has within it.\nAll of these things require changes to the way IRE currently works, and that’s the tough part; it’s hard to argue that IRE hasn’t been doing good work, and I am definitely not making that claim. What I’m saying is that as both the market for IRE’s services and the methods for delivering them undergo some significant changes, it’s time for IRE to meet those changes head-on."
  },
  {
    "objectID": "archives/2009/11/21/a-question-of-emphasis/index.html",
    "href": "archives/2009/11/21/a-question-of-emphasis/index.html",
    "title": "A Question of Emphasis",
    "section": "",
    "text": "The job cuts at the Washington Post on Friday have produced a round of comments, broadly summed up by Steve Yelvington earlier today. They certainly begged the question that occurred to me as a former employee of both the Post and WPNI, its soon-to-be merged online operation: “What explains this kind of decision?”\nFirst, let me say that my observations about the general history of WPNI and its relationship with the paper are colored by my own experiences, but I agree with folks like Jay Rosen who say that at one point, washingtonpost.com was clearly a national leader – not just in technical capability, but in the kind of mindset necessary for a news organization prepared to take advantage of the Internet’s possibilities. I supported the creation of WPNI as a separate operation, to allow it more creative freedom, but both the people of WPNI and their colleagues at the Post should have done more to foster a better environment for working together. It’s something that I failed at when I was there.\nBut back to the kind of environment that leads to the departures, voluntarily or otherwise, of so many talented and dedicated employees. I don’t know the people who currently run the Washington Post, but I do think I understand a bit about how the organization works and thinks, having spent about three years there (more than two at the paper and about nine months at WPNI). When I wrote about moving from the paper to the website back in 2007, I left out some details about how that process happened. And I think, in hindsight, that they shed some light on how the organization operates.\nIt’s true that there were a number of people at the Post who were supportive and encouraging of my ambitions to work at WPNI. Among them were my supervisor at the time, Lucy Shackelford, and the paper’s editor, Len Downie. But once I had seriously pursued the idea of working on the website, it took months for the move to happen, and not just for reasons of simple corporate bureaucracy. In a very real way, my transition was held up – I (jokingly at first, and then angrily) referred to it as a filibuster or a senatorial hold – by a few people at the paper. These people, most of whom no longer occupy the positions they held then, are not stupid. They are among the smartest folks I’ve ever worked with, and I have a high regard for their journalistic abilities. But the thinking that caused the editor of the paper to become involved in whether a mid-level staffer moved to the website was, in essence, this: this is a bad idea, because it will hurt the paper. My ego might like to think that this was really true, but I think the reality is that these people could not compare the value of my work for the website to the paper because they did not understand what it is I wanted to do. So they went with what they knew, and that seemed to be a net deficit for them. And thus it was that I mooted the option of simply resigning from the paper in order to join its website.\nI don’t envy the people who run the Washington Post (or any news organization) today. They have a ton of thankless choices to make, and critics on every side. From a certain standpoint, I can appreciate the idea that the paper edition, which generates the overwhelming share of the revenue, should be protected and bolstered as much as possible. But I cannot agree with the idea that this means that you take employees who have proven expertise doing valuable and informative things that don’t always translate into print and cannibalize (or toss away) their talents for the sake of the paper.\nMy fear as a Washington Post subscriber and reader of washingtonpost.com is that, when the folks running the organization turn things around (and I believe that it is not an impossibility or even a long-shot), what emerges will be not only a news organization that is a shadow of its former self – most orgs will have to face that reality – but that it will have put so much emphasis on the paper that it cannot take advantage of the possibilities online. That the folks running things are literally rolling back the progress and smart work that has been done, and will not be able to get it back as fast as they might think. And the people who remain – those who will be charged with the task of rebuilding a news operation that embraces all of the ways that its readers and users can gain value – will have neither the support nor the depth to make it happen."
  },
  {
    "objectID": "archives/2009/11/09/buying-into-computational-journalism/index.html",
    "href": "archives/2009/11/09/buying-into-computational-journalism/index.html",
    "title": "Buying Into Computational Journalism",
    "section": "",
    "text": "The intriguing title of a recent report from scholars at Duke is “Accountability Through Algorithm: Developing the Field of Computational Journalism”. Semi-related to CAR, Computational Journalism is defined as “the combination of algorithms, data, and knowledge from the social sciences to supplement the accountability function of journalism.” I take each of those – algorithms, data and knowledge from the social sciences – as separate elements, because while journalists do have plenty to learn from the social sciences, we also operate in an environment that is not quite academic (and sometimes not at all).\nThe report identifies four areas of potential exploration: techniques for data transformation and pattern discovery in investigative reporting; a digital “dashboard” for journalists; new social and technical structures for interactions among readers and reporters; and sense-making advances from other disciplines. All are interesting and worthy, but to me the first two are particularly so.\nOn the first, the best investigative journalists have been developing tools for extracting meaning from reams of information for years. The change now is that we have a greater platform for these tools in the Internet, and an effort like DocumentCloud is a clear example of that change. The challenge we face is that patterns are interesting to different people for different reasons; what an accountant finds interesting may not always be of interest to a journalist, and vice versa. The current deficit is not in the area of tools; it is the occasionally trickier area of adapting those for the task of journalism. That requires the guiding influence of people like Sarah Cohen, a newly minted Knight Chair at Duke, who is studying these issues right now. But it also requires the active participation of a wide range of news organizations and journalists. In the Internet, we have a leveling platform, but only if more journalists participate. That may be a greater challenge than the technical one.\nOne way to get there is the second idea – a journalist’s dashboard. This would provide reporters with a way to keep track of the deluge of information coming into newsrooms. But again, the technological side of that equation, as difficult as it is, is less of a concern to me than the implementation and adoption of the results. We know how to gather various bits of information in one place. We’re not that good at distilling the best of them, or even knowing where to start. The good news is that we have blueprints for this kind of thing: the people and companies who make great Web apps that distill masses of data into understandable results. The bad news is that we, as a business, work very differently. We don’t really share much, outside of experiences at conferences or over drinks, and particularly not at the institutional level. And we’re downright awful, in general, at adapting good ideas for our own uses.\nFor the idea of Computational Journalism to work, a lot is riding on a movement that is slowly growing but urgently necessary for the news industry: the increasing adoption, use and proliferation of open-source tools. The CAR community has seen an influx of use of various types of open-source software, from databases to GIS systems to web frameworks. More and more reporters and editors are embracing different styles of journalism. But the broader concept of opening up our newsrooms, both philosophically and in terms of our content and efforts, has been slow in coming. It requires not just the creation of tools, but also the development of journalists and readers who will use those tools most effectively. And that’s more than an algorithm – to say nothing of Twitter – can solve alone.\nOh, and Duke folks? Can we get a version of that report that embraces the Web as much as the concept? HTML will do fine."
  },
  {
    "objectID": "archives/2009/10/28/the-fecs-disclosure-data-catalog/index.html",
    "href": "archives/2009/10/28/the-fecs-disclosure-data-catalog/index.html",
    "title": "The FEC’s Disclosure Data Catalog",
    "section": "",
    "text": "The good folks at the Federal Election Commission launched a disclosure data catalog recently, continuing the federal government data catalog trend. And while there are few (if any) people better at explaining campaign finance data than the FEC’s Bob Biersack, the data catalog is a work in progress and has room for improvement.\nIt should be noted that the FEC has been giving away campaign finance data in bulk form since 1979, and has remained admirably consistent in its approach. Text files stored on FTP servers still work quite well, thank you, and while that system could use some improvements, too, plenty of campaign finance reporters rely on it.\nBut in launching a new product with new data, the FEC has a chance to really do things right. Take, for example, the definitive listing of leadership PACs, those committees used by politicians to help boost their standings within the party. The new data catalog has a listing of leadership PACs, which sure beats tracking these by hand like I used to do. But it underscores one of the fundamental issues involving FEC data: the lack of an ultimate candidate ID.\nTake a look at that list – notice how the “sponsor name” and “affiliated committee name” are interchangeable (and, what’s more, leadership PACs aren’t supposed to be affiliated with an official campaign committee)? The FEC has no single equivalent of the unique committee ID that can exist across multiple election cycles. There is a unique candidate ID, but if a candidate runs for more than one office he or she gets additional candidate IDs, with no easy way to tie them to each other or back to the person.\nUntil that system exists, it’s difficult to track the full extent of a politician’s campaign finance activity – yes, most of them only run for a single office, but that happenstance is not a pillar of good information management. The consequence is that organizations that want to tie together a politician’s activity need to invent their own ids for doing so, which means duplication and an inability to share across data sources. Now that campaign finance APIs exist, this is a bad idea that needs to be fixed.\nCongress itself has a permanent unique ID for members regardless of whether they serve in the House, Senate or both. The FEC should do the same, beginning with the current election cycle and working backwards for current candidates. Might be something worth taking to the FEC’s new data blog."
  },
  {
    "objectID": "archives/2009/07/24/one-way-to-encourage-innovation/index.html",
    "href": "archives/2009/07/24/one-way-to-encourage-innovation/index.html",
    "title": "One Way to Encourage Innovation",
    "section": "",
    "text": "Innovation. We’re told over and over (often by people who don’t actually do much more than talk, but that’s another story) that our industry needs it. So, you ask, how I can get me some of that innovation stuff? In my experience, there’s only so much that a single person (or a small group of people) can do inside a larger organization to develop new ideas and see them thrive. You need help, often from the very structures that new ideas might seem to be challenging.\nSo here’s an idea: reward innovation with concrete responses. Yesterday, J-Lab at American University announced the winners of its annual Knight-Batten Award for innovations in journalism, and my employer won the grand prize for a body of work that included Represent, an app that my colleague Andrei Scheinkman and I built along with Stephan Weitburg. The honor and attention from that award is really great, and a cash reward doesn’t hurt, either. But we didn’t build Represent with Knight-Batten in mind.\nWe built Represent because The Times gave us the incentive and motivation, via a company-wide technology challenge designed to solicit working prototypes or applications for nytimes.com. Winners get a cash bonus (always a good incentive to enter) but also the resources to see their ideas come to life on the site (or internally, since internal apps also qualify). The former is a very nice thing indeed, but the latter is more important in the long-term, since people like to see their work showcased. Our contest is open to all employees of the company, and can be built in pretty much anything, which means that technology itself is an enabler of progress, not a barrier.\nThe key here is that for a small investment, the Times got some of its employees to work on projects that they were personally interested in, on their own time. The winners and the company benefit from new ideas, and the prospect of winning helps bring more people into the process. Does your news organization do this? Why not?"
  },
  {
    "objectID": "archives/2009/06/02/the-case-against-teaching-access/index.html",
    "href": "archives/2009/06/02/the-case-against-teaching-access/index.html",
    "title": "The Case Against Teaching Access",
    "section": "",
    "text": "I’ve been at the Medill School of Journalism at Northwestern University since last week, talking to faculty members about using data management and analysis tools (spreadsheets, databases, mapping) in their courses. When they asked me to provide some training on Excel and Access, I agreed, but asked for the chance to make a case for teaching any database but Access to students. Specifically, I suggested that universities and training organizations like IRE teach SQLite, which has the advantages of being cross-platform and accessible via a Firefox add-on. My class this semester at George Washington University and my time here at Medill have only reinforced my conviction on this."
  },
  {
    "objectID": "archives/2009/06/02/the-case-against-teaching-access/index.html#the-case-against-access",
    "href": "archives/2009/06/02/the-case-against-teaching-access/index.html#the-case-against-access",
    "title": "The Case Against Teaching Access",
    "section": "The Case Against Access",
    "text": "The Case Against Access\nAccess costs money. In SQLite, MySQL and PostgreSQL, there are superior database programs that are free and open-source. If you’re asking your students, many of whom may be buying Mac laptops, to get Access, you’re putting an additional burden on them. And if that’s all they know once they graduate and manage to land a job, if that place doesn’t have Access, they may need to get it (or have nothing at all).\nThe Access query grid hides the fact that underneath, Access runs SQL queries. So a user is able to construct and execute a SQL query without writing any SQL whatsoever. This is, imho, a bad thing, as it makes it possible to get results without actually knowing what you are doing. When we teach the query grid, we’re teaching behavior over understanding, or at the very least we’re allowing behavior to compete with understanding. And that doesn’t even begin to address the issue that the query grid doesn’t do everything that SQL can. In terms of teaching, this is critical; we’re not properly equipping students for the opportunities and challenges they could face.\nAnother issue is data portability: Access databases don’t support dumping to a .sql file, which is a great way to transfer SQL data without losing data types. Access does export to many formats, including Excel, CSV and XML, but the lack of SQL dump ability is a pain for transferring data. If you want to send somebody an Access database, you can either send them the entire file (providing they have Access installed), or you can export each of the tables and have them re-import them. And if you do email that .mdb (or now, .accdb) file, be warned that they do get quite big. To demonstrate this, I loaded the same three tables into Access 2007 and SQLite and the Access file was nearly 3 times the size of the SQLite database.\nFinally, there’s the Web. Know many popular Web sites that run off an Access database? Me neither. If all you know is Access and not the underlying SQL concepts, your transition to a popular server software like MySQL is going to be more difficult. Sure, you say, but it’s better than nothing. But as far as the Web goes, Access is almost nothing itself. So why would you teach a program that has very little future on the Web – the platform of today and tomorrow?\n\nThe Case for SQLite\nSQLite is my choice for the candidate to replace Access in journalism education. In addition to the advantages listed above, it’s also easy to “install.” If you can download files, unzip them and move them to a location on your hard drive, you can “install” SQLite. If you can install a Firefox add-on, you can manage it in the browser. And you can take your database files home with you or email them around. The add-on supports importing CSV files, SQL dumps and XML (although all databases can have issues with importing XML). It looks and works the same on a PC or a Mac. Most importantly, it demands an understanding of SQL that you can avoid when learning Access.\nWhen I first learned SQL at an IRE bootcamp, we were using FoxPro and we learned how to type the SQL commands. That knowledge only becomes more valuable as you learn the limits and possibilities of SQL. Journalism educators and trainers should commit to teaching SQL on the broadest platform possible and with an emphasis on the syntax and meaning of the language itself, not on which buttons to click. Otherwise we risk sending students out into this new journalism world even less-prepared to handle data intelligently, and I don’t think we can afford that."
  },
  {
    "objectID": "archives/2009/06/25/the-fundamental-training-need/index.html",
    "href": "archives/2009/06/25/the-fundamental-training-need/index.html",
    "title": "The Fundamental Training Need",
    "section": "",
    "text": "It’s good to see recent writings on the importance of training and skill development for journalists.\nOne of the common responses to such entreaties is exemplified in one of the comments to the 10000 Words piece, which includes this plea: “I understand the need to bolster one’s skill set. But what happened to the days when we actually, you know, worried about reporting rather than slavishly trying to master every piece of technology?”\nIf only that was the real problem.\nThe real problem is the way that we as journalists manage information, because that determines so much else: the kinds of stories we’re able to envision and construct, the amount of context we’re able to bring to bear in a short amount of time and our ability to connect the dots. In general, and this is my scientific conclusion, we suck at managing information.\nThat’s nothing new, you might say, and you’d be right. But what has changed is that a lot of the people and institutions we cover are now getting smarter about this stuff, and are using better tools to help them manage information. From tracking crime to measuring customer loyalty, the sophisticated use of information is a crucial factor in many modern activities. Us? We’re still knocking rocks around hoping to generate a spark.\nI’m not knocking learning skills like how to maintain a blog. I’m just saying that if all we do is teach new tools and skills, we’re making the underlying problem harder to solve, not easier, because we’re just encouraging the production of even more separate and disconnected piles of information. More photos over here, more spreadsheets over there. We’re still drowning in information and we can’t figure out how to use it to our best advantage, like finding undiscovered patterns and coming up with definitive explanations instead of the ol’ three-person anecdote story.\nSo yeah, teach those CAR and multimedia skills. Have everybody Twitter. But please, let’s find a way to address the fact that for many journalists, Microsoft Word is the primary tool for organizing any and all kinds of information. Let’s make sure that our silos of content (text archives, photo archives, databases, etc.) can at least be made to talk to each other, if not naturally, then through APIs or metadata or something. And let’s start talking about how a news organization’s information belongs to the organization, not just to individual reporters and editors, and how our products could be so much better if we adhered to that principle before a story/photo/slideshow is published, not just after."
  },
  {
    "objectID": "archives/2009/12/25/the-gift-of-data/index.html",
    "href": "archives/2009/12/25/the-gift-of-data/index.html",
    "title": "The Gift of Data",
    "section": "",
    "text": "One of the more challenging and interesting projects at work lately has been the work we’ve done on the Toxic Waters series by Charles Duhigg. Since the stories have explored water quality throughout the United States, the web component accompanying some of the stories have been national in scope as well. You can’t provide locally relevant information for a mass audience in a story, even one of Timesian length.\nThat constraint - which still exists on the web, albeit it less so than for print publications - makes it easier to justify working with hundreds of thousands or millions of rows of data to build an interface that allows readers to find out about polluters or drinking water systems close to them.\nThese kinds of apps aren’t easy; the hardest part is the interface, for which Tyson Evans and Matt Bloch deserve the credit for the water series. But the difference between those apps and the “data ghetto” kind of app that provides a search box and not much else is more than time and talent. In some cases, it’s less a “nice-to-have” than a must-have. Earlier this month, Duhigg and Griff Palmer wrote a powerful piece showing that millions of Americans are drinking contaminated water. A great story, but check out the first dozen or so comments by readers. Half of them asked for a list of drinking water systems or a map or some other way to find out if their local systems were providing unhealthy water. That app accompanied the story published later in December.\nReaders aren’t dumb; they want to know what we know, and they know that the web makes it possible for us to share with them at a national and even local level. The level of commitment and effort that we put into responding to their need for relevant and meaningful information will go a long way towards building a better relationship with them. The kind of web application that provides a summary of millions of records requires a different approach from, say, a lookup of government employees. But that doesn’t necessarily mean less effort and thought, just a willingness to treat both in a manner that respects the unique characteristics of each. So as we are in the season of gift-giving, all of us - myself included - need to think more about what we’re really giving to our readers when we post data on the web."
  },
  {
    "objectID": "archives/2017/12/28/academy-fight-song-part-2/index.html",
    "href": "archives/2017/12/28/academy-fight-song-part-2/index.html",
    "title": "Academy Fight Song, Part 2",
    "section": "",
    "text": "It begins, as so many things do these days, with a tweet.\nThe tweet references an academic job listing from the University of Maryland, where I have taught journalism as an adjunct, and the eye-rolling reactions of professional journalists to it. Like many job listings in the past few years, it has just the right level of “Computer Jesus” in its minimum (!) qualifications:\n\nThe successful candidate will have a passion for journalism and the watchdog role it should play in a democratic society. Research and teaching topics of interest to both schools include data analytics as applied to journalism and/or social media; fairness, accountability and transparency in algorithms; news automation; audience analytics and engagement, including those illuminating trends in politics, society or sports; data visualization; social networks; and/or digital innovation. The successful candidate will be expected to have a Ph.D. at the time of appointment.\n\nNow, let’s get something out of the way: there are indeed academics who are qualified for this job, and presumably one of them will be hired to do it. My belief that the population of qualified candidates with substantial journalism experience is relatively small may be mistaken; I’d love to see evidence otherwise.\nBack to the tweet by Jason Martin, who chairs DePaul’s journalism program and previously worked as a reporter. Martin’s claim is about how many professionals view their academic counterparts with disdain, to the extent that we think about them at all. That’s a real thing, although I don’t think that’s what motivated my colleagues’ reactions. As a journalist who has taught at the university level for more than 15 years, I think the reason that some of my colleagues who do work in data journalism engaged in the Twitter eye-rolling is pretty clear: the bulk of the most relevant work being done in these areas is being done in newsrooms by professional journalists. Most of them will only ever teach as an adjunct and, despite the efforts of good administrators, are rarely accepted as anything but temporary necessities by traditional faculty.\nSo, when we hear this: “Professionals fail to recognize the ways our research informs & enhances industry.”\nWe also hear this: “We need working professionals to teach many of our advanced skills-based classes that we tell students keep us on the cutting edge, but they will never be considered full participants in higher education unless they have credentials we recognize.”\nYou would roll your eyes, too.\n/////\nThere is no good reason why the study of what used to be called “computer-assisted reporting” and its descendants has languished in relative obscurity at many universities. If you’re looking for academic work on this area, you could start with Phil Meyer, who practiced journalism using social science methods and then continued his work in academia.\nHere’s what Meyer wrote in 1996 about journalism education:\n\nEngineering, medicine, business administration, library science - almost any profession you can name that has research committed professional schools has always generated a continuing demand for new knowledge. Every new graduating class from a school of engineering or medicine is viewed nervously by the practitioners in its field because the kids have knowledge that the old guys don’t. Aging engineers wind up in administrative or sales jobs because the technology has changed faster than their ability to keep up.\n\n\nThe sources of that change have been people with research degrees, many of them in universities.\n\n\nWhether you are ready or not, that is happening to us, even on the craft side. New graduates who can create Web pages, digitize photographs and do electronic searching are in demand precisely because they have skills that midcareer professionals haven’t had the time or interest to acquire. There is also demand for newly trained database reporters who have analytical and statistical skills never learned by their seniors.\n\nThe whole thing is worth a read, and then some reflection. It’s not that there aren’t scholars who are examining the use of new technologies or the changes that have overrun journalism in the past 20 years. But Meyer also created and critiqued techniques, like The USA TODAY Diversity Index he and Shawn McIntosh, then at the news organization, invented in 1991. His work has been deeply relevant to how we think about journalism but also how we practice it. That’s a big reason why professional journalists doing data-related work hold Meyer in such high esteem.\nThe sad part is that Meyer could just as easily have published his essay in 2000, 2004, 2008, 2012 or even 2017. Journalism education has not remained exactly the same as it was in 1996 - there are more classes in different areas, including quantitative skills - but these changes have not been broad-based nor have they substantially changed the composition of faculties. Are journalism schools producing the kinds of research now that are guiding newsrooms? Having taught at seven different universities during the past 15 years, my impression is that journalism graduates are more educated and have been exposed to new techniques and technologies, although not in the depth needed. I also believe that the driver of this growth has been the work of industry, not academia. As an aging professional, I worry about graduating kids from other fields more than I do those graduating from most journalism schools.\nAdvanced degrees, which could be providing the fuel for pushing our field forward, are instead viewed either as necessary credentials just to enter the profession or as a pathway to teaching and research that too often does not address what is happening in the profession, particularly when it comes to the use of quantitative methods.\nThere are, of course, journalism programs that do a better job of teaching data courses: hiring full-time faculty and even making them eligible for tenure, or offering multiple classes instead of one. But overall the picture is not encouraging. In their 2016 report, “Teaching Data and Computational Journalism”, Charles Berret and Cheryl Phillips surveyed more than 100 journalism programs from the United States and found that “many journalism programs do not have a faculty member skilled in data journalism” and nearly half offer no classes at all. Most are taught by professionals working as adjuncts.\nMartin suggests that this need not be the case: “And let’s be blunt: good quant PhDs with journalism experience can teach data journalism.” Nobody denies that this is the case, but I’d like to see a list of good quant PhDs with journalism experience currently employed by universities. Is the academy producing enough of them to take over the instruction of data analysis from professionals? If not, is there any expectation of when that might be the case? Are the incentives even in place for this to happen?\nUniversities that want to serve student needs when it comes to data and digital skills and practices cannot say they are doing it well if they off-load that responsibility to adjuncts teaching upper-level electives or as part of graduate programs that can be prohibitively expensive. Every time I teach, I hear from graduating seniors that they wished they had had any exposure to these types of classes in the past four years - and these are students from R1 research institutions with well-known journalism programs.\nThe report offers multiple suggestions for changing this situation – all of them useful – but there is a more fundamental issue at play here, the one that Martin’s tweet raises: if there are loads of benefits to journalism that accrue from the work of journalism PhDs, why do many professional journalists think otherwise? What can be done about that? And are universities willing to take those steps?\nIt’s instructive to look at Martin’s response to a question about why a PhD requirement was necessary for that Maryland job:\n\nProduce research that contributes to development/reputation of dept/college & field. Research lines such as gig in question are expected to create publications thru rigor, peer review. Universities have standards for research productivity & evaluate faculty across disciplines.\n\nAll of these benefit the university, the college/department and the researcher, perhaps in that order. If the research itself has benefits for the industry or teaching, how are those valued? Professionals can rightly look at this situation and think: how does this job benefit the aspect of journalism education most visible to them - the education of future journalists and the study of their craft? These aren’t trolling questions. Professional journalists want to increase the skills of journalism students and the number of them who can do quantitative work. We wouldn’t essentially volunteer our time teaching if we didn’t.\nI think there are at least two areas that we need to address. The first is to have academics make their research more accessible (both in terms of relevance and availability) and the second is for professionals to have a way to participate in the conversation about that research in a manner that promotes respect and allows industry concerns to inform academic research.\nTo do both, we could look to political science, where quantitative methods are blossoming along with an ethos that encourages the sharing of data and research. In early January I’ll be attending the Southern Political Science Association conference, mostly because it has the academic work that is most relevant to my own professional interests of politics and elections (also, it’s in New Orleans). The work of the people I go to see is very much grounded in academic tradition; it also often speaks directly to current issues and events. I get both story ideas and new ways of thinking about how we are doing (or not doing) our job from this conference, and I can recognize the potential uses for journalism. And increasingly, it’s easier to get at their data. The potential impacts of quantitative political science research have not been without controversy, but from a professional perspective these are conversations we need to be having. And political science has done a better job lately of trying to make its research more digestible for general audiences, which communication scholars could use as a basis for experimentation.\nIn contrast, the theme of the 2017 Association for Education in Journalism and Mass Communication conference was “Closing the Gap: Media, Research and the Profession.” Here’s the nut: “Professionals no longer need some of the skills taught in our classes, and, too often, research conducted by educators is no longer relevant to a rapidly evolving industry.” Better sharing would help combat this, and it starts small. Journalism academics should insist on publishing their research instead of having faculty bio pages that contain literally no links to their work, Dr. Martin. More GitHub repositories, more open sharing of work, please. “We’d like to share our research about mass communication but are prevented by publishing agreements” is another classic eye-roller, even if it’s not something within the complete control of academics. Professionals are figuring this out; it’s time academics join us so that they can dispel our weaker (or non-existent) criticism.\nThe next necessary step to reduce the disconnect that Martin cites is to get more journalists and academics talking about each other’s work. Nikki Usher, a George Washington University professor, graciously (particularly considering my reaction to our initial encounter) offered to organize such a session last year when I was teaching at GW’s School of Media and Public Affairs. Her description of the session is accurate; I’d only add that I don’t look to academics to develop literal tools for use in the newsroom but to evaluate what we’re doing in a way that we often can’t do. Some of the ideas I offered up include things that probably do sound trivial or boring to researchers, but as newsrooms are literally redefining their structures and operations, even measuring the outputs would help advance our understanding of the craft. These conversations will be awkward or even uncomfortable at times; they are all the more necessary for it. Prof. Usher’s has thoughts on this process that are worth reading, although in some cases I think she answers glib questions with glib answers.\nI am not anti-academic. I saw the frustrations my dad dealt with in achieving his PhD and securing a university job that expected published research. My own undergraduate education was more theoretical (rhetoric!) than practical, and the time I have spent teaching has been some of the most rewarding of my career. But I am increasingly concerned that the imbalance between professionals and the academy, in which both of us retreat to our corners and snipe at the other, is not only corrosive to both but poses a larger threat to journalism education. I’m not sure that data journalism is best taught in an academic setting, that the existing incentives and structures offer few rewards for those who try and serve students and industry poorly. I think that segment of journalism education could actually be done better outside the university setting, which would be problematic for schools trying to attract students. Mostly, I’m not sure how much more I want to participate in a system that takes our energy and time but isn’t interested in supporting work that isn’t connected to a PhD, as Stephen Stirling puts it so well.\nIf we want to bridge this gap, maybe what’s necessary is for professionals who teach data-related classes to teach less and to focus more on discussing research and advising both journalism schools and student journalists. I serve on councils for the University of Florida and West Virginia University, and perhaps that is where I can make more of a difference, along with trying to serve as a mentor for individual students and young professionals. The hard reality is that my teaching experience has increased the disconnect between my own work and the academic institutions that prepare students for it, not helped to narrow it. We all lose when that gap persists, but students lose the most, and in the end we will all pay for it."
  },
  {
    "objectID": "archives/2010/05/11/using-the-nyt-congress-api-with-excel/index.html",
    "href": "archives/2010/05/11/using-the-nyt-congress-api-with-excel/index.html",
    "title": "Using the NYT Congress API with … Excel?",
    "section": "",
    "text": "It’s true that Excel has been a decreasing part of my toolkit for several years now, and that I never quite had the love for it that I do for various database managers. But I’m guessing that’s the exception, not the rule, in the broader journalism community. So when it came time to propose a lightning talk for the 2010 CAR Conference last week, I chose to pull out the ol’ spreadsheet and show how you could get started with the NYT’s Congress API with a familiar tool.\nTo do this, I had to not only drag out Excel but also do it on Windows, since Excel’s Web Query feature isn’t available on the Mac. (You could also do this, albeit in a slightly different manner, using OpenOffice and Google Spreadsheets.) Here’s how it works using Excel.\nFirst, you’ll need an API key. To get one, go to The Times Developer Network and register (note: you’ll need to be a registered user of nytimes.com first).\n\n\n\nAPI Key Signup\n\n\nYou’re registering an “application”, and then you can add specific API keys to that account. Let’s add one for the Congress API. The key itself is a longish string of letters and numbers that gets appended to every API request URL, including the ones we’ll make from Excel. Let’s copy the API key so we can easily grab it (note that this particular key has been disabled, so using it won’t work).\n\n\n\nAPI key\n\n\nLet’s find an API call that we can use be looking at the Congress API’s documentation. Let’s pick the “members leaving office” response, otherwise known as the casualty list. All that’s required is the chamber (‘house’ or ‘senate’) and the congress (currently only the 111th-forward is supported). If we choose the House, the URL will look like this, except that you’ll need to specify your Congress API Key.\nThe version number should be “v3″ and you don’t need to specify a format after leaving (xml is the default). You should quickly get an xml file that looks roughly like this:\n\n\n\nThe response\n\n\nTo get that xml into Excel, we’re going to use Excel’s Import Data feature. I’m not one of those cool kids who has Excel 2007 at their fingertips, so I’m going to use Excel 2002. Import Data can be found at Data -> Get External Data -> Import Data.\n\n\n\nImport part 1\n\n\nThen change the file type to xml and paste the full API url into the box just above the file type.\n\n\n\nImport part 2\n\n\nIt works for local files and Web urls. Then click on “Open” to start the process. The import process consists of Excel asking you where to put the file. Just click “OK” and you should soon see something like this:\n\n\n\nResults\n\n\nThe header row in row 2 isn’t perfect, but it should suffice. You probably don’t need the copyright statement in column A. But now you’ve got a way to pull data into Excel from an API! If you have questions or comments, please don’t hesitate to post them below. If you’re having issues with the API, the forum is the best place to head."
  },
  {
    "objectID": "archives/2010/02/18/lightning-talks-at-nicar/index.html",
    "href": "archives/2010/02/18/lightning-talks-at-nicar/index.html",
    "title": "Lightning Talks at NICAR",
    "section": "",
    "text": "This year’s computer-assisted reporting conference in Phoenix has a couple of new sessions on the schedule. One of them is an idea a couple of us have been pushing for a few years: lightning talks.\nA staple of technical conferences, lightning talks are based on the notion that while 45-50 minutes presentations are good, sometimes you only need about 5 minutes to express an idea or show off an example of something interesting or useful. And I’m pretty sure, based on sessions at the hotel bar at past conferences, that there are plenty of ideas out there. Attracting and organizing them is the goal of this app built by my boss, Aron Pilhofer, and launched today.\nThis is a new thing for the CAR community, but I think it could be a useful way to draw out ideas from people who aren’t doing panels or who might shrink from a 50-minute session. So let’s get started. Want to give a talk?"
  },
  {
    "objectID": "archives/2010/02/23/a-gentle-introduction-to-google-app-engine/index.html",
    "href": "archives/2010/02/23/a-gentle-introduction-to-google-app-engine/index.html",
    "title": "A Gentle Introduction to Google App Engine",
    "section": "",
    "text": "As part of our roll-out of version 3 of the NYT Congress API, I was tasked with coming up with a sample application that uses the API to do something mildly interesting, or at least functional. I had gotten a book on Google App Engine for my birthday and was pretty excited to see that some of the basic philosophies of Django were either incorporated directly into GAE or were easy to adapt to it. So when I started on the sample app, I picked GAE and dove in.\nApp Engine’s Python runtime, unsurprisingly, sticks pretty close to the language’s core tenets: it uses YAML files for configuration (hey, it’s whitespace!) and can run pretty much an entire app using just 2-3 files. A NYT colleague, Derek Gottfrid, built a sample app for our article search API comprising five files, including the README. Yes, it violates the separation of logic and design that most frameworks try to respect, but it works.\nGAE provides the basic building blocks a lot of Web apps need, nearly all optional: a backend in Datastore, a URL Fetch service that is wrapped by Python’s familiar urllib and urllib2 libraries, mail and messaging services and memcache. Webapp is a basic framework for building apps not exactly like Django but not so unfamiliar, either.\nThe development server will be familiar to anyone who has tinkered with Django, and GAE handles static files via separate servers, which is how it should be. And since it comes with a version of Django built-in, you can bring along some handy utilities, like simplejson, with a single import statement. And as I said earlier, you don’t have to separate display logic into template files, but you can, and the syntax is nearly identical to Django templates.\nThe sample app takes two random members of the Senate and compares their voting and bill sponsorship records in the 112th Congress. The app’s code is like the app itself: fairly tightly-focused and without a lot of trappings. It’s just service calls to the API and a single template for display. In building it, I didn’t make use of any persistent storage, so I didn’t delve into Datastore, but it looks pretty useful. One of its helpful features is that as you develop your app, it generates indexes used to help return data in the most efficient manner.\nIf you’re already familiar with Django, making the small step to App Engine isn’t that big of a trip. Have Guido explain things to you first, and then try it out. You can also run a stripped-down version of Django on GAE, and I’m looking to see if there’s a project I can adapt to try it out. In the meantime, if you want to tinker with the sample app, by all means fork it and see what else you can do with the API. And let me know what you come up with!"
  },
  {
    "objectID": "archives/2010/07/31/a-github-for-data/index.html",
    "href": "archives/2010/07/31/a-github-for-data/index.html",
    "title": "A GitHub for Data?",
    "section": "",
    "text": "Clay Johnson, late of Sunlight Labs and now writing at the splendidly-named InfoVegan, says that what the “Open Data” movement needs is a better way to store data on the Web. Something like a GitHub for data:\nWhy can I not type into a console gitdata install census-2010 or gitdata install census-2010 --format=mongodb and have everything I need to interface with the coming census data?\nTechnically, there’s not much reason why this couldn’t happen. Sure, some government datasets are very large, and some are in arcane and oddball formats, but these are technical problems that can be overcome. But the biggest issue, for data-driven apps contests and pretty much any other use of government data, is not that data isn’t easy to store on the Web. It’s that data is hard to understand, no matter where you get it.\nIn a sense, a GitHub for data could help solve this problem, too, because you can write documentation and many GitHub projects have excellent documentation. But there also are projects with very limited documentation – heck, some of them are mine. This is the biggest gap to better apps, that so few people really understand the data and its pitfalls. I’d like to see what Clay wants to see, too, but right now I’m more interested in:\ngitdata install census-2010\nIf the person executing that command is, say, Paul Overberg.\nThat’s not to say that I’m in favor of a situation where only those with expertise have access to data. What I’m saying is that the very act of what Clay describes as a hassle:\nA developer has to download some strange dataset off of a website like data.gov or the National Data Catalog, prune it, massage it, usually fix it, and then convert it to their database system of choice, and then they can start building their app.\nIs in fact what helps a user learn more about the dataset he or she is using. Even a well-documented dataset can have its quirks that show up only in the data itself, and the act of importing often reveals more about the data than the documentation does. We need to import, prune, massage, convert. It’s how we learn."
  },
  {
    "objectID": "archives/2010/07/12/how-apis-help-the-newsroom/index.html",
    "href": "archives/2010/07/12/how-apis-help-the-newsroom/index.html",
    "title": "How APIs Help the Newsroom",
    "section": "",
    "text": "As nice as it is to get praised for the civic-mindedness of your work, the not-so-secret secret about APIs at The Times is that we’re the biggest consumer of them. The flexibility and convenience that the APIs provide make it easier to cut down on repetitive manual work and bring new ideas to fruition. Other news organizations can do the same.\nThis week, for example, we launched a page to track Republican senators’ positions on the nomination of Elena Kagan to the Supreme Court. The fabulous graphics department has done things like this in the past, such as with the House vote on health care. Both of those graphics were assembled from lots of different pieces of information – electoral results and previous votes among them – and the Kagan data includes stuff like whether the senator in question is running for re-election this year.\nYou could, of course, ask people to gather up all that information, but if you’re going to do something like this more than once, it makes sense to have a way to automate as much as possible. That’s where the APIs come in. For the Kagan graphic, we used the NYT Congress API to pull in information on senators and their votes, which leaves the gathering of information about their statements on Kagan as the lone manual task. In other words, only the stuff that is specific to this app requires manual effort.\nSimilarly, the new Districts API we released plays well with our other APIs, so that I was able to build a simple demo app that takes advantage of the fact that our Congress API, among others, can return the current member for a particular district.\nFor newsrooms, the utility of APIs goes beyond creating Web apps. Making data available via APIs is a little like giving the newsroom the ability to ask and answer questions without having to tie down a CAR person for long periods of time. APIs can provide data in whatever format you choose, which means that a wider range of people can take advantage, from graphic artists used to working with XML to reporters comfortable with CSV files. When your data is more accessible and flexible, the possibilities for doing things with it expands.\nSo if you have a big local election coming up, having an API for candidate summary data makes it easier to do a quick-and-dirty internal site for reporters and editors to browse, but also gives graphics folks a way to pull in the latest data without having to ask for a spreadsheet. Chances are that if serious data analysis is what you need, that’ll be done in some desktop application or database server anyway. The API is just a messenger, albeit one that is always on and able to spawn lots of ideas and experiments.\nIf you’re looking to build an API, remember that it’s just a Web application delivering data in a structured format (XML and JSON being two popular formats these days). There are lots of options in terms of what you use to build and serve an API, so it’s important to pay attention to the design: which information you’ll deliver, and how. Being a significant user of your own API is really important, too; it’ll give you the best sense of how well you’ve designed your responses, and what you might be missing."
  },
  {
    "objectID": "archives/2010/09/11/how-far-weve-come/index.html",
    "href": "archives/2010/09/11/how-far-weve-come/index.html",
    "title": "How Far We’ve Come",
    "section": "",
    "text": "There’s been a bit of discussion lately in the open government community about how to assess federal government efforts at meaningful transparency. Of the stuff I’ve heard and read, I tend to come down on the side of Gunnar Hellekson, who writes that the Sunlight Foundation (a frequent and leading voice on transparency) has “dangerously conflated transparency for reform” in criticizing the pace of access to quality government data.\nSunlight’s Tom Lee disagreed, defending the criticism of USASpending.gov on the grounds that the site has been around for three years: “If it’s too soon to call for fixing them, how much longer should we wait?” As Gunnar notes, Sunlight has earned a bit of a right to issue such statements, as they’ve done quite a lot in the cause of increased access to government (as opposed to other outfits which produce non-items like how much weathermen give to political campaigns – seriously). But in my opinion, even Sunlight hasn’t earned the right to say that the government is “more interested in style than substance” when they consider LeBron James’ political contributions worthy of mention.\nWhile most of the efforts to increase transparency in government are fairly public and may seem pretty obvious to most people, sometimes it’s hard to understand the situation within government agencies. This isn’t an excuse for employees who would seek to hinder public access to government records — I’ve encountered relatively few of those — but an honest accounting of the situation. I was reminded of the gap between an impatient transparency community (including journalists) and government agencies while looking back at some older work emails. Here’s one, from early 2006, that I received while I was at The Washington Post:\nI am the Systems and Web Adminstrator for Senator Blanche Lincoln. I’m interested in linking from Sen. Lincoln’s website to the Congress Votes Database you designed. What do I need to do legally to add this link?\nNow, clearly this isn’t a question of the technical ability to add a link to a senator’s Web site. It’s about the process, the culture, an entirely new way of doing things. So is the appropriate response to say, “Holy crap, this is unacceptable.”? I’d say no. If I got an email like that today, that would be cause for alarm. But that mindset seems so far away now, as it should. Small victory, perhaps, but that’s how long-term campaigns are built.\nClearSpending.org is a really useful step from Sunlight, one of the most useful things they’ve done, imho, because they’ve taken an issue where access isn’t the main issue, but quality, and they’ve done their research. But there are times when, as well-intentioned as their efforts are, they don’t make things clearer. I truly admire the technical work behind Poligraft, but it does not help me understand the issue of influence any more than I did. In some cases, it simply overwhelms me rather than point to the most important stuff.\nI’m grateful that organizations like Sunlight are pushing for greater access to accurate public data. It’s not too much to ask. But just as government processes can seem alien and counterproductive at times, so can those of transparency advocates. My hope is that both sides continue to work in good faith on what is a long road. You don’t always have to use carrots, but if you’re willing to do the meaningful work, when you do resort to the sticks you won’t end up having to defend yourselves as much."
  },
  {
    "objectID": "archives/2010/09/14/hard-problems/index.html",
    "href": "archives/2010/09/14/hard-problems/index.html",
    "title": "Hard Problems",
    "section": "",
    "text": "I wasn’t going to respond to Ellen Miller’s comments on my previous post, mostly because I thought I had said what I wanted to. But now that O’Reilly has picked up on things, I figure it might be worth one last attempt on my part. Your experience, of course, may argue against that.\nEllen writes:\nEventually, Derek, someone has to stand up and say “but he isn’t wearing any clothes,” just like that kid at the parade. And that’s what we’ve done about USASpending …\nThe Administration’s delivery on their transparency promises is pretty poor. Perhaps only worse is the quality of some of the data we are seeing. It’s not wrong to point that out and demand that it to be fixed. It’s the responsible thing to do.\nWell, yes, it’s certainly fair to point out where the government has fallen short of its promises. No argument there. I think a close examination of what Sunlight has said and done shows that things aren’t that simple. Let’s consider Data.gov, for example. By pushing for access to datasets, Sunlight got what it requested: a bunch of datasets. Assuming that they’d all contain pristine information would have been a foolish notion, and I’m sure there are scores of journalists who could have said as much. Quality has always been an issue.\nBut let’s take a look at what Sunlight said after Data.gov was announced but before it was released. Sunlight Labs asked for five things:\n\nBulk access to data\nAccountability for Data Quality\nClear, understandable language\nService and developer friendly file formats\nComprehensiveness\n\nAssuming those were listed in rough order of importance, Data.gov has done quite a bit towards satisfying some of those conditions, but certainly nothing approaching perfection. There is a lot of data you can get in bulk. And you can rate the quality or usefulness of each dataset (although the public doesn’t seem to be much interested in doing that). But to quality specifically, the Sunlight post says: “In order to make this the most efficient process possible, Government should rely on the customers of its data to pinpoint where the problems are.” (emphasis mine)\nThis doesn’t excuse the publication of dirty or incomplete data, but asking for the release of what is known to be imperfect data and then saying that things aren’t working because the data isn’t good enough seems to be a bit … off. Or somewhat obvious. Or maybe reminiscent of Captain Renault.\nTom Lee’s post introducing ClearSpending – which is a responsible and considered response to the issue of data quality – acknowledges the complexity of things: “These are hard problems, and the people responsible for USASpending don’t really have the power necessary to get other agencies into line.” So knocking them for redesigning the site – something Sunlight considers important enough to do repeatedly as a public service – seems to me to fall a bit flat.\nWhat bugs me about this situation is that there are times when the “open government” community suffers from similar issues – not exactly data quality, but of a broader concern that Tom writes about:\nFrankly, I’m worried about what happens when people start asking what concrete things the open government movement has accomplished. We need to make sure that the answer isn’t “accidentally misleading a lot of people.”\nShow me how this Poligraft analysis of a typical NYT story about a Sunday talk show appearance deepens my understanding of the interconnections between the the people mentioned in it. It doesn’t. That’s not because Poligraft is a bad idea; it’s because the goal it sets out to achieve is hard, really hard. Does that mean that Sunlight is failing to deliver on its promises? Or the Capitol Words project that’s currently on hiatus, is that a failure, too?\nOf course not. But these are hard problems, and they require a lot more than some of the rhetoric I’ve seen of late. I’d love to see more stuff like ClearSpending, without some of the rhetoric that has accompanied it."
  },
  {
    "objectID": "archives/2010/12/07/why-students-should-come-to-the-car-conference/index.html",
    "href": "archives/2010/12/07/why-students-should-come-to-the-car-conference/index.html",
    "title": "Why Students Should Come to the CAR Conference",
    "section": "",
    "text": "Update: the student price for the conference, $100, does not include IRE membership. That’s $25. Both are bargains.\nHey there, journalism student!\nA bunch of your colleagues are having a get-together in February, and you should come. Actually, you need to be there.\nI’m talking about Investigative Reporters and Editors’ annual Computer-Assisted Reporting Conference, held in Raleigh, North Carolina. Several days of panels, hands-on training and talks with journalists from all kinds of news organizations. There’s even a great side event on data visualization. This conference changed my career, and I’m betting it can do the same for you.\nFor many of you, this may be a step into the unknown. Your universities probably don’t have much instruction in using spreadsheets, databases or mapping software for journalism. You’ve been focusing on the other fundamentals – writing, editing, maybe some multimedia skills – all of which are a proper part of your education. So what is “computer-assisted reporting” and why should you attend this conference?\nI was pretty nervous to attend my first NICAR conference, as they used to be called, and when I sat around the hotel bar for the first time I’m pretty sure I made no worthwhile contributions to any discussion. But I listened to, and learned from, people who are giants in our line of work, and they were generous and awe-inspiring. And I found my people.\nWhat do we do? We work with information in its many formats: text, maps, spreadsheets, databases. We learn to crawl, then walk and sometimes break into a light jog. Mostly, we get ideas that we can put to work. We try to find stories in places that other reporters don’t look – in documents or data, for example. And we try to use many different methods of storytelling, because not every story requires 800 words and a locator map.\nLet me give you an example. In 1996 I worked for The Palm Beach Post doing some metro reporting and trying to do data work when I could, and my window to that world was the NICAR community. One of my colleagues, Andrew Metz, had heard about a local official doing land swaps with his county’s largest landowner that gave the official a larger, more valuable piece of personal property. But when Andrew went to try and nail it down, he found that this county kept its property records on 3×5 index cards. Even for a small county, it would take weeks of work to figure out.\nBut I remember hearing from the NICAR-L listserv about stories that analyzed property records kept at the state level, and found out that Florida indeed maintained a database of property records from all 67 counties. I promptly ordered the data from the state and was informed that two 9-track tapes were on their way to our office.\nAt this point you might be saying, “9-track tapes?” I sure did. But once again the NICAR community saved me. In this case, one of the legends of CAR, Elliot Jaspin, had written software to convert information stored on a 9-track tape into something that a PC could read. To quote Jaspin: “A reporter who can’t read a magnetic tape is as illiterate as the 15th Century peasant confronted by Gutenberg.” You probably never heard that in your classes, but it’s true. Andrew was able to write the story, using the data to prove what previously had only been rumor.\nThe point here is not that each of you must master a broad set of data-related skills and be experts at everything. The point is that by connecting with this community of journalists, you’ll find out that so much more is possible than you ever knew. Stories you may have dismissed as undoable suddenly are much closer to becoming reality, and new forms of storytelling are now within your grasp. That you literally can make yourself more productive and more valuable. The community around this conference is entirely about sharing what we know and bringing more people into the fold.\nNow, we’re journalists here, so we can also be cranky, or slightly inappropriate at times. IRE members are, by and large, self-starters driven to better themselves, and they don’t have a ton of indulgence for those who aren’t willing to work. But if you’re willing to listen and try (and sometimes fail), there are plenty of rewards.\nOnce you’re in, you’ll become a contributor, too – on the listserv and at conferences (see some of my early presentations if you’d like a chuckle). You’ll become an evangelist for making better use of data in your own newsrooms, and you’ll stand out among your peers who cannot do what you can.\nIRE is friendly to students, too. Registration for the conference is $100 for students, and that includes does not include IRE membership, but that’s just $25. If you can get to Raleigh, there’s probably someone who is willing to share a hotel room (ask Andy Boyle, who drove from Nebraska to Indianapolis in March 2009 to attend). If you want to find a community of people who are doing interesting and valuable things in journalism, who are among the leading practitioners of the craft and who are eager to share what they know, then you should be in Raleigh in February. I will be, and I’ll be happy to say hello and welcome you to the club."
  },
  {
    "objectID": "archives/2019/03/09/the-more-important-problem/index.html",
    "href": "archives/2019/03/09/the-more-important-problem/index.html",
    "title": "The More Important Problem",
    "section": "",
    "text": "The first indication that I did not approach this NICAR-L improvement project in the right way should have been when I posted on GitHub a CSV file with metadata from 16 years’ of posts to the listserv. Not long after, I got a message saying that maybe I should take it down because it disclosed individuals’ email addresses.\n“Huh,” I thought. “Why would it be a problem for journalists’ emails to be public?” I asked myself, a guy who has been on the listserv for more than 20 years whose worst experience on it consists of people I respect dunking on me in a way that I wouldn’t object to. The social and professional risk of posting on the listserv is low for me, but not for everyone. The Internet, by and large, has been a pretty safe place for me, but not for everyone.\nIf I needed any reinforcement of that initial indication, it came when I looked at the small group of people who showed up at the conference session to talk about ways to improve the listserv experience. The folks who came – all good, smart people – had this much in common: they were, like me, white guys. I had billed the session as an attempt to explore technical solutions that would make the listserv more useful for its expanding community of users. Maybe even a chance to restore the listserv’s culture to what it had been in the days when the community numbered in the hundreds, not the thousands. You know, when it was mostly guys like me who shared a lot of the same experiences.\nThat I had not really thought much about how different people experience the listserv differently says a lot about the actual challenge we face in trying to make NICAR-L more useful for everyone (and something about me, too). The easier lesson is that technical solutions may be necessary to achieve the goal of a better listserv, but they are not sufficient. The harder - and more important - lesson is that if we want things to improve, we’ll need to think hard about what factors into individual decisions to post and not post, and what kind of community we want. Otherwise the technical work won’t be addressing the more important problem.\nThere’s a Google doc from our initial discussion that includes some potential goals and ideas we talked about. All of them are possible and many of them could make a real difference, even if only in a small way. I don’t think any of them are outlandish or techno-utopianism run amok. (You can, if you like, comment on them.)\nBut the idea that we could make fundamental improvements in the listserv experience without addressing the culture that surrounds it is folly. We can’t recommend NICAR-L to students and newcomers and then expect them to magically know how to adjust to whatever tone and culture they find if we don’t define the culture we want and build the processes that encourage it. We also can’t consider the listserv in isolation. The NICAR crowd has other options for sharing knowledge and building community, from Slack to affinity groups, and those are unlikely to disappear. Those competitors might help inform how we consider the listserv, too.\nSo what’s the current culture of NICAR-L? I’m not sure I have a good answer, because my own perception is influenced by how I experienced it 15-20 years ago. One of the participants at the conference session said he doesn’t post because he’s not sure who is on the listserv. When I was an active participant that wasn’t a concern for me. Now, I get the sense that NICAR-L is not as homogenous as it was (which is a good thing for many reasons) and that there’s also greater uncertainty about what the listserv is for and what people should expect from it. That’s a shame, because I really believe that there is a lot of value not just in the archives but in the current membership. But in my excitement about the possibilities, I looked right past the idea that human problems often require human intervention.\nHere’s what I’m going to do: I’d still like to explore some of the ideas that were raised at the conference session, with the goal of trying to understand the culture of the listserv and how we all react to it. And I’d like to do what I can to encourage all of us to talk about what we want from the listserv and what we need to get there. For me, one step is to re-engage more on the listserv. Ultimately, technology will only make a difference if we have clear understanding of the problems we’re trying to solve and if we’re willing to have an open conversation about the kind of culture we want to have."
  },
  {
    "objectID": "archives/2019/02/24/making-the-best-nicar-listserv/index.html",
    "href": "archives/2019/02/24/making-the-best-nicar-listserv/index.html",
    "title": "The NICAR-L Improvement Project",
    "section": "",
    "text": "If you’re coming to the 2019 CAR Conference in March, chances are you’re a member of the NICAR-L listserv. The only session idea I pitched this year was on how to make the listserv better, and I’d love your help.\nListservs are fascinating social experiments. People join, ask questions of strangers and then hopefully through discussion become fuller members of a community. When it works, it’s one of the best things about the Internet. When it doesn’t, the whole point of a listserv - the sharing of knowledge - falls apart. NICAR-L is no different from any other listserv, but because IRE is a member-driven organization, we’ve got a chance to make real improvements using the skills we have.\nIf you’ve been a member of NICAR-L as long as I have – since at least 1997, as near as I can figure – then you know how much it has changed. There are many more people on it now, which is really great. Interest in our little corner of the journalism world has grown and people want to figure out how we do what we do. That growth naturally brings some challenges with it.\nHere’s one example: about six years ago I made a new Gmail label: “Best of NICAR”. That was my way of dealing with a listserv that, in my view, suffered from a lot of repeat questions and the loss of shared understanding that can happen when a community experiences rapid growth. I know that I interact less on the listserv, and I’m pretty sure I’m not alone in that.\nTo be very clear: NICAR-L’s growth is itself a very good thing, particularly in that it has brought different voices into our community with different ideas and problems. When I joined IRE, the CAR conference was attended by 300-400 people, mostly white men with very similar interests and backgrounds. Many of the questions on the listserv were ones that a lot of us had in our minds, which made the conversation “easier” if not better.\nWe cannot and should not attempt to return to the time when most CAR conference attendees knew of each other (or actually knew each other). We should try to have a listserv that provides the kind of guidance and community that I was fortunate to have as a younger journalist.\nThe CAR conference session this year is pretty vaguely defined, and there’s a limit to what we can accomplish in an hour. That’s why I’m writing this, to put forth my hopes and (this being the CAR conference) some data we can use. This isn’t a fixed road map, just a starting point.\nSome possible goals:\n\nMaking the most useful posts and threads easier to find.\nCreating a categorization scheme of some kind, or adapting an existing one.\nMaking it possible for users to provide feedback on the usefulness of posts.\nA better UI for the listserv archive.\nMaking it easier to find answers to common questions.\n\nMany of these would require a separate web app, and while IRE has been extremely encouraging of this effort, I think it’s difficult to expect the organization to, say, redirect money dedicated to newsroom trainings towards listserv improvement. What might make such improvements more likely is the active involvement of IRE members. That’s where you come in.\nIRE has very helpfully provided listserv archives from 2002 to the end of 2018. We’ll make those available to session attendees, although I’m not posting them on the Internet. As a starter, however, I’ve posted on GitHub a basic Python parser and metadata from each of the postings.\nSome possible questions to explore:\n\nCan we identify valuable or interesting threads?\nCan we identify common questions repeated often?\nAre there categorization schemes we can apply to posts or even post metadata?\n\nWhat else can we do to make NICAR-L better? Feel free to add an issue on the GitHub repository or even post a thread on the listserv!"
  },
  {
    "objectID": "archives/2018/04/05/the-best-training/index.html",
    "href": "archives/2018/04/05/the-best-training/index.html",
    "title": "The Best Training",
    "section": "",
    "text": "I’ve been asked by students what the best training is to be a journalist. My position on this is very clear: waiting tables at a honeymoon resort is the best training.\nNo, really, that’s it. After you’ve delivered breakfast in bed to people who arrived late the night before, probably hammered, and now in the early morning can’t find the door, much less their pants, you can pretty much deal with any sort of situation that requires human interaction.\nInterviewing strangers can be hard. But most of them are at least wearing some clothes, which reduces the awkwardness, you know?\nI spent four summers and various holidays at a honeymoon resort in the Pocono Mountains of Pennsylvania, one of the places with those giant fiberglass jacuzzis shaped like champagne glasses. Should you go, please listen carefully: the instructions say you put a CAP-ful of bubble bath into the jacuzzi. Because it’s a jacuzzi, not a bath for an infant. If you pour the whole bottle in, you get that TV commercial where the bubbles grow eyes and fan out across the floor like an invading army. Housekeeping will hate you even more, and that’s an achievement for people who are cleaning up after you and your beloved have tried out the place, if you know what I mean. One cap-ful. I can’t set the breakfast tray down if the table is covered by 3 feet of bubbles.\nHere’s how it works: I’d drive a golf cart equipped with a tray-carrying back up to your door, grab the trays and start knocking. You could always tell if a couple was new because you’d hear them crashing into unfamiliarly-placed furniture on their way downstairs. There are windows in some of these rooms, but generally honeymoon resorts aren’t the place where you want an audience. When the man (it’s almost always the man) opens the door, he usually realizes two things: one, that the presence of sunlight has temporarily blinded him, and two, that there is an ACTUAL PERSON standing there with breakfast. He immediately tries to hide his body behind the door.\nThat’s a pretty smart play, with one flaw: the walls of the place are covered in mirrors. Ain’t no hiding place here, buddy, and I can see that I won’t be getting a tip because YOU HAVE NO PANTS ON.\nThere are two approaches here: you can do the silent nod, drop off the tray and get out of there as quickly as possible. That’s the route most newbies take, because I mean, it makes sense. Why would you want to hang around and chat? But that’s what the veterans know: the cheerier you are - “GOOOOD MORNING!” - and the longer you linger, the better the chance that you turn the awkward tables on your already uncomfortable hosts. You do this all the time - they don’t. That’s when you gotta do the small talk, asking how their stay has been, whether they’ve used the jacuzzi yet, or just saying how nice it is outside. After a few seconds, you can tell if he’s going to go back upstairs and get some money or not.\nThe only real risk with lingering is that you get asked to play photographer. Yes. That is a thing. Ok, it’s not a naked thing, but it’s still a thing. Sometimes people want to have their picture taken on the bed, because it’s a round bed and, really, when are you going to be lying in a round bed again? The photo session is high on the discomfort factor, but if you stick it out you almost always get a tip. And you’ve definitely entitled to give the head nod when the couple arrives for dinner that evening. No waving, though. That’s just weird.\nRepeat that process 15-20 times a morning for three months and I assure you that approaching complete strangers to talk is nothing, even if you’re asking some pretty personal questions.\nThat’s the best training. There may be some long-term side effects."
  },
  {
    "objectID": "archives/2011/03/06/what-apis-mean-for-data-journalists/index.html",
    "href": "archives/2011/03/06/what-apis-mean-for-data-journalists/index.html",
    "title": "What APIs Mean for Data Journalists",
    "section": "",
    "text": "Anthony DeBarros of USA Today and I talked about APIs at this year’s CAR conference in Raleigh. We got a lot of “Web people”, to use a lame expression, in the audience. If you’re a reporter who works with data, why should you care?\nThe simple answer is that APIs are an extension of what reporters do every day: ask questions. The difference is that instead of forcing reporters to gather data from multiple sources, format it to fit your local database needs and then update that database when new releases are available, APIs allow reporters to query live data from all over the Web. If you have experience working with, say, Microsoft Access and setting up an ODBC connection to a remote database, APIs are kind of like that – except that you have near-instant access to more sources of data, more useful tools (like geocoders) and more timely information than ever before.\nMy path working with data went something like this: spreadsheets came first, which I routinely describe as the “gateway drug” of computer-assisted reporting. Some people become such Excel wizards that it almost doesn’t make sense for them to move beyond that expertise; there is so much you can do in a spreadsheet that alone it would be worth the time to learn. But there were things about spreadsheets that annoyed and frustrated me. Pivot tables were a clumsy fit for me – they got me close to what I wanted in many instances but never quite there. And so I moved onto databases.\nDatabases are still one of my favorite things. They are powerful, relatively flexible and range in utility from the ultra-portable SQLite to the transactional goodness that is PostgreSQL. But they take time and effort to build, maintain and – perhaps most importantly in the long run – connect to additional sources of information. APIs are not a complete solution to these problems, but they provide a very good one that data journalists should be familiar with and consider incorporating into their work.\nA simple example is the reporter who wants to track the votes of his or her state’s delegation in Congress. There are several APIs for this data, including the one I work on and another by OpenCongress. The reporter could build a database of these votes by hand or write scripts to parse the House and Senate vote data and insert them into it. But why, when the data is freely available via HTTP?\nIt can’t be that simple, can it? Well, no. But it can be simpler. The data you get from APIs usually comes in XML or JSON. Data journalists have, for better or worse, been dealing with XML for awhile now. JSON may be less familiar, but it is quite nice to deal with and there are plenty of libraries with which to do so. But even better than that is the fact that other people have already solved that problem for you. Not long after we released the NYT Congress API I noticed a Ruby client library for it on Github. I had never met the author; he had never contacted me. Just the same, he made it easier for people using Ruby to query the API and get back data. There’s also an excellent Python library for it, written by NPR’s Chris Amico.\nThus can you, the data journalist, benefit from other people who need and use APIs. Check out GovKit, a Ruby wrapper to multiple government and political APIs, created by the folks at the Participatory Politics Foundation. Go play with it, and figure out what sorts of things you can do when the number of data sources you’re able to tap into multiplies overnight. The possibilities for journalists are only limited by the kinds of questions we can imagine and try to answer. APIs can make it easier to act on that greatest of questions: What if?"
  },
  {
    "objectID": "archives/2011/05/01/interviewing-data/index.html",
    "href": "archives/2011/05/01/interviewing-data/index.html",
    "title": "Interviewing Data",
    "section": "",
    "text": "To my mother’s regret, I was never the literature lover she is. And I am not remotely the writer I might have been expected to be, given that my parents both taught English, one at the high school level and the other at college. I also am not the most graceful of interviewers, as my questions tend to run on for too long instead of zeroing in on clear questions.\nYou might ask, “How is it that you’ve managed to keep a job in journalism, then?”\nThere’s no single answer to that, although the majority of it would have to be everything I learned from being a member of Investigative Reporters & Editors. And of that part, what I’ve really learned to love and work at is the other kind of interviewing. The one you don’t hear much about in journalism school: interviewing data.\nTo be fair, you really don’t hear all that much about the craft of interviewing people at journalism school, either. There is the occasional class, but the way that most people I know get better at it is simply by doing. When people ask me how I can approach complete strangers and ask them detailed and occasionally personal questions, I’m quick to reply that I spent four summers delivering breakfast in bed to newlyweds in the Poconos. When you’ve had a naked man answer the door at 8 a.m. and tell you to put the trays down next to the tripod-mounted video camera, talking to evenly partially-clothed strangers gets pretty easy.\nInterviewing data takes practice, too, although I can’t really find a parallel from my days waiting tables. Both kinds of interviewing have much in common: you want to be as prepared as possible so as to better evaluate the results and be able to adapt your questions to the situation. Both require you to place a solid block of skepticism, even suspicion, on your shoulders as you embark. And both, if done well, can result in an unexpected admission – something even the subject of the interview didn’t really “know”.\nThis is why I continue to teach spreadsheets in classes, because they make for excellent initial interview tools. Looking at some data in a spreadsheet, you can easily size it up with basic sorting and filtering. That’s kind of the “getting-to-know-you” phase of the data interview. What are the ranges of this data? What looks unusual? Just as you get first impressions upon meeting someone, you get similar feelings about data.\nWith data you have to ask all the basic questions you do with a person, just so you know exactly what you’re dealing with. Questions like: “How old are you?”, “Where were you born?”, “Who do you report to?” work for both people and data (although I suppose “made” is a better word than “born”). And then, once you’ve got a solid foundation, you ask the trickier questions, the ones that you need to really think about. The ones that, when you’re planning a big interview with the subject of your investigation, you game-plan and write out as if they were lines in a soap opera.\nAnd that’s where the big difference is: with data, you can ask a lot of potentially embarrassing questions, and the data won’t complain, walk out or threaten to sue. You can ask variations of the same question 20 times and the data won’t mind. When I say that I prefer interviewing data to people, this is why. Data will only lie to you if it’s just bad data or if you misunderstand the question. Unfortunately, almost every data set is “bad” in some way. But once you find that out, you usually can deal with it.\nWith the increased availability of information in structured forms, the skill of interviewing data is even more valuable now that it has been in the past. And yet it’s still considered a niche, a specialty skill. It’s odd, because what makes a good interviewer is not whether she uses a digital recorder or a pen. The technology itself is a tool. The crucial factor is the skill in being an interviewer – preparation, knowing what questions to ask and knowing when something isn’t right.\nYou wouldn’t stumble into an interview with a source having done no research, no preparation. Why in the world should journalists treat data sources any differently?"
  },
  {
    "objectID": "archives/2011/10/17/what-we-dont-know-about-elections/index.html",
    "href": "archives/2011/10/17/what-we-dont-know-about-elections/index.html",
    "title": "What We Don’t Know About Elections",
    "section": "",
    "text": "If you happened to be at the recent Online News Association conference in Boston and happened to attend the session on covering the 2012 elections, then a good bit of this will be repetitive. Since there wasn’t a ton of time to expand on what I said, and I don’t want to leave the impression that I’m critical of all election coverage, consider this the write-through.\nFirst, I stand by what I said about how little we understand about the way that elections are won or lost these days. It’s not that political journalism has strayed from its roots, or stopped covering important elements of a modern campaign. It’s that the elements of a modern campaign have changed, and as journalists, we have not kept pace.\nYou might respond that campaigns still involve quite a lot of stuff that we do understand, such as debates and visits to state fairs and town hall meetings. True. But the nature of media and technology has brought extensive changes to the electoral system, and I don’t believe that we as journalists devote enough attention to understanding those changes. Remember the Dean campaign in 2003? Most of the coverage was on the, for then, staggering online fundraising managed by some doctor from Vermont. But that Wired piece I referenced had it right; Dean’s accomplishment was less a mastery of the Internet but a willingness to embrace its fundamental aspect: you give up some control by bringing other people in, and you gain a host of possibilities. You may, of course, choose badly or falter in some other way, but the lessons and possibilities are becoming clear. At the time, as a Web geek who loved politics, I felt that journalists couldn’t really explain the Dean campaign, because it was so alien to us. Today’s campaigns make me long for the simplicity of 2003.\nBut let’s stick with fundraising for a bit. Political fundraising can be hugely expensive, because campaigns need to amass large number of donors. Unless you’re the President, it’s hard to repeatedly gather the wealthiest Americans and have them fork over $2,500 or more for the pleasure of your company. So a smart campaign sticks with what works: direct mail is costly, for example, but it’s also effective. Telemarketing takes time and money, but it also works pretty well. Let’s not mess with the script too much. But what if you can mess with the script? Now it’s possible, even trivial, to experiment with Web site design or even advertisements in order to gauge their effectiveness and improve upon them. President Obama had a Director of Analytics for his 2008 campaign, and has been hiring data scientists experienced in predictive modeling.\nWhite men smoking cigars in cramped rooms making gut calls is how we’ve usually understood campaign decision-making. This? Whole new ballgame. Yes, there is still a mass audience that is shaped by the media and big events. But there are now thousands and thousands of “small” audiences – or rather, they always were there. Now campaigns can identify them and deliver precision messages to them. And they can find them online in different ways; an hour after posting on Twitter about the Obama’s campaign use of Github, the campaign’s Digital Director was following me. And that’s the easy part.\nWhile campaigns have a public presence that is mostly recorded and observed, the stuff that goes on behind the scenes is so much more sophisticated than it has been. In 2008 we were fascinated by the Obama campaign’s use of iPhones for data collection; now we’re entering an age where campaigns don’t just collect information by hand, but harvest it and learn from it. An “information arms race,” as GOP consultant Alex Gage puts it.\nFor most news organizations, the standard approach to campaign coverage is tantamount to bringing a knife to a gun fight. How many data scientists work for news organizations? We are falling behind, and we risk not being able to explain to our readers and users how their representatives get elected or defeated.\nNone of this is to say that we need to completely abandon our ways of covering elections. Horse-race coverage is and should be a part of campaign coverage, because in many respects elections are like horse races. Things can change rapidly, and small things can have big impact. We still should be on the ground, talking to voters, showing up at town halls and covering debates. We still need to show up and do the legwork.\nBut if we can’t appreciate, much less understand, what modern campaigns are doing to win elections, how can we hope to explain elections? If we don’t collect at least some of the information available to us – realizing that we can’t get our hands on everything that the campaigns do – we’ll miss the story. Elections will become even bigger surprises to us, and then how long will it be before readers start to ask whether we actually know the people and places we cover?\nSurprises make the news. Some of my favorite stories from the 2004 presidential election are in a book by my friends Peter Wallsten and Tom Hamburger, then of the Los Angeles Times. Here’s one anecdote from the key state of Ohio:\nOne suburban African American woman in Ohio, for example, told us that though she tends to vote Democratic, she was deluged in 2004 with calls, e-mail messages and other forms of communication by Republicans who somehow knew that she was a mother with children in private schools, an active church attendee, an abortion opponent and a golfer.\nThink about what this kind of thing means. It means that we cannot assume that the campaign visible to the mass audience is the same campaign that’s being pitched to individuals and groups around the nation, and that winning coalitions can be built not just by harnessing large groups (unions, religious voters, etc.) but also by piecing them together in small units. President Bush’s margin in Ohio in 2004? About 2.5 percent. The only thing that I don’t like about this anecdote is that Wallsten and Hamburger’s book appeared nearly two years later. Is there any evidence that we as journalists have closed the gap since then?\nTo understand how elections are now being waged, we need to have as many of the tools as do the campaigns. We need to build our own storehouses of data – voter registration, voter history, Census, campaign finance, advertisements and more. We need to be able to tap into the rich stream of material that’s being created and disseminated every day. We need to be able to see the value in small data points that can lead to bigger things.\nElections are great stories. They deserve to be told from a position of confidence and knowledge. We have work to do."
  },
  {
    "objectID": "archives/2011/07/27/why-teach-sql/index.html",
    "href": "archives/2011/07/27/why-teach-sql/index.html",
    "title": "Why Teach SQL?",
    "section": "",
    "text": "There was an interesting discussion on the NICAR-L listserv today about teaching database skills. More specifically, which software to teach and how to teach it. Should you go with SQLite, as I do? What about MS Access (the consensus seemed to lean against)? Is it too much to ask students to install database server software such as MySQL or PostgreSQL?\nThese are complicated questions, made moreso by the options now available for teaching database skills. When I attended an IRE database bootcamp in 1997 (taught by my now-colleague Jo Craven McGinty), there were basically three options: the then-young Access, FoxPro or Paradox. Hard to believe, but back then I worked in a newsroom that had FoxPro and Paradox, but not really Access (Note: if you are under 30 and reading this, you may not even know what FoxPro and Paradox are. That’s ok. They, an in particular FoxPro, were wonderful database managers in their day.)\nNot only do we now have open source options (SQLite, MySQL, Postgres) and SQL Server, but we also have a variety of “database-like” Web applications, like Fusion Tables and Google Refine, that can do some of the things that only desktop software used to do. And let’s face it, Excel is a very powerful tool for data analysis. Many of the things a reporter might want to do to a data file, such as sorting and filtering, are arguably a lot easier in Excel or another spreadsheet.\nSo why even teach SQL, then? The reasons I do it, and will continue to, are these:\n\nSQL is an excellent and relatively simple way to enhance your data interviewing skills. When you have to write out your questions, you tend to think about them a little more than if you’re just pointing and clicking around. This is why when I had to teach Access, I bypassed the visual query builder. Yes, SQL queries involve writing more than doing an Excel filter, but those syntax errors also make you consider what you’re doing, and that’s a good thing.\nSQL is still common enough on the Web that teaching it provides an additional branch, if you will, of learning, or at least makes it easier. When I explain how Facebook assembles all your friend’s posts, comments and pictures, I usually do so by pointing out the existence of FQL. If you already know SQL, it’s a very small leap to understanding, at a basic level, how Facebook works.\nThere are some times when you will absolutely need to use a SQL database. Or, at least, something that’s not Excel. Multi-million-row tables. Regular expression-based pattern matching. Intensive, complicated queries. If you haven’t explored SQL, you might not know these are even possible, and you might give up.\n\nAs to what to use when teaching SQL, I stick with SQLite despite Sarah Cohen‘s completely valid point that date and time support is much more complicated than it should be. Perhaps a new installment of Troy Thibodeaux’s excellent tutorial will help address that issue. In the meantime, let’s keep teaching SQL – and asking questions."
  },
  {
    "objectID": "archives/2011/08/10/in-defense-of-building-tools/index.html",
    "href": "archives/2011/08/10/in-defense-of-building-tools/index.html",
    "title": "In Defense of Building Tools",
    "section": "",
    "text": "My first job in Web development was as a member of washingtonpost.com’s “Tools Team.” I was, in title if not in practice, a Tool.\nDone snickering? Let’s move on.\nThe Tools Team built mostly internal applications and services that helped the Web site run better. I mainly got to work on front-facing projects like the Congress Votes Database, the 2008 presidential campaign and an innovative series on lobbyist Gerald Cassidy. But I did work on a few internal tools, and since I joined The Times in late 2007 I’ve built a few more. I’ve found that such tools are not so different from what we now consider to be journalism by Web development. Chosen wisely and done well, they can have impacts that go far beyond a single story or series. We should not dismiss them as “not journalism.”\nIf you’re at the geekier end of the journalism spectrum, then chances are your colleagues know about the stuff you can do. They may not understand it or be able to explain it; a former managing editor of mine, when told about the various technical steps to accomplish something useful, would invariably respond with a touch of wonder: “Fuckin’ Internet!” You can explain your work to a decent percentage of your colleagues by invoking Harry Potter or the Lord of the Rings and leave it at that.\nBut that doesn’t mean that building tools that can be used by broad segments of the newsroom is a one-way street or has to lead to a divide between you and the other journalists. There will be people in every newsroom who mainly take and rarely give, and in those situations being a technologist is no different from being a clerk. Good tools, like good apps, are a product of collaboration and improve the ability of the newsroom in general. They also make for more and better apps.\nCase in point: At The Times we have an Inside Congress app that displays information about votes and bills in Congress. The tool that underlies that app is enormous – it has tons more information, and we’re working to surface more and more of it. But the tool – an internal interface – has uses for our congressional reporters, our graphics editors and for me as a developer. I can point a reporter to the vote record comparison tool instead of having to run a database query or, worse, asking someone else to manually recreate something. We use the tool as a sort of canary in the mine to alert us to odd or interesting events, from committee assignment changes to bill sponsorship withdrawals to unusual voting patterns. In some cases, having the data internally has led to improvements in the app itself, such as our “key amendments” pages for certain bills. I didn’t think of that, but someone else who saw the internal tool did, and we built it together.\nPerhaps most important to me as a developer, building the internal tool has broadened the number of people I work with and has given me a range of ideas for making apps easier to build and better. Not all of them pan out, but some of them do. Put another way: the tool actually helps me develop closer working relationships with my colleagues.\nA good tool doesn’t just make it easier for a reporter to create a story. It actually seeds the story, or makes it possible for more people in a newsroom to collaborate. When you have data but no tool, you become a gatekeeper of a sorts – which is appropriate in many circumstances, but not all. I can’t possibly know what my colleagues are thinking about, considering or being alerted to, but I can make it easier for them to test out theories and do some exploration on their own. Some of them prefer to do their own work, and we certainly miss some opportunities for apps that way. But others consult with me quite a bit, since they now have a much better idea of what we have and what we might be able to do with it.\nSkeptics might respond that there is a difference between tools built around journalistic content, like the Congress app, and those that “merely” solve a technical problem. This is a short-sighted argument. What we do as builders of Web applications (external or internal) is informed by everything we touch. Pulling a piece of one tool for use someplace else is a useful technique because it reinforces the value of not repeating yourself and because it sometimes enables you to look at an old problem or situation from a new vantage point.\nBack at washingtonpost.com, my former colleague Adrian Holovaty liked to say that we didn’t build internal versions of our apps because the public version was the internal version. Fair enough, to a point, but I think that line can veer into the data ghetto when not rigidly policed.\nMost of my colleagues, I’m confident, have very little idea what it is that I specifically do. Sometimes I spend the time educating, and sometimes I let our tools help with the evangelization process. However they see my work, I’m pretty happy as long as it contributes to our journalism together. App developer? Sure. Tool maker? Why not. Labels don’t interest me much, and most of my colleagues don’t seem to care. The results – the journalism – are what matter."
  },
  {
    "objectID": "archives/2016/09/29/government-and-civic-tech/index.html",
    "href": "archives/2016/09/29/government-and-civic-tech/index.html",
    "title": "Government and Civic Tech",
    "section": "",
    "text": "Looking at what has happened to civic technology organizations at the national level lately, you could get the impression that the federal government, having absorbed some of the lessons of organizations like the Sunlight Foundation and Code for America, has pulled energy and talent away from those entities, making it harder for them to exist.\nI think there is some truth to this, but it is not nearly the whole story and we risk drawing the wrong lessons from it. In general, I subscribe to what Josh Tauberer says on this, but I think it’s worth exploring what it means after this migration from civic organizations into government.\nThe flow of people from civic technology groups (and those in similar orbits, like journalism) into government service is, in once sense, part of a progression. Organizations like Sunlight have done some of the work they argue that government should be doing: making data more easily available and creating user experiences that citizens might actually find useful. It makes sense that governments do that where appropriate, and to some extent that could impact Sunlight’s ability to raise money to do similar work.\nI am not convinced that government technologists are doing the same work, however, or that, as Bill Hunt writes, such efforts are “seemingly bypassing the need for civic tech entirely.” Labeling that kind of work as civic tech is accurate only up to a point. Instead, I’d offer “government tech” as a more fitting description of the work being done. This makes sense: if the federal government hires technologists, it stands to reason that they would work on government technology issues and problems. The examples of 18F and the U.S. Digital Service show that, given the resources, government technologists will tackle government problems like procurement, design and data integration.\nThere are some exceptions to this – I am still impressed and somewhat surprised that 18F has been working on an API for the Federal Election Commission’s data, for example – but as a taxpayer I would expect that government technologists work on the many issues and opportunities their situations present. One could reasonably argue that there is a civic component of this, and to a point that’s true. But I do not think it is identical to the mission and purpose of organizations such as Sunlight or to the work of individuals like Josh Tauberer. Parallel at times, sure.\nThink of it this way: if Congress ever decided to do something like 18F or USDS, an obvious project would be adding voting data to Congress.gov. But there’s a reason that Congress.gov, as fine as it is, doesn’t show recent votes by lawmakers or offer users the ability to compare voting records: it serves no government purpose to do so. Comparing voting records, or seeing how often your representative misses votes, are civic tasks: they are designed to benefit citizens, regardless of the wishes of elected officials. That’s one reason people like GovTrack: it tells them things they want to know about their representatives that those representatives have little incentive to provide.\nThe barriers to providing such information have never been technical, not really. When Josh and others were asking the Library of Congress for bulk legislative data 10 years ago, it wasn’t as if the technical capacity to make it happen didn’t exist. The reasons given for it not happening were entirely political.\nThat doesn’t mean that government technology work has little or no civic benefit: plenty of it clearly does, and my friends who work in agencies in desperate need of their talent are doing this for all of us. As a taxpayer and someone who works with technology, I value that work and I hope it continues.\nBut it might not, for a couple of reasons. Funding for government programs can end, some of these jobs are explicitly temporary and many of the people attracted to government service in this area have skills that are worth far more in the private sector. If you are hired into the federal government near or at a GS-15 level, there is little room for you to earn more. Some could choose to stay, for good reasons.\nSome of the people who joined 18F and USDS will go back to the private sector, and we’ve got some for-profit efforts that do similar work in the civic space, too. That’s not a bad thing; it validates the idea that information can be powerful and useful. I don’t begrudge people making money from public information (well, maybe I do if they are government employees themselves), but as with government work, it’s not exactly the same work as what Dan O’Neil and his former colleagues at Smart Chicago Collaborative have been doing. That stuff is hard.\nInstitutions and programs come and go. What remains is the need for focused work that makes it easier for citizens to understand what their governments are doing with their money and in their name. Some of that can be done by government. Some of that can be done by the private sector. But some of it - the less popular stuff that can’t be easily monetized or takes a lot of effort to do - won’t ever happen without dedicated people who work outside those two sectors. That’s part of what made the idea of Sunlight Labs, even if it didn’t always deliver the goods, so necessary.\nI agree that we largely are past the days of foundations throwing millions of dollars at individual tools and platforms that marry technology with a civic purpose (and here I should say that a project of mine, OpenElections, got $200,000 from the Knight Foundation, which was a huge help). That’s probably a good thing in some respects, because it led to a surplus of similarly-oriented projects that all required maintenance. We’ve seen the result of that: consolidation or elimination, hopefully preserving the best ideas and practices. If funders want to see a different focus, or more explicit outcomes, the people working in this area need to respond to that or find another way to do what they want to do.\nIt may be frustrating to everyone in this space that we are still working on the infrastructure of civic information, but there is so much work to be done, and there is clear evidence that building such platforms can make it easier for others to create meaningful things that put more power in the hands of citizens. Let’s keep doing that however we can, trying to avoid the mistakes we’ve all made, and not forget that people, and not institutions, are why we do this."
  },
  {
    "objectID": "archives/2005/05/18/xpdf-on-the-mac/index.html",
    "href": "archives/2005/05/18/xpdf-on-the-mac/index.html",
    "title": "Xpdf on the Mac",
    "section": "",
    "text": "Last year I wrote a piece for Uplink on using Xpdf to convert PDF documents into text tables, but that piece focused on using xpdf on Win32 systems. Here’s an adapted guide to installing and using Xpdf on the Mac.\nHere’s a question that should have a familiar ring: How do I get text out of a PDF file?\nPainfully, if your experience has been anything like mine.\nThe mere existence of Adobe Acrobat files has been a boon for reporters because governments and agencies everywhere have been able to make documents broadly available over the Internet. It’s hard not to love that.\nBut if you’ve ever tried getting tables out of a PDF document – and we’ve all tried – the results usually aren’t worth the effort. Until now.\nA free command line utility called Xpdf will save you time and aggravation. It will, in most circumstances, enable you to go from PDF to Excel in a matter of seconds, rather than minutes or hours. Did I mention that it’s free?\nYou can find Xpdf and it comes in packages for Windows and Linux/Unix. OS X users should download the source code (at this writing the file is xpdf-3.00.tar.gz) to their desktop. That file will expand into a folder labeled xpdf-3.00. Open up the Terminal and type the following (hit return after each step):\ncd Desktop cd xpdf-3.00 ./configure\nThis will take a minute or so. Then type:\nmake\nAgain, you’ll wait a few minutes until it finishes, then:\nmake install\nYou may have to use “sudo make” and, when prompted, enter the password for your computer.\nOnce xpdf installs, you can put a PDF file anywhere in your home directory (I usually have a single folder for this) and navigate to that directory in the Terminal using “cd /location of file”, and then typing:\npdftotext -layout pdfname.pdf\nDepending on the size of the PDF file, your output text file (with the same name as the original) will be in the same folder in a matter of seconds.\nLet’s go through the command line syntax. First, the command “pdftotext” is required for this process, and “pdf2text” won’t work. The “-layout” tag tells Xpdf that you want to preserve the layout that the PDF file uses, which keeps the text in those nice, clean tables. And you need to have the fullname of the file (I recommend a single-word name, even though Windows supports filenames with spaces). That’s it.\nThe resulting text file will be the entire text of the PDF, meaning that you may have to wade through pages of text in order to get to your tables. The preservation of the PDF’s layout means that if a page contained two tables side-by-side, that’s the way they will look in the text file, too.\nXpdf doesn’t work in all instances; specifically, it won’t convert PDFs that have been locked by their creators. Don’t bother asking the author of Xpdf, either, as he has posted a message on his Web site indicating that he will not add that ability.\nBut for most government documents, Xpdf can be a huge time-saver and allow you to spend more time actually analyzing data rather than trying to free it from the confines of the PDF."
  },
  {
    "objectID": "archives/2005/12/05/congressional-vote-database/index.html",
    "href": "archives/2005/12/05/congressional-vote-database/index.html",
    "title": "Congressional Vote Database",
    "section": "",
    "text": "So humor me today, and the CAR stories will return tomorrow. I’d like to announce a project that Adrian Holovaty of washingtonpost.com and I started working on a few months ago: the Post’s congressional votes database. The first of many, many Post web apps built using Django, this site is a browsable archive of votes in the House and Senate since 1991. It has plenty of features, including vote breakdowns (try “by astrological sign”) and RSS feeds for each member, so you can be notified when a vote has been posted. Adrian did the heavy lifting, but as a normal person he doesn’t have my intimate knowledge of Congress (thanks, CQ!). So it worked out well, I think. The site is powered by Python, PostgreSQL and Django."
  },
  {
    "objectID": "archives/2002/03/25/the-long-bet/index.html",
    "href": "archives/2002/03/25/the-long-bet/index.html",
    "title": "The Long Bet",
    "section": "",
    "text": "It’s Dave Winer vs. the New York Times in a bet over, well, it’s not exactly clear. Dave says it’s about “which will be authoritative,” although that’s not what the question itself says - or what the answer to the question will reveal. In fact, I’m quite sure Dave will win the bet, because right now Google is much better at indexing weblogs than the Times. Sort of like picking the judges. And the methodology of the bet itself is suspect. For instance, if we took Dave’s suggestion and searched Google for “Enron” today, Oliver Willis’ EnronGate blog is the 11th result, higher than any Times piece. But guess what? Oliver’s top three links are all Times pieces!. So which is more “authoritative,” Oliver for pointing to the Times or the Times for printing the story in the first place? Besides, the whole thing can be rigged anyway. But there are other points in Dave’s essay that deserve attention and rebuttal.\nFirst, there is little doubt that weblogs do enable individuals with expertise to get that information out to a wider audience. That’s a good thing. But the crux of my argument stems from this part of Dave’s essay: “This process is fed by the changing economics of the publishing industry which is employing fewer reporters, editors and writers. But the Web has taught us to expect more information, not less, and that’s the sea-change that the NY Times and other big publications face – how to remain relevant in the face of a population that can do for themselves what the BigPubs won’t.”\nYes, newspapers have been downsizing, although the pattern of hirings and layoffs is a natural cycle for the business. But the real point is that newspapers do things that very few individuals would do on their own for no pay – extensive investigations that take months and cost thousands of dollars. Some of these investigations are incredibly difficult to accomplish. Would bloggers have footed the bill for the Florida ballot review? Would they risk their lives to uncover corruption? Would they analyze thousands of records to show how a state board of medicine is failing to protect the public?\nMy guesses: No, no and no.\nI’m reminded of what Phil Meyer told the NICAR conference on March 16. He looked around the room at a small crowd - smaller than the year before, certainly - and asked whether our work was still important, whether now, in a time of budget cutbacks and profit margin pressure, investigative journalism still had a place. His answer? “Now more than ever.” In five years our nation and our world will need no less.\nNews organizations are more than just the people who have been covering the tech industry, and certainly we’ve made our share of mistakes. But there’s a larger picture to consider, and that’s where the work of a large news organization like the Times becomes so valuable.\nYou can define “authoritative” any way you like, I suppose, but I tend to think of it as meaning a message or product that will try to tackle not just the most obvious questions but those below the surface or on the periphery. Will bloggers know all about the Enron of 2007? I have no doubt. Will the Times or some other large newspaper be able to explain it in its entirety - the social, economic and political effects? I fervently believe it. If not, all of us - bloggers and non-bloggers alike, will be the worse for it."
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About The Scoop",
    "section": "",
    "text": "Hi, I’m Derek Willis, and thescoop.org has been my site since the late 20th century. I teach and do journalism, especially data journalism. I’m a sports geek (a fan of West Indies cricket & Maryland women’s basketball), a collector of political data and a reader of non-fiction (sorry, Mom). I don’t have a newsletter or a podcast. The image on the home page is from a thank-you note written by a seventh grader after a “Career Day” visit.\n\nAbout Me\nI was born in Pennsylvania, graduated from the University of Pittsburgh with a degree in rhetoric and communications and attended graduate school at the University of Florida (no degree, sorry again Mom). I began my journalism career at The Palm Beach Post, working in the news library and then as a reporter and technology coordinator. From 1998-2003 I covered Congress and elections for Congressional Quarterly. In February 2003 I joined The Center for Public Integrity as a writer/data specialist working on state and federal projects, where a report I co-authored was honored with a Sigma Delta Chi award by the Society of Professional Journalists. In November 2004 I became the Research Database Editor at the Washington Post; in February 2007 I became Database Editor at washingtonpost.com, and in November 2007 I became an Interactive Developer with The New York Times, working mostly on political and election-related applications and APIs. I joined The Upshot in 2014. From 2015-2021, I worked as a News Applications Developer at ProPublica.\nI was co-editor of CQ’s Federal PACs Directory, published in 1998, and of Unstacking the Deck, a guide to covering campaign finance published by Investigative Reporters and Editors. I’ve been a member of IRE since 1995 and have spoken at numerous conferences and training events around the nation on using the Internet and campaign finance data. I have served as an adjunct faculty member at The George Washington University, Northwestern University and Georgetown University, teaching data reporting classes. I live outside Washington, D.C., with my family. Email me at dwillis AT gmail DOT com.\n\n\nInternet Achievements\n\nFinal winner of Slate’s “Six Degrees of Francis Bacon” contest, April 18, 1998.\n“Wanker of the Day”, Atrios, Jan. 17, 2006.\nCited for wasting your time with Sarah Palin’s email by “The Daily Show”, June 12, 2011.\nDragged by AOC, Jan. 18, 2019."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Derek Willis",
    "section": "",
    "text": "Hi, I’m Derek. I love using data to find and tell stories. And asking questions.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome Back\n\n\nA new beginning, but not entirely.\n\n\n\nDec 31, 2022\n\n\n\n\n\n\n\n\n\n\n\nThe More Important Problem\n\n\n\n\n\n\nMar 9, 2019\n\n\n\n\n\n\n\n\n\n\n\nThe NICAR-L Improvement Project\n\n\n\n\n\n\nFeb 24, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "now/index.html",
    "href": "now/index.html",
    "title": "What I’m Doing Now",
    "section": "",
    "text": "I teach data and computational journalism at the Philip Merrill College of Journalism at the University of Maryland, where I try to get students excited about interviewing data and using it to make for better and different stories.\nIn my spare time, I write a weekly newsletter for Decision Desk HQ, a leading provider of live election results and related data, and analyze women’s college basketball data for Her Hoop Stats.\nI also run a volunteer project on election results called OpenElections. You can help us collect and convert official precinct-level election results!"
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "I’ve been teaching data journalism, either in professional and student workshops or at universities, since the late 1990s. Many of these sessions have been under the auspices of IRE, which has some of the best journalism training in the world (you should become a member, especially if you are a student).\nBut I’ve also taught or helped teach college-level courses at Washington Adventist University, George Washington University, Northwestern University, American University, Georgetown University, West Virginia University and the University of Maryland. Teaching is in my blood; my parents are both teachers, and there are few things that are more important. That’s why I’m a full-time teacher now.\nAlong the way, I’ve developed some strong feelings about how journalism is taught and learned, and have written about them from time to time. Here is a selection, in reverse chronological order:\n\nWhen Teaching Data Journalism, Keep It Simple for Students\nTake an Interviewing Approach to Find Stories in Data\nData Journalism, Student Media Edition\nThe Natives Aren’t Restless Enough\nTeaching Hospitals, Journalism Education and a Hatchet Job\nAcademy Fight Song\nWhy Teach SQL?\nThe Case Against Teaching Access"
  },
  {
    "objectID": "help/index.html",
    "href": "help/index.html",
    "title": "Ways I Can Help",
    "section": "",
    "text": "Summary: Think I can help? Email me: dwillis@<google’s email product>\nI have been extremely fortunate in my career in journalism, having worked at great organizations and with really good people. I know lots of folks and have had plenty of different experiences, and I’m in a position to use what I have learned to help others. This is a list of ways that I can help; if you see something here that would be useful to you, please email me.\nThere might be things I’m willing to do that aren’t listed here, but if it’s not listed here it might not be something I’m willing to do for free. However, do feel free to ask anyway – the worst I’ll do is not reply or say “no”. Journalism is about asking questions.\n\nWays I’m available to help anyone\n\nAnswer specific questions via email about topics I know about. If you write me an email with a short, clear question, and I know the answer, I’ll try to answer in a fairly quick time-frame. If you’re curious about something related to campaign finance, or congressional data or election results or cricket, ask away. I’m not going to do original research for you, but I will try my best to point you in the right direction or take a look at your methodology.\nIntroductions. I will provide introductions to people I know, if you tell me why you need the introduction and I believe it’s an introduction that the recipient would find valuable. Please include a brief paragraph about you that I can use when I send the email to the other person.\nAdvice on job searches and interviews, particularly within the data journalism world, but I do know folks beyond that, so ask away about your situation. This includes questions about compensation/benefits or help with resumes and cover letters.\n\n\n\nWays I’m available to help underrepresented high school and college students in journalism\nIf you are from a background traditionally underrepresented in journalism - especially in data journalism - I will do more.\n\nEverything listed above, I will do particularly for you. Not sure if I can help? Ask anyway.\nIf you are working on a story or a pitch or a presentation in an area I know about, I will review it for you. Again, if in doubt, ask.\nData journalism skills advice. If you’re trying to figure out how to do accomplish some data journalism task, or looking for materials that will help you learn new skills, I will help you. My assistance could range from sending a list of useful links to working through a problem or even providing some basic training (likely to be remote, but ask). For specific coding questions, your best bet is Stack Overflow, but happy to tackle broader questions.\nOpenElections. If you’re a student looking for experience working with election data, there’s a chance that I can get some funding for an internship with OpenElections, a nonprofit I run that acquires, converts and publishes election results in the U.S. Summers are probably best for this, but ask anyway."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Derek Willis",
    "section": "",
    "text": "Welcome Back\n\n\n\n\n\nA new beginning, but not entirely.\n\n\n\n\n\n\nDec 31, 2022\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe More Important Problem\n\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2019\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe NICAR-L Improvement Project\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2019\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Best Training\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2018\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAcademy Fight Song, Part 2\n\n\n\n\n\n\n\n\n\n\n\n\nDec 28, 2017\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGovernment and Civic Tech\n\n\n\n\n\n\n\n\n\n\n\n\nSep 29, 2016\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Favorite Things\n\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2015\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCivic Data and Journalism\n\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2015\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe’re All Publishers Now\n\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2014\n\n\nDerek Willis\n\n\n\n\n\n\n  \n\n\n\n\nLightning Strikes\n\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2014\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow It Starts\n\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2014\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Journalism, Student Media Edition\n\n\n\n\n\n\n\n\n\n\n\n\nOct 9, 2013\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Natives Aren’t Restless Enough\n\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2013\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeaching Hospitals, Journalism Education and a Hatchet Job\n\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2013\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Develop in the Newsroom?\n\n\n\n\n\n\n\n\n\n\n\n\nJul 16, 2013\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Good is Dat?\n\n\n\n\n\n\n\n\n\n\n\n\nJul 2, 2013\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLessons from ‘Data-Crunched Democracy’\n\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2013\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat They Say About Us\n\n\n\n\n\n\n\n\n\n\n\n\nMay 21, 2013\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAcademy Fight Song\n\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2013\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Itemizer\n\n\n\n\n\n\n\n\n\n\n\n\nApr 24, 2013\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSteve Coll\n\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2013\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMobile Apps - Where the data lives\n\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2013\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMagic Removal\n\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2013\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn Aaron Swartz\n\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2013\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Data-Driven Congressional Reporter\n\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2012\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn Vengeance\n\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2012\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThoughts on NewsFoo\n\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2012\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow I Got Here\n\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2012\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat I’ve Been Up To Lately\n\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2012\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCongressional Data on GitHub - A Way Forward\n\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2012\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShare Your Knowledge\n\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2012\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinding New Money\n\n\n\n\n\n\n\n\n\n\n\n\nMay 28, 2012\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLost in the Weeds\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2012\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur Mark Knoller Problem\n\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2012\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn Legislative Data Transparency\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2012\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat We Don’t Know About Elections\n\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2011\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn Defense of Building Tools\n\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2011\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Teach SQL?\n\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2011\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterviewing Data\n\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2011\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat APIs Mean for Data Journalists\n\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2011\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Students Should Come to the CAR Conference\n\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2010\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHard Problems\n\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2010\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Far We’ve Come\n\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2010\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA GitHub for Data?\n\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2010\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow APIs Help the Newsroom\n\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2010\n\n\nDerek Willis\n\n\n\n\n\n\n  \n\n\n\n\nUsing the NYT Congress API with … Excel?\n\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2010\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Gentle Introduction to Google App Engine\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2010\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLightning Talks at NICAR\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 18, 2010\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Gift of Data\n\n\n\n\n\n\n\n\n\n\n\n\nDec 25, 2009\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Future of IRE Training\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2009\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Question of Emphasis\n\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2009\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuying Into Computational Journalism\n\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2009\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe FEC’s Disclosure Data Catalog\n\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2009\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne Way to Encourage Innovation\n\n\n\n\n\n\n\n\n\n\n\n\nJul 24, 2009\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Fundamental Training Need\n\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2009\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Case Against Teaching Access\n\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2009\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo, Really, Show Us The Data\n\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2009\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepresent and GeoDjango\n\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2008\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhite House Beat Feature Request\n\n\n\n\n\n\n\n\n\n\n\n\nSep 23, 2008\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Difference\n\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2008\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Birth of Quadruplets, or Understanding the Process\n\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2008\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaspio’s Lessons\n\n\n\n\n\n\n\n\n\n\n\n\nJun 29, 2008\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Future of News Libraries\n\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2008\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn Bomb-Throwing\n\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2008\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOf the Web vs. On the Web\n\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2007\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInnovation Belongs in the Newsroom\n\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2007\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Times\n\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2007\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeaching Data on the Web\n\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2007\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn Trials, Software and Otherwise\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2007\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutsourcing Database Development, or the Caspio Issue\n\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2007\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDjango, iCal and vObject\n\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2007\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Original (and Future?) Facebook\n\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2007\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe New Competition\n\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2007\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinding Enterprise Reporting\n\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2007\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShoot the Google\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2007\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy The Web\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2007\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCongressional Vote Database\n\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2005\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nXpdf on the Mac\n\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2005\n\n\nDerek Willis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Long Bet\n\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2002\n\n\nDerek Willis\n\n\n\n\n\n\nNo matching items"
  }
]